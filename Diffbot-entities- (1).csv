title,date_str,author,siteName,tags_label,text,sentiment,pageUrl,quotes_quote,
Investors Back Diffbot’s ‘Visual Learning Robot’ for Web Content,d2012-05-31T20:15,Lizette Chapman,WSJ,"Palo Alto,Diffbot,investor,Brad Garlinghouse,content management system","So, just like people, Diffbot looks at a webpage and instantly identifies the important objects on the page while stripping out other components like advertising banners and privacy policies that are not core to the person's reason for visiting the page.
Diffbot makes its APIs available to developers--the Palo Alto, Calif.-based start-up is now processing 100 million API calls each month--who are using it for to transform the web into a usable database. They then use that data for applications they're developing to mobilize websites, migrate content management systems, generate tags and aggregate articles, among other things.
Tung, who heads the six-person team, said he waited as long as he could to raise the seed round, which closed this month.
""We're at a point now where our servers are going down,"" he said, referring to the increasing demand by developers for the technology. ""We need to expand our offering.""
Tung said that along with expanding the team with a few key hires, Diffbot will expand from the current two categories in news and home pages it now covers to the other 16 or so.
Diffbot investors include Stanford accelerator StartX, where Tung is still an entrepreneur in residence, as well as Matrix Partners. Individual investors include Sky Dayton, founder of EarthLink; Andy Bechtolsheim, co-founder of Sun Microsystems; Joi Ito, director of the MIT Media Lab, Brad Garlinghouse, CEO of YouSendIt; and executives from Facebook, Twitter and Yahoo.
Dayton, who visits the start-up about once a week, has helped with hiring and in talking to customers. He pointed to Diffbot being used with within apps at AOL, OnSwype and others.
""I make customer intros, but it kind of sells itself,"" said Mr. Dayton.
Write to Lizette Chapman at lizette.chapman@dowjones.com. Follow her on Twitter @zettewil",,https://blogs.wsj.com/venturecapital/2012/05/31/investors-back-diffbots-visual-learning-robot-for-web-content/,We're at a point now where our servers are going down.,
Investors Back Diffbot’s ‘Visual Learning Robot’ for Web Content,d2012-05-31T16:15,Lizette Chapman,WSJ,"Robot,Content (media),World Wide Web,Learning,Stanford University,Diffbot,Investor","Mike Tung wants to take the web apart and re-build it for a new audience: computers.
A graduate of Stanford University’s Artificial Intelligence program, Tung created Diffbot with the goal of creating a visual learning robot that extracts and analyzes web content the same way humans do.
The idea, which has gathered $2 million in seed funding, is simple.
Tung determined that all content on the web can be categorized into 18 or so different “page types” (like a home page, social networking profile, review etc.) that can be visually analyzed using layout and contextual cues.",,http://blogs.wsj.com/venturecapital/2012/05/31/investors-back-diffbots-visual-learning-robot-for-web-content/?mod=google_news_blog,,
http://web.archive.org/static/js/analytics.js:13 in onload_func Status: 200 OK X-Diffbot-Render-Hostname: choi5 Content-Type: text/html; charset=utf-8 X-Location: http://web.archive.org/web/20030216041056/http://xbox.gamerweb.com/reviews/xbox/shenmueii.asp,d2002-11-14T00:00,,archive.org,"Diffbot,Dreamcast,List of HTTP status codes,Shenmue II,Video game console,Hostname,Character encoding,Video game,UTF-8,Shenmue","The most ambitious videogame project arrives on the most ambitious videogame console. Shenmue II recaps, then picks up where the Dreamcast original left off.
Shenmue appeared on the Dreamcast to critical acclaim. Shenmue II was originally scheduled to appear on the Dreamcast console in North America to continue the story. With SEGA going third party, the chances of further chapters appearing seemed a little less likely, especially when a European release happened for the second part of Suzuki-san's epic story of a young man's quest for vengeance and answers, and didn't for North America. With the Shenmue saga reportedly lasting 16 chapters, there's a lot of story left to tell. Dreamcast owners waited, and waited.
Microsoft stepped up to the plate backing SEGA and Suzuki-san's dream, giving Shenmue II a place to call home, at least in North America. While European Dreamcast owners got to play the game quite a while ago, it wasn't until almost a year after the Xbox launched, nearly a full two years after the original Shenmue that the sequel (really a continuation of the story) to the Dreamcast critical darling arrives. The wait is most definitely worth it though, even if you haven't played the original.
The story of Ryo Hazuki is a classic one of family vengeance, and players new to the saga will be brought up to speed rather quickly with the included DVD of Shenmue The Movie (which was released in Japan). Clocking in at approximately 90 minutes, the film was culled together using many of the key sequences from the Dreamcast game. Using both FMV and gameplay footage (including highlights of the 70 man fight) the backstory to Shenmue II is told. Hopefully there will be a similar digest for the rest of the story in future installments.
After watching his father's murder at the hands of the mysterious Lan Di, young Ryo seeks out the men who killed his father and in the process stolen one of two mirrors that were hidden. A mysterious letter arrives, from ZhuYuan Da (written in Chinese), telling the elder Hazuki to seek the aid of Master Chen and to beware of those seeking the mirror. It seems the letter was a little too late in arriving.
Master Chen knows about the two mirrors, and ultimately aids Ryo in seeking passage to Hong Kong to find Yuanda Zhu (another form of Zhu Yuan Da) for answers regarding the two mirrors, as well as Lan Di, to seek vengeance for his father's murder. While Master Chen doesn't know where ZhuYuan Da is, he knows of somebody that might. Armed with a letter of introduction from Master Chen, Ryo heads to Hong Kong, and the original Dreamcast classic ends.
Leaving his home and friends behind him, young Ryo arrives in Hong Kong wide eyed and is promptly accosted. This is where the first essence of immersion occurs. This game is all about the details, and Suzuki-san captures what it feels like to come off the boat for the first time in Hong Kong. People will offer tours, advice, places to stay, taxi rides - you name it. Hong Kong is one of the most confusing and disorienting cities in the world - a great setting for a quest.
Here, we also find one of the great flaws of Shenmue II. Since the game was slated for the Dreamcast (much like the PS2 port of Headhunter) there is a limit to what can be displayed visually. Hong Kong is a bustling city, unlike the sleepy town of Yokosaka, Japan, where Ryo is from (and we spent so much time in during the first chapter). There are more people on the streets walking around, but not as many as one would expect. Certain advances and tweaks done during the conversion to the Xbox, but unfortunately, the fact that this was a Dreamcast game is unavoidable when looking at the streets.
To its credit, Shenmue II appears to have fixed the static movements of pedestrians. People ""flow"" better in this episode than in the previous. Character model quality will vary depending on how much time Ryo will interact with that person. People that we only see fleetingly still have that Dreamcast block look to them, while other characters (such as Joy) have some actual curves installed. There is a limit to what can be done with a conversion, and AM2 should be given praise for not simply doing a quick port of Shenmue II. It shows that the developers worked on improving this chapter visually, even with a limited amount of time.
If they really wanted, the developers could have spent another year converting the models for every character, but then the game may have never been released, there are so many people. That's very evident in the first couple of hours of play. This world is much larger than that of the original Shenmue. With a bigger world come more people, especially in a city the size of Hong Kong.
Since Hong Kong is such a big city, the criminal element is also present. There's more than just your run of the mill thug that Ryo would wind up fighting protecting some young kid from back home. Here, the neighborhood toughs are of a different breed, and they give the player their first taste of the QTE (Quick Time Event) style gameplay.
While coming off the boat, the player is engaged in FREE (Free Radical Eyes Entertainment) gameplay - walking around, approaching people on the street, engaging in conversations, taking notes in the notebook, purchasing maps, gathering information, etc. This will serve as the bulk of the gameplay, allowing player to sort of pick and choose the path that Ryo takes to fulfill his quest. The first QTE happens when Ryo's backpack is stolen, with the second mirror inside. Since the mirror is one of the keys to finding his father's murder (and why), it's paramount that Ryo take pursuit. Players will respond to on screen cues (such as A or B or a directional arrow) - and if completed in time, Ryo will successfully avoid a crate, or an opening door for example. Miss, and the young hero falls behind and eventually will lose sight of his quarry.
This brings us to the third primary gameplay mechanic, the fighting. Ryo has a set palette of moves that he arrives in Hong Kong with. Buttons are assigned for punching, kicking and grabbing/throwing. Used in combination with the thumbstick, Ryo can usually find his way on the other side of a fight intact. Occasionally, Ryo will fight in the first person mode, rather than in profile as in most fighters. This is a welcome change, one that would be nice if the player were given the option of choosing which position to be in for a fight (first or third).
Don't expect to find the level of depth as you would in AM2's fighting masterpiece Virtua Fighter 4. While the two games do share a lineage as far as fighting goes, the styles are different, and the Shenmue series is at it heart a story, not a brawler. There is much more packed in to the game than code for fighting. This isn't to say that there isn't room for advancement. That will come later.
Carrying the letter of introduction from Master Chen (from the first game), Ryo seeks out Lishao Tao. This provides the player with their first sub-quest, and reason for interaction with all of the NPCs that litter Hong Kong's various neighborhoods. New to Shenmue II is the ability to direct the conversation. Ryo approaches a person, and with the Y button can talk about money (where to find a part time job, gambling, etc.) and the A button selects a ""normal"" conversation regarding the story. This addition to the gameplay is most welcome, and adds some depth, particularly where the monetary concerns are. Ryo needs to make money to earn his bed (no going home every night), so right away, finding a job is rather key.
Ryos first job is moving boxes along the pier. This provides some quick cash to pay for the room Ryo stays in as well as some cash that can be spent in other ways. This also provides the player with their first mini-game. Acting very much like a rhythm game, Ryo will in tandem with a NPC move crates from one pallet to another, going forward, left or right. Thankfully, unlike the original game, the job is voluntary as it's not the only way to make money in the game. Instead of being forced to go through daily forklift races (as players of the original Shenmue will remember) players can elect to return to the pier to move boxes, or find other sources of employment.
Other ways to obtain money include gambling (arm wrestling mini games for example), selling items at a pawnshop, or working at other art time jobs. Pawnshops will vary in what they will buy and for how much; so collecting fliers from the various pawnshops in the various neighborhoods will be necessary in order to keep things straight.
The environments in Shenmue II are not only more massive, they are better detailed as well. The walls have more of a variance to them between buildings, and there seems to be more types of buildings as well. Though you can't pick up everything like in the first game, there's so much to do you won't need to go around picking up cups and tapes and whatever else happens to be in the closet.
The vertical nature of Hong Kong (and Kowloon) has been accurately represented, a tough feat to accomplish accuratly.
While walking down the street, there will be other pedestrians, most (if not all) of which you can talk to. The problem is that at times, a person will just ""appear"" seemingly out of nowhere. This is due to a limitation of the Dreamcast, which the game was originally coded for. These people can usually be avoided with little difficulty, and are of little consequence to the overall outcome of the game. Occasionally, a person will be able to lead the player to the place that Ryo is searching for.
Since the game was converted from the Dreamcast, there are limitations, and concessions that have been made visually in Shenmue II. That being said, the game still looks good. Even better, is the work that was done on certain characters to bring them up to the Xbox level graphically. Ryo looks particularly fine, with added details over the Dreamcast version of Shenmue and Shenmue II. Several characters have obviously gotten a visual overhaul as well. The slowdown that was apparent in the Dreamcast version has apparently been fixed, since it doesn't occur very often here on the Xbox. While the game may have originally been coded for the Dreamcast, it still looks just as good if not better than some games that were made for the current generation of consoles.
The sound has improved over the original, but the voice acting still leaves quite a bit to be desired. There are far too many times when our impassioned hero sounds like Steven Segal on Quaaludes. The flip side of this is the wealth of conversations that Ryo has with virtually every inhabitant of Hong Kong. Nearly everybody will say something when Ryo approaches, even if it is to say that they don't know anything. While the voice acting may not be stellar, the actual script and depth of conversation in the game is staggering.
New to Shenmue II is the option to have a conversation regarding either the story (the main part of the game) or one concerning monetary matters (such as finding a pawnshop, part time job, or some gambling). This new conversational element adds to the gameplay immensely, since now players don't have to worry about what kind of conversation Ryo will have when approaching a person. If Ryo has a few hours to kill, finding some part time work may be in order. By activating the $ conversation icon, this avoids any unnecessary talk regarding things the player already knows.
Many of the conversations lead to minigames, either as gambling, or part time work. Much like the real city, the Hong Kong of Shenmue II is a maze like structure full of nooks and crannies for the unsavory element to ply their trade. Dice and other games of chance proliferate the streets, as well as the opportunity to arm wrestle. These are only a few instances of how Ryo's pockets can become quite full.
Spending the money is again, all too easy. Similar to Shenmue, there are places to insert coins and collect trinkets. Unfortunately, many of the people that played the original game won't be able to bring their collection to Hong Kong. Ryo does enter the bustling port with a collection already started, but part of the fun was the collecting and trading of items. Missing from Shenmue II is the online component, and the ability to trade items as well as rankings for the various arcade games.
One quirk with the toy collecting is that it belies the time frame that the game is set. Each day passes, and a running calendar is kept. Since the game is set in 1987 (the whole story begins in 1986), there should be no way that Sonic figures, let along Virtua Fighter 2 figures (a game from the late 90's) would exist. Do we really care? No, it's all part of the charm. The figures are rather cool, whether they are Sonic, VF's Sarah, NiGHTS, or any number of odd trinkets.
""...the sheer number of gameplay elements that are incorporated have drawn the player into the world with no escape.""
Aside from spending money with gambling and collecting (as well as getting drinks from vending machines and lodging) another way to spend the money is with several arcade games that will be found. Players can spend literally days searching the various nooks and crannies for games (and we're talking real time, not game time). One of the first that becomes available is darts. Heck, there are even a few jukeboxes spread around for Ryo to waste his money on.
Later on, other games become available. Rather than having to go through the game in order to play each minigames (aside from the gambling), there is a menu option from the start menu allowing players to access unlocked games (particularly the arcade games, such as Space Harrier and Afterburner). Heck, even some of the minigames will aid the player in honing their QTE battle skills. Many people enjoyed the original Shenmue for its minigames, alongside the epic storyline. Shenmue II doesn't disappoint in either story, or minigames.
The music and atmospheric sounds (such as crowds) are quite stellar. Many of the street scenes actually sound like Hong Kong. The music evokes appropriate moods for each scene, building tension, or letting the actions speak for themselves. It truly is nice to hear a game with a soundtrack that enhances the experience the way this one does.
Sound effects are a mixed bag. There is great variety, but at times when Ryo is running on dirt, it sounds as if he is running on stone. This comes from the sheer immense nature of the game - there is no way to have every little thing perfect in a game of this size, some leeway has to be given. That doesn't mean that we can overlook bad effects (or voice acting) but it does mean that we should take it into account when playing, and ultimately enjoying, the game.
One nice change over the original is the ease of saving. Now, regardless of where players are in the day, going to the folder area (the Y button) will bring up the option to save the game (along with a wealth of over items, such as maps, fighting moves, collections, pawnshop flyers, etc.). This adds considerably to the playability of the game. Now, instead of having to sit for a while and play through an entire day before being able to save, now it is possible to sit for a half hour at a time and play (though at that rate, Shenmue III could be released before this installment is completed).
The the new camera feature is also a nifty addition to Shenmue II. It fits right in with the tourist ""fish out of water"" type theme, as well as giving the player a quick and easy way to reference things. With a touch of the Black button, the moment (whichever one it may be, in game or FMV) is captured. Not only can players (as Ryo) take pictures, but there are various filters that can be switched out to give the game different moods. Players can play through the game in a sepia tone, or black and white if preferred.
With all these features, it's easy to forget the actual gameplay of Shenmue II. Everything ties into everything else in this game. The visuals lead to the sound, which lead to items that add depth, which sound great, that take up time, which passes, and is conveyed wonderfully, bringing us back to the visuals. While you can't pick anything up like the original, the detail in Shenmue II is staggering. Hong Kong actually looks like Hong Kong, from the tall buildings, to the tile lined interiors of the small shops. Shenmue II is so engrossing, not only in the level of detail that it presents, but also in the story, which is engaging, and has many different twists, changing on the player enough to keep them interested, but not so much as to be unbelievable.
The story doesn't really start to pick up pace until a few hours into play. By then, it's too late in a way. While some players may be put off by the lack of depth in one aspect of the game (such as the fighting) the sheer number of gameplay elements that are incorporated have drawn the player into the world of Ryo Hazuki with no escape. After leaving Hong Kong for Kowloon (and eventually beyond), the scenery and sub quests change, but the basic gameplay elements are the same. What bugs me is that Kowloon is actually part of Hong Kong, not anywhere near as seperate as the game would have you believe. Hong Kong is an island, with Kowloon connected by a 5 minute ferry ride, or the subway. Since this is a game though, some license must be given (heck, Aberdeen and Wan Chai are not connected in the real city).
The locations are huge. The storyline is epic. The environments are detailed. How detailed? There are people cleaning inside buildings that you can't even enter. That's the level of detail that Suzuki-san put into this game. Shemue II lives up to the hype and delivers on so many levels, it's absolutely staggering.
Perhaps the biggest gripe that anybody could have about the game is the controls. Ryo doesn't turn around quickly, and cornering can be a bit of a chore. When standing beside a map, adjusting Ryo's position can be like steering a cow through a fish tank. Fighting can also seem a little unresponsive at times. The control issues however don't detract from the immersive nature of the game. After getting a handle on the (slight) control issues, players will have no problem getting through the game.
The game's pace may leave many gamers accustomed to the frantic pace of titles such as GunGrave wanting more. Shenmue II does have plenty of action, but there is a lot of running around looking for the correct person to answer a question as well. This game rewards patience with one of the best stories wrapped in a truly unique gameplay experience.
To say that the Shenmue saga will be looked upon in the gaming history books as epic is probably an understatement. The scope envisioned with this series has never before been attempted, let alone achieved. The series will have an influence on videogames in the future, whether conscious or not (the next three Tomb Raider games are said to be three chapters of an overall story). This is Suzuki-san's crowning achievement, history in the making. To have it take place is one thing. To have been executed so well is another.
Shenmue II: The Scores
Graphics
Sound
Gameplay
Depth
Presentation
Overall
8.0
7.5
10
9.5
9.5
9.4
The Final Word: Shenmue II is without a doubt one of the greatest games ever, regardless of console. Sure, the graphics are a little rough around the edges, and Ryo sounds like he just woke up most of the time, but really, many of the greatest games have flaws. It would have been nice had the game included some online features as in the original. The strength of the characters and story as well as the incorporation of many different styles of gameplay (including a few old arcade favorites) completely redeems the title. Anybody interested in scope, vision, and story, owe it to themselves to get this game.
gamerweb reviews policy
Wayback Machine doesn't have that page archived.
Want to search for all archived pages under http://ads.addvantagemedia.com/cgi-bin/advertpro/ ?",-0.04938,http://web.archive.org/web/20030216041056/http://xbox.gamerweb.com/reviews/xbox/shenmueii.asp,,
Diffbot,d2012-08-16T00:00,,wikipedia.org,"Stanford University,Stanford Student Enterprises,Diffbot","Diffbot is a developer of machine learning and computer vision algorithms and public APIs for extracting data from web pages / web scraping. The company was founded in 2008 at Stanford University and was the first company funded by StartX (then Stanford Student Enterprises), Stanford's on-campus venture capital fund.[1]
The company has gained interest from its application of computer vision technology to web pages, wherein it visually parses a web page for important elements and returns them in a structured format.[2] In 2015 Diffbot announced it was working on its version of an automated ""Knowledge Graph"" by crawling the web and using its automatic web page extraction to build a large database of structured web data.[3]
The company's products allow software developers to analyze web home pages and article pages,[4] and extract the ""important information"" while ignoring elements deemed not core to the primary content.[5]
In August 2012 the company released its Page Classifier API, which automatically categorizes web pages into specific ""page types"".[6] As part of this, Diffbot analyzed 750,000 web pages shared on the social media service Twitter and revealed that photos, followed by articles and videos, are the predominant web media shared on the social network.[7]
The company raised $2 million in funding in May 2012 from investors including Andy Bechtolsheim and Sky Dayton.[8]
Diffbot's customers include Adobe, AOL, Cisco, DuckDuckGo, eBay, Instapaper, Microsoft, Onswipe and Springpad.[5][6][9]
[edit]
External links[edit]
Official website",,https://en.wikipedia.org/wiki/Diffbot,,
Diffbot's Knowledge Graph Indicates Machine Learning Still Remains Early In Its Lifecycle,d2018-12-05T14:41,David A. Teich,Forbes,"Knowledge Graph,Machine learning,Artificial intelligence,Product lifecycle,Diffbot,Information technology,Knowledge,Satya Nadella","Diffbot is a startup focused on using artificial intelligence to better provide companies information found on the internet. The core product is a knowledge graph they claim has mapped “over 10 billion entities” with their associated information and relationships. As part of their marketing, they have released a short report about the state of the machine learning industry.
The key slide I saw in the report is included here.
The slide purports to show which organizations are hiring machine learning (ML) talent. The top companies are no surprise. For a couple of years, Microsoft MSFT +0.74%
CEO Satya Nadella has had a very public focus on extending ML and AI across Microsoft’s products. It’s a slight surprise that is a bit ahead of , but both firms are clearly focused on the future of ML and what it can do for their businesses. , two large, global, consultancy organizations, are strongly focused on the future. A previous article pointed to Accenture’s interested prototype displayed in October at Tableau 2018.
What caught my eye, however, was how limited the list was. There are the few large companies who can afford to play in a sand box, and then there are the universities. That information lead to a request to Diffbot for a bit more information and they were kind enough to send me a spreadsheet with more information on organizations. That information shows three clear areas of interest in hiring ML talent:
As mentioned, it is no surprise that large companies can afford to be ahead of the curve and that they invest in new technologies early in the product life cycle. It should also be no surprise that financial companies are interested in ML. The speed and volume of financial transactions have already moved past the ability of people to manage such transactions in a timely and accurate way. ML holds the promise of both faster transactions that keep a competitive edge and the identification of fraudulent transactions that need to be quickly controlled.
What is even more significant to me is how the knowledge graph points to universities. Research in many areas, including physics, medicine, and more, can be enhanced by the power of machine learning to analyze large amounts of data. However, that is not something that directly impacts business performance. Academic research is critical not only for our understanding of the universe but will also filter to the business world. The key is “filter”. Knowledge and technology transfer is not instant.
The cloud software environment means that many of the large companies will be able to include ML into business applications much more rapidly than in the past, and fintech companies will be less visibly impacting business and the economy, but the disparity between those and the smaller companies still implies the technology is early in its lifecycle. The inclusion of a large number of universities in the list supports that analysis.
Machine learning is important and is beginning to filter into the business world, but it is only beginning. Businesses should be keeping an eye on ML, but business customers are not going to fail if they don’t immediately move to suppliers who currently provide ML. Look carefully at the ROI machine learning brings to your systems, it could be worth waiting for a more mature set of solutions.",-0.19496,https://www.forbes.com/sites/davidteich/2018/12/05/diffbots-knowledge-graph-indicates-machine-learning-still-remains-early-in-its-lifecycle/,,
Diffbot Automatic Web Extraction Services,,,amazon.com,"Apis,Diffbot",Diffbot APIs automatically extract only the important content from web pages. Provide a web page or web site and Diffbot will give you the relevant data.,,https://aws.amazon.com/marketplace/pp/B07DJBCYXQ?qid=1552253875681&sr=0-41&ref_=srh_res_product_title,,
Silicon Valley stars pony up $2M to scale Diffbot’s visual learning robot,d2012-05-31T11:00,Barb Darrow,Gigaom,"Silicon Valley,Mike Tung,Andy Bechtolsheim,Joi Ito,Flipboard,Brad Garlinghouse,Diffbot,StartX,Gigaom,John Davi","What do tech luminaries Andy Bechtolsheim, Sky Dayton, Joi Ito and Brad Garlinghouse have in common? They’re all backing Diffbot, the startup that’s building visual robot technology that parses web site content to make it easier to reuse.
Diffbot, the first company funded out of Stanford’s StartX accelerator program, makes its APIs available to users wanting to extract the components of web pages in a way that makes that content reusable and easier to mash up into apps, Diffbot founder and CEO Michael Tung told me this week. It’s identified 18 web page types and the API handles two of them — front page and article — to date and is building support for the others. GigaOM’s Ryan Kim covered the launch of Diffbot’s first APIs last fall.
Unlocking web content
“We’ve got this great thing, the Internet, full of web pages, the problem is they’re made for human beings to read and understand, particularly people in front of a browser … but that’s inaccessible to software applications, hundreds of thousands of apps like Siri(s aapl), that only work with a handful of APIs that they’re hard-coded for,” Tung said.
“Yelp is great for searching places, Flipboard is great for discovering news. Our main insight is the web can be broken down into 18 types of pages, news, people,places, photos, etc. and our goal is to teach a machine to understand all that,” Tung said. The company is working on more APIs to bring all that content into its reach.
At a recent hackathon, one participant built a web reader for his blind father using Diffbot’s APIs. “For a blind person, using the web is miserable. [Today’s] screen readers read all the text starting at the top, including the nav bar and scroll down. Diffbot analyses that page, determines the title, author, text and can read it in a more natural way,” Tung said.
Diffbot can look at web pages created for human beings and analyze them visually so the app can treat the web as a big data base. It is now processing more 100 million API calls monthly for software developers using the service for Web site mobilization, tag generation and other functions.
A-list backers
Bechtolsheim, the founder of Sun Microsystems(s Orcl); Sky Dayton, founder of Earthlink and Boingo; Joi Ito, director of the MIT Media Lab: Brad Garlinghouse, a former Yahoo(s yhoo) exec and now CEO of YouSendIt (see disclosure) all invested in this $2 million seed round as did Jonathan Heiliger, the Facebook(s fb) vet now at North Bridge Venture Capital Partners.
The company is using a freemium model, encouraging developers and others to submit URLs to the system for content extraction. The service is free up to a certain number of API calls. “We want to apply Diffbot to the entire web, but it’s expensive to build a web crawler; we only analyze the URLs that people send us,” Tung said.
John Davi, Diffbot’s VP of product and a Cisco(s csco) veteran, said the submissions in themselves will be valuable. “Our long-term vision is to avail ourselves of the cream of the content that comes out. We’ll be able to see the important pages — the articles and recipes that people submit — and we think there’s value in knowing that.”
Disclosure: YouSendIt is backed by Alloy Ventures, a venture capital firm that is an investor in the parent company of this blog, Giga Omni Media.",,http://gigaom.com/cloud/silicon-valley-royalty-pony-up-2m-to-scale-diffbots-visual-learning-robot/,"We’ve got this great thing, the Internet, full of web pages, the problem is they’re made for human beings to read and understand, particularly people in front of a browser … but that’s inaccessible to software applications, hundreds of thousands of apps like Siri(s aapl), that only work with a handful of APIs that they’re hard-coded for.,Yelp is great for searching places, Flipboard is great for discovering news. Our main insight is the web can be broken down into 18 types of pages, news, people,places, photos, etc. and our goal is to teach a machine to understand all that.,For a blind person, using the web is miserable. [Today’s] screen readers read all the text starting at the top, including the nav bar and scroll down. Diffbot analyses that page, determines the title, author, text and can read it in a more natural way.,We want to apply Diffbot to the entire web, but it’s expensive to build a web crawler; we only analyze the URLs that people send us.,Our long-term vision is to avail ourselves of the cream of the content that comes out. We’ll be able to see the important pages — the articles and recipes that people submit — and we think there’s value in knowing that.",
Don’t Read The Comments — Let Diffbot Analyze Them Instead,d2015-03-31T15:54:16,Anthony Ha,TechCrunch,Diffbot,"Diffbot‘s mission, according to CEO Mike Tung, involves “teaching a robot how to read and understand web pages.” Today it expanded that understanding to include forums, comments, reviews, and other online discussions.
When Tung talks about understanding web pages, he means turning the content into structured data — say, looking at an article and identifying the title, author, text, images, topics and so on. That information, in turn, can help businesses find and track the content that’s relevant to them. (Diffbot customers include Microsoft/Bing, Cisco and eBay.)
Until today, however, Diffbot could perform its analysis on an article or a product page, but it couldn’t do the same for the comments under the article or the reviews under the product description.
Tung said there are a couple of specific challenges when it comes to analyzing these kinds of discussions. For one thing, comments are often presented in a JavaScript widget, so it’s not as straightforward as pulling the text — it requires “a bunch of visual analysis,” he said. For another, discussions often use more casual, colloquial, and emoji-heavy English, so Diffbot needed to develop “a more specialized language model.”
You can try it out for yourself using Diffbot’s test-drive page, where you can see Diffbot’s analysis for any page. To try it out, I looked at the results for a post I wrote last week that got more comments than usual, and I could see the basic attributes of each comment — author, time, text, language and author link.
This gets more interesting in aggregate when you can start finding larger trends in the conversation — Tung noted that while there are a lot of social media monitoring tools, it’s harder to track conversations across the web, where you’ll find “detailed, well-thought-out discussions.” For example, he said a shoe company could identify which shoes customers identify as most comfortable in their online conversations.
Diffbot says its new Discussions API supports Facebook Comments, Disqus, Livefyre, WordPress, Blogger, Automattic’s Intense Debate, Kinja, Hacker News, Reddit and more.",,https://techcrunch.com/2015/03/31/diffbot-discussions-api/?ncid=rss,,
Analyze SitePoint Author Portfolios with Diffbot,d2014-08-20T00:00,Bruno Skvorc,SitePoint,"Diffbot,SitePoint","Minimal. Clean. Simple. Lightweight. Responsive.
Download Our FREE Base Theme!
As the managing editor of the PHP channel for SitePoint, I deal with dozens of authors, hundreds of topics and a constantly full inbox. Filtering out inactive authors and pushing the prolific ones to the top of the queue is hard when the channel is this big and a one-man operation, so enlisting the help of bots only makes sense.
I recently started the construction of an in-depth work-analysis tool that helps me with social spread, reviews, activity tracking, personality profiling, language editing and more, hopefully automating a large portion of my work soon, and a key component is author activity. Specifically, tracking how much they publish in any given week, month or season.
Author Profiles
Each SitePoint author has a profile page which lists their bio, their social network links, and their published posts. For example, here's mine and here's Peter's. Each post snippet has the relevant information I need in order to track activity: a date, a title and a URL. By grabbing all of an author's posts, we can group them by date and extract some statistics.
Granted, the publication time depends on a variety of factors - from my own ability to squeeze reviews into the current work queue, to sponsors and other channel preferences. Still, any insight is good insight, and as my tool helps me automate parts of my workflow, reviews will happen sooner.
That said, how can we fetch this author data reliably?
To API or not to API
The logical approach would be to consume an API. Something like a call to https://api.sitepoint.com/v1/author/bskvorc?area=posts would make the entire task a breeze. Alas, SitePoint has no API and we're forced to crawl it, unless we have database access (for the purpose of this demo, let's assume we don't).
Diffbot to the rescue! We've written about Diffbot before, so give our introductory post a read if you haven't already to get familiar with it. In a nutshell, we'll use Diffbot to automatically crawl all the pages of an author's profile, extract the data we need, and get it back in JSON format.
Getting Started
I'll assume you've already set up a Diffbot test account so you can follow along. By going through the contents of this tutorial, you'll come away with knowledge on how to apply the Custom API to any post type online, and will be able to successfully harvest any website you come across.
Enter the Diffbot developer dashboard now, using your provided token. Under ""Custom API"", go to ""Create a Rule"" and select ""Custom API"". You'll be asked to give the Custom API a name. I used ""AuthorFolio"" for mine.
Once the API endpoint is created, you can enter a test link. For the purposes of this demo, I used my own author profile: http://www.sitepoint.com/author/bskvorc/
The first screen you'll see after the Test executes is a blank one:
Domain Limiting
By default, the rules apply to all http://sitepoint.com.* links. You can change this by clicking ""Change this"" under the Test URL in the dev dash.
In the text field which opens, replace (http(s)?://)?(.*\.)?sitepoint.com.* with (http(s)?://)?(.*\.)?sitepoint.com/author/.*.
We added another URL segment to the ruleset, making it apply to a more specific set of pages, thus preventing interference with any other APIs we'll potentially be throwing at the sitepoint.com domain along the line. The AuthorFolio API now only reacts to author pages.
The Diffbot API requires you to define a domain regex before you define your custom fields, else you'll have to redefine them after changing the regex. This will change in the future, but for now, keep it in mind.
Your First Custom Field
Let's start things off easy. We'll grab the author name first.
Click ""Create a Custom Field""
Use the preview window to click on the author name
Check to see if the preview above the preview window outputs the correct name
If it does, give the field a name (like ""author"") and click Save
After the preview popover disappears, you should see the author field populated.
Repeat this procedure to get the bio. You should have a result like this one:
Your First Collection
Since ""posts"" is a repetitive entity (there are several posts on each page of the author profile), we can't use a simple field to fetch them - else only one post could be fetched. Instead, we use a collection.
Custom API Collections are sets of elements that get repeated. When you define a collection, you define something that appears several times, and then apply the rule to that collection. This creates a bucket of all matching items, returning them as a JSON array. Let's demonstrate that to make it clearer.
Click ""Create a Custom Collection""
Give it the name ""Posts""
Under ""CSS selector"" put .search-results-list article because that's what each post's CSS is described as.
Click ""Save""
Once the screen reloads, your new collection will be saved, but you won't see any values inside just yet. It will, however, let you know how many instances it found (in my case, it was 10 due to SitePoint's author profiles being paginated every 10 results).
Now that the collection is defined, we can add fields. Let's start with the post title.
Click ""Add a custom field to this collection""
Give it the name of ""posttitle""
Under ""CSS selector"", put .article_title and click Save
Once the results reload, you should be able to list out all 10 posts by clicking ""Show More Results"".
To get the post URL, we need to target the href attribute of the link in the post's title.
Click ""Add a custom field to this collection"" and give it the name of ""posturl""
Under ""CSS selector"" put .article_title a
Add an Attribute filter, and set href as the attribute, like so:
Notice how the preview already shows the correct URL. Save and wait for the reload, then notice each ""Post"" instance in the collection has two properties now: title and url.
You can easily extract the date from the bottom of the post frame by looking at its class through the Dev Tools. I called this field ""postdate"".
But by looking through the Dev Tools, we notice that SitePoint uses the time HTML element to represent publication dates. This works in our favor as it lets us also extract a standardized Y-M-D date for use in other applications that are to consume this fetched data. The time element uses the datetime attribute for the data value, and that's what we'll grab. Let's make a new field: ""postdate_ymd"".
Create a new field called ""postdate_ymd"".
Target the same CSS selector as before: .article_pub-date but add the time element into the mix, so it looks like this: .article_pub-date time.
Add an attribute filter, and put in datetime, then Save.
Voilà! We have the Y-m-d datetime value now.
We've extracted almost all values we need now - but all this only applies to the first page of the author profile. If you scroll down to the bottom, you'll notice some pagination links. Does this mean we'll have to issue a custom API call to each page, then concatenate the extracted values manually? Of course not, that would be tedious. Diffbot's API is smart enough to recognize paging patterns when it sees them, so all we need to do is tell it where the ""next page"" link is, and it will take care of things for us. In fact, it's so smart it'll actually automatically merge the contents of all the pages it finds, and apply our extraction rules to the final merged page.
To activate automatic pagination with Diffbot's Custom API, we need to define a new field and call it ""nextPage"".
Create a new field (global, outside collections) called ""nextPage""
Scroll down to the bottom and click the next page link (||)
Add an attribute filter for href
Once you save and the result reloads, you should have the full set of posts at your disposal. If you look at the JSON output, you'll see the numPages property, and the entire set should be much bigger than what you were used to up until now while testing on the first page only. You should also get the set of ""nextPages"" - all the pages it could automatically deduce as those following the first one:
Conclusion
In this post, we looked at website crawling with Diffbot - a bot that visually extracts data from rendered pages. We used it to harvest the name, bio and all the post links of a SitePoint author, and to automatically paginate through the results. By calling the API URL regularly, as in http://diffbot.com/api/AuthorFolio?token=xxxxxxxxx&url=http://www.sitepoint.com/author/bskvorc/, we can get the results in a pure JSON format, ready for consumption by other apps. In a followup article, we'll look at some more advanced techniques for avoiding repetition of collections while paginating, and we'll add more specific domain rules. Stay tuned!
Questions? Comments? Post them below!",0.05507,https://www.sitepoint.com/analyze-sitepoint-author-portfolios-diffbot/,,
Diffbot Teaches Computers to Get Their Shop On,d2013-07-31T00:00,Rachel Metz,MIT Technology Review,Computer,"Diffbot’s latest API release could help shopping sites keep tabs on competitors’ prices.
Startup Diffbot, which uses machine-learning algorithms to tech computers to “understand” Web pages (see “A Startup Hopes to Help Computers Understand Web Pages”), has expanded its capabilities to the realm of online retail–a move that could make it easier to keep an eye on changing product prices and to display shopping website content on a variety of gadgets.
The company offers several application programming interfaces (APIs) that allow computers to “read” the different components of Web pages. An API for article pages, for example, can take a URL from a news website and pick out features like headlines, text, pictures, and tags. Similarly, the product API looks at a page, extracts data such as images, a product description, and the price, Diffbot CEO and cofounder Mike Tung says.
The release of this API in particular was important because of the popularity of online shopping, Tung says. He can imagine retailers using the API to more simply scour the Web to watch competitors’ changing prices, or sites like Pinterest to grab more detailed information from the product pages that users share with others.
It’s also the latest step in Diffbot’s plan to develop computer vision techniques that can analyze all of the many different types of Web pages out there–something the company thinks is necessary as we increasingly access the Web on multiple gadgets with different size screens, rather than just on desktop and laptop computers.
“The future is in having information in these structured databases, and being able to display it however you want as you move from your cellphone to your tablet to your Google Glass to your HD TV, for example,” Tung says.
You can try out a test of the product API, or any of the others the company offers, here.",,http://www.technologyreview.com/view/517716/diffbot-teaches-computers-to-get-their-shop-on/,"The future is in having information in these structured databases, and being able to display it however you want as you move from your cellphone to your tablet to your Google Glass to your HD TV, for example.",
Crawling and Searching Entire Domains with Diffbot,d2015-07-01T00:00,Bruno Skvorc,SitePoint,"Uniform Resource Locator,Diffbot,domain of a function,web search engine,SitePoint,search algorithm,application programming interface,WordPress,web crawler,entire function","In this tutorial, I’ll show you how to build a custom SitePoint search engine that far outdoes anything WordPress could ever put out. We’ll be using Diffbot as a service to extract structured data from SitePoint automatically, and this matching API client to do both the searching and crawling.
I’ll also be using my trusty Homestead Improved environment for a clean project, so I can experiment in a VM that’s dedicated to this project and this project alone.
What’s what?
To make a SitePoint search engine, we need to do the following:
Build a Crawljob which will index and process the entire SitePoint.com domain and keep itself up to date with newly published content.
Build a GUI for submitting search queries to the saved set produced by this crawljob. Searching is done via the Search API. We’ll do this in a followup post.
A Diffbot Crawljob does the following:
It spiders a URL pattern for URLs. This does not mean processing – it means looking for links to process on all the pages it can find, starting from the domain you originally passed in as seed. For the difference between crawling and processing, see here.
It processes the pages found on the spidered URLs with the designated API engine – for example, using Product API, it processes all products it found on Amazon.com and saves them into a structured database of items on offer.
Creating a Crawljob
Jobs can be created through Diffbot’s GUI, but I find creating them via the crawl API is a more customizable experience. In an empty folder, let’s first install the client library.
composer require swader/diffbot-php-client
I now need a job.php file into which I’ll just dump the job creation procedure, as per the README:
include 'vendor/autoload.php';

use Swader\Diffbot\Diffbot;
$diffbot = new Diffbot('my_token');
The Diffbot instance is used to create access points to API types offered by Diffbot. In our case, a “Crawl” type is needed. Let’s name it “sp_search”.
$job = $diffbot->crawl('sp_search');
This will create a new crawljob when the call() method is called. Next, we’ll need to configure the job. First, we need to give it the seed URL(s) on which to start the spidering process:
$job
    ->setSeeds(['http://sitepoint.com'])
Then, we make it notify us when it’s done crawling, just so we know when a crawling round is complete, and we can expect up to date information to be in the dataset.
$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
A site can have hundreds of thousands of links to spider, and hundreds of thousands of pages to process – the max limits are a cost-control mechanism, and in this case, I want the most detailed possible set available to me, so I’ll put in one million URLs into both values.
$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
    ->setMaxToCrawl(1000000)
    ->setMaxToProcess(1000000)
We also want this job to refresh every 24 hours, because we know SitePoint publishes several new posts every single day. It’s important to note that repeating means “from the time the last round has finished” – so if it takes a job 24 hours to finish, the new crawling round will actually start 48 hours from the start of the previous round. We’ll set max rounds as 0, to indicate we want this to repeat indefinitely.
$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
    ->setMaxToCrawl(1000000)
    ->setMaxToProcess(1000000)
    ->setRepeat(1)
    ->setMaxRounds(0)
Finally, there’s the page processing pattern. When Diffbot processes pages during a crawl, only those that are processed – not crawled – are actually charged / counted towards your limit. It is, therefore, in our interest to be as specific as possible with our crawljob’s definition, as to avoid processing pages that aren’t articles – like author bios, ads, or even category listings. Looking for <section class=""article_body""> should do – every post has this. And of course, we want it to only process the pages it hasn’t encountered before in each new round – no need to extract the same data over and over again, it would just stack up expenses.
$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
    ->setMaxToCrawl(1000000)
    ->setMaxToProcess(1000000)
    ->setRepeat(1)
    ->setMaxRounds(0)
    ->setPageProcessPatterns(['<section class=""article_body"">'])
    ->setOnlyProcessIfNew(1)
Before finishing up with the crawljob configuration, there’s just one more important parameter we need to add – the crawl pattern. When passing in a seed URL to the Crawl API, the Crawljob will traverse all subdomains as well. So if we pass in http://sitepoint.com, Crawlbot will look through http://community.sitepoint.com, and the now outdated http://reference.sitepoint.com – this is something we want to avoid, as it would slow our crawling process dramatically, and harvest stuff we don’t need (we don’t want the forums indexed right now). To set this up, we use the setUrlCrawlPatterns method, indicating that crawled links must start with sitepoint.com.
$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
    ->setMaxToCrawl(1000000)
    ->setMaxToProcess(1000000)
    ->setRepeat(1)
    ->setMaxRounds(0)
    ->setPageProcessPatterns(['<section class=""article_body"">'])
    ->setOnlyProcessIfNew(1)
	->setUrlCrawlPatterns(['^http://www.sitepoint.com', '^http://sitepoint.com'])
Now we need to tell the job which API to use for processing. We could use the default – Analyze API – which would make Diffbot auto-determine the structure of the data we’re trying to obtain, but I prefer specificity and want it to know outright that it should only produce articles.
$api = $diffbot->createArticleAPI('crawl')->setMeta(true)->setDiscussion(false);
$job->setApi($api);
Note that with the individual APIs (like Product, Article, Discussion, etc..) you can process individual resources even with the free demo token from Diffbot.com, which lets you test out your links and see what data they’ll return before diving into bulk processing via Crawlbot. For information on how to do this, see the README file.
The job is now configured, and we can call() Diffbot with instructions on how to create it:
$job->call();
The full code for creating this job is:
$diffbot = new Diffbot('my_token');
$job = $diffbot->crawl('sp_search');

$job
    ->setSeeds(['http://sitepoint.com'])
    ->notify('bruno.skvorc@sitepoint.com')
    ->setMaxToCrawl(1000000)
    ->setMaxToProcess(1000000)
    ->setRepeat(1)
    ->setMaxRounds(0)
    ->setPageProcessPatterns(['<section class=""article_body"">'])
    ->setOnlyProcessIfNew(1)
    ->setApi($diffbot->createArticleAPI('crawl')->setMeta(true)->setDiscussion(false))
    ->setUrlCrawlPatterns(['^http://www.sitepoint.com', '^http://sitepoint.com']);

$job->call();
Calling this script via command line (php job.php) or opening it in the browser has created the job – it can be seen in the Crawlbot dev screen:
It’ll take a while to finish (days, actually – SitePoint is a huge place), but all subsequent rounds will be faster because we told the job to only process pages it hasn’t encountered before.
Searching
To search a dataset, we need to use the Search API. A dataset can be used even before it’s complete – the Search API will simply search through the data it has, ignoring the fact that it doesn’t have everything.
To use the search API, one needs to create a new search instance with a search query as a constructor parameter:
$search = $diffbot->search('author:""Bruno Skvorc""');
$search->setCol('sp_search');
$result = $search->call();
The setCol method is optional, and if omitted will make the Search API go through all the collections under a single Diffbot token. As I have several collections from my previous experiments, I opted to specify the last one we created: sp_search (collections share names with the jobs that created them).
The returned data can be iterated over, and every element will be an instance of Article. Here’s a rudimentary table exposing links and titles:
<table>
    <thead>
    <tr>
        <td>Title</td>
        <td>Url</td>
    </tr>
    </thead>
    <tbody>
    <?php
    foreach ($search as $article) {
        echo '<tr>';
        echo '<td>' . $article->getTitle() . '</td>';
        echo '<td><a href=""' . $article->getResolvedPageUrl() . '"">Link</a></td>';
        echo '</tr>';
    }
    ?>
    </tbody>
</table>
The Search API can return some amazingly fine tuned result sets. The query param will accept everything from common keywords, to date ranges, to targeted specific fields (like title:diffbot) to boolean combinations of various parameters, like type:article AND title:robot AND (overlord OR butler), producing all articles that have the word “robot” in the title and either the word “overlord” or “butler” in any of the fields (title, body, meta tags, etc). We’ll be taking advantage of all this advanced functionality in the next post as we build our search engine’s GUI.
We can also get the “meta” information about a Search API request by passing true into the call() after making the original call:
$info = $search->call(true);
dump($info);
The result we get back is a SearchInfo object with the values as shown below (all accessible via getters):
With SearchInfo, you get access to the speed of your request, the number of hits (not the returned results, but total number – useful for pagination), etc.
To get information about a specific crawljob, like finding out its current status, or how many pages were crawled, processed, etc, we can call the crawl API again and just pass in the same job name. This, then, works as a read only operation, returning all the meta info about our job:
dump($diffbot->crawl('sp_search')->call());
At this point, we’ve got our collection being populated with crawled data from SitePoint.com. Now all we have to do is build a GUI around the Search functionality of the Diffbot API client, and that’s exactly what we’re going to be doing in the next part.
Conclusion
In this tutorial, we looked at Diffbot’s ability to generate collections of structured data from websites of arbitrary format, and its Search API which can be used as the search engine behind a crawled site. While the price might be somewhat over the top for the average solo developer, for teams and companies this tool is a godsend.
Imagine being a media conglomerate with dozens or hundreds of different websites under your belt, and wanting a directory of all your content. Consolidating the efforts of all those backend teams to not only come up with a way to merge the databases but also find the time to do it in their daily efforts (which include keeping their outdated websites alive) would be an impossible and ultra expensive task, but with Diffbot, you unleash Crawlbot on all your domains and just use the Search API to traverse what was returned. What’s more, the data you crawl is downloadable in full as a JSON payload, so even if it gets too expensive, you can always import the data into your own solution later on.
It’s important to note that not many websites agree with being crawled, so you should probably look at their terms of service before attempting it on a site you don’t own – crawls can rack up people’s server costs rather quickly, and by stealing their content for your personal use without approval, you also rob them of potential ad revenue and other streams of income connected with the site.
In part 2, we’ll look at how we can turn everything we’ve got so far into a GUI so that the average Joe can easily use it as an in-depth SitePoint search engine.
If you have any questions or comments, please leave them below!
Bruno is a blockchain developer and code auditor from Croatia with Master’s Degrees in Computer Science and English Language and Literature. He's been a web developer for 10 years until JavaScript drove him away. He now runs a cryptocurrency business at Bitfalls.com via which he makes blockchain tech approachable to the masses, and runs Coinvendor, an on-boarding platform for people to easily buy cryptocurrency. He’s also a developer evangelist for Diffbot.com, a San Francisco-based AI-powered machine vision web scraper.",,https://www.sitepoint.com/crawling-searching-entire-domains-diffbot?utm_source=sitepoint&utm_medium=articletile&utm_campaign=likes&utm_term=php,,
Diffbot’s new API allows companies to monitor forum and site comments around the Web,d2015-03-31T00:00,Owen Williams,The Next Web,"Website,World Wide Web,Application programming interface","Diffbot’s new discussions API, announced today, makes it easy for developers to monitor comment threads almost everywhere on the Web from a single place.
Businesses are typically monitoring social media for negative comments or praise, but they can’t check every discussion thread everywhere on the internet. Diffbot’s new API aims to help them keep track of comments on any site, too.
The idea is that using the new API, developers can build custom apps to monitor discussions on any site (be it a specific site or a wider search) for mentions of a business, product or keyword and pick up whenever it’s mentioned, along with other information like the sentiment of the post (if it’s negative or positive), any links that were added and more.
Diffbot provides a number of APIs that help structure information found on the Web for use in applications; it allows apps to see the web in the way that humans understand it looking at the page.
Diffbot’s discussion extraction API is available from today for developers to start integrating into their apps. It’s easy to imagine it being quickly adopted considering how easy it makes it to scrape discussions.
You can take the new API for a visual test drive here to see what kind of data it pulls out from sites and presents to you.",,https://thenextweb.com/apps/2015/03/31/diffbots-new-api-allows-companies-to-monitor-forum-and-site-comments-around-the-web/,,
Diffbot launches APIs for monitoring web pages,d2011-08-25T10:41,Heather Kelly,VentureBeat,"Diffbot,Seed money,Web page,StartX,Web search engine,Application programming interface","Internet search startup Diffbot launched its API today for visually scanning, parsing and extracting information from web pages. Diffbot detects what type of layout a page has, then searches it for common visual cues to monitor when any content changes on a page, or to extract specific information for developers to use.
The Palo Alto-based company was founded in 2008 by two former Stanford students, CEO Mike Tung and CTO Leith Abdulla, with seed funding from Stanford incubator StartX. Tung originally created Diffbot to monitor the websites for his various Stanford classes and tip him off to any new announcements, posted lectures or assignments via text message.
According to Diffbot's creators, all web pages fall into one of 30 different page-type categories. By pegging what category a page falls in to, it can extrapolate the various types of information on that page. For example, front pages of news sites typically have the same elements: headlines, images, tags, advertisements and article summaries.
""Diffbot understands visually what all of these different elements of the page are and can be used by developers to connect that content to direct action,"" Tung told VentureBeat.
Currently Diffbot has hundreds of developers using the beta API, and some intriguing products have already been created using the tools. AOL's free Editions iPad magazine app uses Diffbot to analyze the front pages of news sites and pull out important new or breaking information. Hacker News Radio tapped Diffbot to pull content from hacker news sites and turn them into spoken reports. The city of São Paulo in Brazil uses Diffbot to track changes on the local government website and turns them into an automated Twitter feed.
Diffbot's Follow API creates an RSS-style feed of fresh content. The On-Demand API currently looks at just two major page types, Frontpage and Article, but Diffbot plans on releasing more in the future, and that's when things could get interesting.
""Once we have released the API for all 30 page types, we hope to enable a new type of mobile application - one where the user can take actions directly on web data, instead of reading a bunch of blue links,"" said Tung to VentureBeat. ""Something like SIRI, but for the entire web, and not just a set of handpicked APIs.""",-0.44395,https://venturebeat.com/2011/08/25/diffbot-launches-apis-for-monitoring-web-pages/,,
Analyze SitePoint Author Portfolios with Diffbot,d2014-08-20T00:00,Bruno Skvorc,SitePoint,"Diffbot,SitePoint","As the managing editor of the PHP channel for SitePoint, I deal with dozens of authors, hundreds of topics and a constantly full inbox. Filtering out inactive authors and pushing the prolific ones to the top of the queue is hard when the channel is this big and a one-man operation, so enlisting the help of bots only makes sense.
I recently started the construction of an in-depth work-analysis tool that helps me with social spread, reviews, activity tracking, personality profiling, language editing and more, hopefully automating a large portion of my work soon, and a key component is author activity. Specifically, tracking how much they publish in any given week, month or season.
Author Profiles
Each SitePoint author has a profile page which lists their bio, their social network links, and their published posts. For example, here's mine and here's Peter's. Each post snippet has the relevant information I need in order to track activity: a date, a title and a URL. By grabbing all of an author's posts, we can group them by date and extract some statistics.
Granted, the publication time depends on a variety of factors - from my own ability to squeeze reviews into the current work queue, to sponsors and other channel preferences. Still, any insight is good insight, and as my tool helps me automate parts of my workflow, reviews will happen sooner.
That said, how can we fetch this author data reliably?
To API or not to API
The logical approach would be to consume an API. Something like a call to https://api.sitepoint.com/v1/author/bskvorc?area=posts would make the entire task a breeze. Alas, SitePoint has no API and we're forced to crawl it, unless we have database access (for the purpose of this demo, let's assume we don't).
Diffbot to the rescue! We've written about Diffbot before, so give our introductory post a read if you haven't already to get familiar with it. In a nutshell, we'll use Diffbot to automatically crawl all the pages of an author's profile, extract the data we need, and get it back in JSON format.
Getting Started
I'll assume you've already set up a Diffbot test account so you can follow along. By going through the contents of this tutorial, you'll come away with knowledge on how to apply the Custom API to any post type online, and will be able to successfully harvest any website you come across.
Enter the Diffbot developer dashboard now, using your provided token. Under ""Custom API"", go to ""Create a Rule"" and select ""Custom API"". You'll be asked to give the Custom API a name. I used ""AuthorFolio"" for mine.
Once the API endpoint is created, you can enter a test link. For the purposes of this demo, I used my own author profile: http://www.sitepoint.com/author/bskvorc/
The first screen you'll see after the Test executes is a blank one:
Domain Limiting
By default, the rules apply to all http://sitepoint.com.* links. You can change this by clicking ""Change this"" under the Test URL in the dev dash.
In the text field which opens, replace (http(s)?://)?(.*\.)?sitepoint.com.* with (http(s)?://)?(.*\.)?sitepoint.com/author/.*.
We added another URL segment to the ruleset, making it apply to a more specific set of pages, thus preventing interference with any other APIs we'll potentially be throwing at the sitepoint.com domain along the line. The AuthorFolio API now only reacts to author pages.
The Diffbot API requires you to define a domain regex before you define your custom fields, else you'll have to redefine them after changing the regex. This will change in the future, but for now, keep it in mind.
Your First Custom Field
Let's start things off easy. We'll grab the author name first.
Click ""Create a Custom Field""
Use the preview window to click on the author name
Check to see if the preview above the preview window outputs the correct name
If it does, give the field a name (like ""author"") and click Save
After the preview popover disappears, you should see the author field populated.
Repeat this procedure to get the bio. You should have a result like this one:
Your First Collection
Since ""posts"" is a repetitive entity (there are several posts on each page of the author profile), we can't use a simple field to fetch them - else only one post could be fetched. Instead, we use a collection.
Custom API Collections are sets of elements that get repeated. When you define a collection, you define something that appears several times, and then apply the rule to that collection. This creates a bucket of all matching items, returning them as a JSON array. Let's demonstrate that to make it clearer.
Click ""Create a Custom Collection""
Give it the name ""Posts""
Under ""CSS selector"" put .search-results-list article because that's what each post's CSS is described as.
Click ""Save""
Once the screen reloads, your new collection will be saved, but you won't see any values inside just yet. It will, however, let you know how many instances it found (in my case, it was 10 due to SitePoint's author profiles being paginated every 10 results).
Now that the collection is defined, we can add fields. Let's start with the post title.
Click ""Add a custom field to this collection""
Give it the name of ""posttitle""
Under ""CSS selector"", put .article_title and click Save
Once the results reload, you should be able to list out all 10 posts by clicking ""Show More Results"".
To get the post URL, we need to target the href attribute of the link in the post's title.
Click ""Add a custom field to this collection"" and give it the name of ""posturl""
Under ""CSS selector"" put .article_title a
Add an Attribute filter, and set href as the attribute, like so:
Notice how the preview already shows the correct URL. Save and wait for the reload, then notice each ""Post"" instance in the collection has two properties now: title and url.
You can easily extract the date from the bottom of the post frame by looking at its class through the Dev Tools. I called this field ""postdate"".
But by looking through the Dev Tools, we notice that SitePoint uses the time HTML element to represent publication dates. This works in our favor as it lets us also extract a standardized Y-M-D date for use in other applications that are to consume this fetched data. The time element uses the datetime attribute for the data value, and that's what we'll grab. Let's make a new field: ""postdate_ymd"".
Create a new field called ""postdate_ymd"".
Target the same CSS selector as before: .article_pub-date but add the time element into the mix, so it looks like this: .article_pub-date time.
Add an attribute filter, and put in datetime, then Save.
Voilà! We have the Y-m-d datetime value now.
We've extracted almost all values we need now - but all this only applies to the first page of the author profile. If you scroll down to the bottom, you'll notice some pagination links. Does this mean we'll have to issue a custom API call to each page, then concatenate the extracted values manually? Of course not, that would be tedious. Diffbot's API is smart enough to recognize paging patterns when it sees them, so all we need to do is tell it where the ""next page"" link is, and it will take care of things for us. In fact, it's so smart it'll actually automatically merge the contents of all the pages it finds, and apply our extraction rules to the final merged page.
To activate automatic pagination with Diffbot's Custom API, we need to define a new field and call it ""nextPage"".
Create a new field (global, outside collections) called ""nextPage""
Scroll down to the bottom and click the next page link (||)
Add an attribute filter for href
Once you save and the result reloads, you should have the full set of posts at your disposal. If you look at the JSON output, you'll see the numPages property, and the entire set should be much bigger than what you were used to up until now while testing on the first page only. You should also get the set of ""nextPages"" - all the pages it could automatically deduce as those following the first one:
Conclusion
In this post, we looked at website crawling with Diffbot - a bot that visually extracts data from rendered pages. We used it to harvest the name, bio and all the post links of a SitePoint author, and to automatically paginate through the results. By calling the API URL regularly, as in http://diffbot.com/api/AuthorFolio?token=xxxxxxxxx&url=http://www.sitepoint.com/author/bskvorc/, we can get the results in a pure JSON format, ready for consumption by other apps. In a followup article, we'll look at some more advanced techniques for avoiding repetition of collections while paginating, and we'll add more specific domain rules. Stay tuned!
Questions? Comments? Post them below!",0.01032,https://www.sitepoint.com/?p=87423,,
Diffbot Teaches Computers to Get Their Shop On,d2013-07-31T00:00,Rachel Metz,MIT Technology Review,"Computer,Diffbot,Mike Tung","Startup Diffbot, which uses machine-learning algorithms to tech computers to “understand” Web pages (see “A Startup Hopes to Help Computers Understand Web Pages”), has expanded its capabilities to the realm of online retail–a move that could make it easier to keep an eye on changing product prices and to display shopping website content on a variety of gadgets.
The company offers several application programming interfaces (APIs) that allow computers to “read” the different components of Web pages. An API for article pages, for example, can take a URL from a news website and pick out features like headlines, text, pictures, and tags. Similarly, the product API looks at a page, extracts data such as images, a product description, and the price, Diffbot CEO and cofounder Mike Tung says.
The release of this API in particular was important because of the popularity of online shopping, Tung says. He can imagine retailers using the API to more simply scour the Web to watch competitors’ changing prices, or sites like Pinterest to grab more detailed information from the product pages that users share with others.
It’s also the latest step in Diffbot’s plan to develop computer vision techniques that can analyze all of the many different types of Web pages out there–something the company thinks is necessary as we increasingly access the Web on multiple gadgets with different size screens, rather than just on desktop and laptop computers.
“The future is in having information in these structured databases, and being able to display it however you want as you move from your cellphone to your tablet to your Google Glass to your HD TV, for example,” Tung says.
You can try out a test of the product API, or any of the others the company offers, here.",,https://www.technologyreview.com/s/517716/diffbot-teaches-computers-to-get-their-shop-on/,"The future is in having information in these structured databases, and being able to display it however you want as you move from your cellphone to your tablet to your Google Glass to your HD TV, for example.",
Text Extraction Using Dragnet and Diffbot,d2019-08-12T23:22:27,Iris Fu,Medium,"Dragnet franchise,sentiment analysis,Diffbot,Dragnet","Written by Krytzof Urban, Lead Data Scientist on January 16, 2018
Showing an ad on a website is easy. The tricky part is to display an ad that the reader may find useful because it fits in with the context of the page. Here at GumGum we go to great length to figure out the visual and textual context of a website.
With respect to text, we apply Natural Language Processing (NLP) tasks like topic detection, Named Entity Recognition, Sentiment Analysis, and more. But before we can apply our higher-level analysis we need to prepare the text. For website analysis, the first step is finding the main text and ignoring the useless bits such as HTML, CSS, code, ads, and links to other articles (also called boilerplate).
The motivation
In the past, we used the Boilerpipe library for text extraction. Boilerpipe looks at shallow text features (like link density and word count) per HTML block to determine whether that block belongs to proper content (check out the paper it’s based on to learn more). It works reasonably well, and heck, there’s now even a Python wrapper for it.
Boilerpipe is designed to work well for an “average” page. The problem is, page layout varies greatly between websites. We are working with thousands of website publishers, and for too many of them Boilerpipe failed to produce acceptable results. So we did what every respectable researcher does at this point — hack the tool to buy some time. We injected domain-specific text extraction rules into Boilerpipe, but obviously that became a pain to maintain. It was time to work on a solution that is automated, scales, performs well for any page layout, and doesn’t break the bank.
The approach
After a bit of researching and testing we decided on Dragnet. For one, it does a decent job out of the box. It also is a Boilerpipe-and-then-some, as it considers the same shallow text features but also looks at semantic features (read the article it’s based on for more info). Most importantly, though, it allows us to train machine learning models for text extraction for each and every one of our publishers.
Here’s an example of Dragnet performance trained on a corpus of 790 manually annotated Japanese documents. We used the Extremely Randomized Trees classifier. They are a more randomized version of Random Forests. In theory, by randomizing feature split order and thresholds, trees are less correlated to each other which reduces the risk of overfitting (at the cost of possibly increased variance).
The figure above shows precision, recall, and f-score on a per-token analysis. The x-axis shows the performance (for example, a precision of 1 means that no boilerplate was picked up), while the y-axis depicts the number of documents for which the performance was achieved.
A f1-score of 0.85 for non-English documents, that told us we might be on the right track (other comparisons report similar performance but of course they were using different data).
The data
Machine learning — check, performance — check. But what about scalability? GumGum works with thousands of publishers. Getting say 1,000 pages manually annotated per subdomain is expensive. But what do you do if you’re too cheap (ahem, economically responsible) to spend big bucks on annotation and yet worried about getting good training data for your classifier? The answer (in our case) is: use Diffbot.
Diffbot offers a number of web services based on NLP and Visual Intelligence analysis. For our purposes we use their Article API. Its JSON returns extracted text plus some additional page information. Here is a sample JSON response for one of the URLs in our test set:
One aspect we particularly like about Diffbot’s approach for text extraction is that it’s computer vision based. In theory, this should mirror a human annotator’s strategy. And indeed, our tests confirm that annotation quality is on par with that from third party annotation services.
Now we were ready to put together a pipeline that continuously creates, updates, and deploys text extraction models for all our subdomains. Training data consists of Diffbot results, manual annotations, or (in many cases) a mix of both.
Our system resides in Amazon’s cloud (AWS). S3 is a scalable storage service. We use it to store our text extraction models and at the same time make them available to all of our NLP production instances.
The results
Dragnet measures performance of its models in terms of HTML blocks. For example, if a HTML page consisted of 20 blocks, and 15 of those were correctly classified as “boilerplate” or “not boilerplate” then accuracy would be 0.75. Our text extraction block accuracy is on average 0.9 for any given subdomain with little variance.
We usually use 300 files to train and test the models. Our tests show that any corpus size north of 200 files (using 10 extra trees) leads to stable classification performance.
The table below shows a direct comparison between our Boilerpipe and Dragnet approaches. Using Levenshtein Distance to measure text extraction accuracy on a per-character basis, we can see that while Dragnet picks up more unwanted text (i.e. more deletes) it does a much better job in finding the correct text (i.e. fewer inserts) compared to Boilerpipe.
Digging deeper into the relatively large percentage of unwanted text extracted by Dragnet, we realized that about half of the extra text consists of image captions (which may or may not be considered part of the good text).
And thus we have a scalable process to automatically create domain-specific text extraction models with state-of-the-art performance. Looking forward to your comments and suggestions!",0.528,https://medium.com/gumgum-tech/text-extraction-using-dragnet-and-diffbot-117b33b01d98,,
Crawling and Searching Entire Domains with Diffbot,d2015-07-01T00:00,Bruno Skvorc,SitePoint,"Diffbot,Data model,Uniform Resource Locator,Web search engine,Web crawler,SitePoint,Application programming interface","In this tutorial, I'll show you how to build a custom SitePoint search engine that far outdoes anything WordPress could ever put out. We'll be using Diffbot as a service to extract structured data from SitePoint automatically, and this matching API client to do both the searching and crawling.
I'll also be using my trusty Homestead Improved environment for a clean project, so I can experiment in a VM that's dedicated to this project and this project alone.
What's what?
To make a SitePoint search engine, we need to do the following:
Build a Crawljob which will index and process the entire SitePoint.com domain and keep itself up to date with newly published content.
Build a GUI for submitting search queries to the saved set produced by this crawljob. Searching is done via the Search API. We'll do this in a followup post.
A Diffbot Crawljob does the following:
It spiders a URL pattern for URLs. This does not mean processing - it means looking for links to process on all the pages it can find, starting from the domain you originally passed in as seed. For the difference between crawling and processing, see here.
It processes the pages found on the spidered URLs with the designated API engine - for example, using Product API, it processes all products it found on Amazon.com and saves them into a structured database of items on offer.
Creating a Crawljob
Jobs can be created through Diffbot's GUI, but I find creating them via the crawl API is a more customizable experience. In an empty folder, let's first install the client library.
composer require swader/diffbot-php-client
I now need a job.php file into which I'll just dump the job creation procedure, as per the README:
include 'vendor/autoload.php';

use Swader\Diffbot\Diffbot;
$diffbot = new Diffbot('my_token');
The Diffbot instance is used to create access points to API types offered by Diffbot. In our case, a ""Crawl"" type is needed. Let's name it ""sp_search"".
$job = $diffbot-|crawl('sp_search');
This will create a new crawljob when the call() method is called. Next, we'll need to configure the job. First, we need to give it the seed URL(s) on which to start the spidering process:
$job
    -|setSeeds(['http://sitepoint.com'])
Then, we make it notify us when it's done crawling, just so we know when a crawling round is complete, and we can expect up to date information to be in the dataset.
$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
A site can have hundreds of thousands of links to spider, and hundreds of thousands of pages to process - the max limits are a cost-control mechanism, and in this case, I want the most detailed possible set available to me, so I'll put in one million URLs into both values.
$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
    -|setMaxToCrawl(1000000)
    -|setMaxToProcess(1000000)
We also want this job to refresh every 24 hours, because we know SitePoint publishes several new posts every single day. It's important to note that repeating means ""from the time the last round has finished"" - so if it takes a job 24 hours to finish, the new crawling round will actually start 48 hours from the start of the previous round. We'll set max rounds as 0, to indicate we want this to repeat indefinitely.
$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
    -|setMaxToCrawl(1000000)
    -|setMaxToProcess(1000000)
    -|setRepeat(1)
    -|setMaxRounds(0)
Finally, there's the page processing pattern. When Diffbot processes pages during a crawl, only those that are processed - not crawled - are actually charged / counted towards your limit. It is, therefore, in our interest to be as specific as possible with our crawljob's definition, as to avoid processing pages that aren't articles - like author bios, ads, or even category listings. Looking for |section class=""article_body""| should do - every post has this. And of course, we want it to only process the pages it hasn't encountered before in each new round - no need to extract the same data over and over again, it would just stack up expenses.
$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
    -|setMaxToCrawl(1000000)
    -|setMaxToProcess(1000000)
    -|setRepeat(1)
    -|setMaxRounds(0)
    -|setPageProcessPatterns(['|section class=""article_body""|'])
    -|setOnlyProcessIfNew(1)
Before finishing up with the crawljob configuration, there's just one more important parameter we need to add - the crawl pattern. When passing in a seed URL to the Crawl API, the Crawljob will traverse all subdomains as well. So if we pass in http://sitepoint.com, Crawlbot will look through http://community.sitepoint.com, and the now outdated http://reference.sitepoint.com - this is something we want to avoid, as it would slow our crawling process dramatically, and harvest stuff we don't need (we don't want the forums indexed right now). To set this up, we use the setUrlCrawlPatterns method, indicating that crawled links must start with sitepoint.com.
$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
    -|setMaxToCrawl(1000000)
    -|setMaxToProcess(1000000)
    -|setRepeat(1)
    -|setMaxRounds(0)
    -|setPageProcessPatterns(['|section class=""article_body""|'])
    -|setOnlyProcessIfNew(1)
	-|setUrlCrawlPatterns(['^http://www.sitepoint.com', '^http://sitepoint.com'])
Now we need to tell the job which API to use for processing. We could use the default - Analyze API - which would make Diffbot auto-determine the structure of the data we're trying to obtain, but I prefer specificity and want it to know outright that it should only produce articles.
$api = $diffbot-|createArticleAPI('crawl')-|setMeta(true)-|setDiscussion(false);
$job-|setApi($api);
Note that with the individual APIs (like Product, Article, Discussion, etc..) you can process individual resources even with the free demo token from Diffbot.com, which lets you test out your links and see what data they'll return before diving into bulk processing via Crawlbot. For information on how to do this, see the README file.
The job is now configured, and we can call() Diffbot with instructions on how to create it:
$job-|call();
The full code for creating this job is:
$diffbot = new Diffbot('my_token');
$job = $diffbot-|crawl('sp_search');

$job
    -|setSeeds(['http://sitepoint.com'])
    -|notify('bruno.skvorc@sitepoint.com')
    -|setMaxToCrawl(1000000)
    -|setMaxToProcess(1000000)
    -|setRepeat(1)
    -|setMaxRounds(0)
    -|setPageProcessPatterns(['|section class=""article_body""|'])
    -|setOnlyProcessIfNew(1)
    -|setApi($diffbot-|createArticleAPI('crawl')-|setMeta(true)-|setDiscussion(false))
    -|setUrlCrawlPatterns(['^http://www.sitepoint.com', '^http://sitepoint.com']);

$job-|call();
Calling this script via command line (php job.php) or opening it in the browser has created the job - it can be seen in the Crawlbot dev screen:
It'll take a while to finish (days, actually - SitePoint is a huge place), but all subsequent rounds will be faster because we told the job to only process pages it hasn't encountered before.
Searching
To search a dataset, we need to use the Search API. A dataset can be used even before it's complete - the Search API will simply search through the data it has, ignoring the fact that it doesn't have everything.
To use the search API, one needs to create a new search instance with a search query as a constructor parameter:
$search = $diffbot-|search('author:""Bruno Skvorc""');
$search-|setCol('sp_search');
$result = $search-|call();
The setCol method is optional, and if omitted will make the Search API go through all the collections under a single Diffbot token. As I have several collections from my previous experiments, I opted to specify the last one we created: sp_search (collections share names with the jobs that created them).
The returned data can be iterated over, and every element will be an instance of Article. Here's a rudimentary table exposing links and titles:
|table|
    |thead|
    |tr|
        |td|Title|/td|
        |td|Url|/td|
    |/tr|
    |/thead|
    |tbody|
    |?php
    foreach ($search as $article) {
        echo '|tr|';
        echo '|td|' . $article-|getTitle() . '|/td|';
        echo '|td||a href=""' . $article-|getResolvedPageUrl() . '""|Link|/a||/td|';
        echo '|/tr|';
    }
    ?|
    |/tbody|
|/table|
The Search API can return some amazingly fine tuned result sets. The query param will accept everything from common keywords, to date ranges, to targeted specific fields (like title:diffbot) to boolean combinations of various parameters, like type:article AND title:robot AND (overlord OR butler), producing all articles that have the word ""robot"" in the title and either the word ""overlord"" or ""butler"" in any of the fields (title, body, meta tags, etc). We'll be taking advantage of all this advanced functionality in the next post as we build our search engine's GUI.
We can also get the ""meta"" information about a Search API request by passing true into the call() after making the original call:
$info = $search-|call(true);
dump($info);
The result we get back is a SearchInfo object with the values as shown below (all accessible via getters):
With SearchInfo, you get access to the speed of your request, the number of hits (not the returned results, but total number - useful for pagination), etc.
To get information about a specific crawljob, like finding out its current status, or how many pages were crawled, processed, etc, we can call the crawl API again and just pass in the same job name. This, then, works as a read only operation, returning all the meta info about our job:
dump($diffbot-|crawl('sp_search')-|call());
At this point, we've got our collection being populated with crawled data from SitePoint.com. Now all we have to do is build a GUI around the Search functionality of the Diffbot API client, and that's exactly what we're going to be doing in the next part.
Conclusion
In this tutorial, we looked at Diffbot's ability to generate collections of structured data from websites of arbitrary format, and its Search API which can be used as the search engine behind a crawled site. While the price might be somewhat over the top for the average solo developer, for teams and companies this tool is a godsend.
Imagine being a media conglomerate with dozens or hundreds of different websites under your belt, and wanting a directory of all your content. Consolidating the efforts of all those backend teams to not only come up with a way to merge the databases but also find the time to do it in their daily efforts (which include keeping their outdated websites alive) would be an impossible and ultra expensive task, but with Diffbot, you unleash Crawlbot on all your domains and just use the Search API to traverse what was returned. What's more, the data you crawl is downloadable in full as a JSON payload, so even if it gets too expensive, you can always import the data into your own solution later on.
It's important to note that not many websites agree with being crawled, so you should probably look at their terms of service before attempting it on a site you don't own - crawls can rack up people's server costs rather quickly, and by stealing their content for your personal use without approval, you also rob them of potential ad revenue and other streams of income connected with the site.
In part 2, we'll look at how we can turn everything we've got so far into a GUI so that the average Joe can easily use it as an in-depth SitePoint search engine.
If you have any questions or comments, please leave them below!",-0.16892,https://www.sitepoint.com/crawling-searching-entire-domains-diffbot/,,
Diffbot lets developers navigate code the way our eyes see the world,d2011-08-25T00:00,Chikodi Chima,The Next Web,"Navigation,Diffbot,Code","Diffbot today announced the release a production version of its API for developers, which lets people navigate the hidden world of the Web visually. All a developer/application needs to do in order to leverage Diffbot is submit a URL and he or she can see when content has changed on a website, or easily understand the different sections of a website, such as important text, advertisements and headlines. Diffbot also helps to distinguish context of material, so that Apple the computer maker is clearly differentiated from Apple the fruit based on other nearby articles.
Diffbot has two API offerings, On Demand and Follow. On Demand was created to analyze home pages and index pages using the common layout markers such as headlines, bylines, and images, with a sepate feature set that can extract clean text and images from web pages. Follow tracks chanegs that are made to a web page, and any updates, similar to an RSS feed. With Diffbot it’s easy for a developer to follow only the part of the page he is most interested in, and easily extract the metadata organized in a meaningful manner.
Like many powerful technologies, Diffbot emerged from a rather simple idea. “I was taking 8 CS courses one quarter, and created Diffbot as a tool for monitoring my class webpages,” says creator Mike Tung. “Anytime a professor posted a new homework assignment, lecture, or announcement, my phone would buzz and show me the new content. My friends wondered how I was always informed about everything in real-time and asked if they could use it, too. I realized, during my work in AI at Stanford, that techniques in computer vision and machine learning could be used to generalize my algorithm to not just analyzing class webpages, but any page on the web.”
Diffbot already has significant traction, and is being used by AOL Editions, which touts itself as “The magazine that reads you,” to extract user recommendations based on interactions with different content. Hacker News Radio is an Internet radio station for the blind that leverages Diffbot to allow users to hear a webpage’s content while avoiding extraneous information such as privacy policies and other non-crucial data.
Diffbot was launched from Stanford’s StartX program by Tung and co-founder Leith Abdullah, both on leave of absence from PhD programs at the school.",0.04333,https://thenextweb.com/apps/2011/08/25/diffbot-lets-developers-navigate-code-the-way-our-eyes-see-the-world/,,
How does Diffbot work?,d2016-12-06T00:00,John Davi,quora.com,"Cascading Style Sheets,background process,What This Means,Our Approach,Asynchronous JavaScript and XML,Diffbot,machine learning,et cetera,JavaScript Object Notation,application programming interface","Background: Diffbot's been working on automatic extraction of web data since 2010, and provides a number of on-demand APIs to automatically structure articles and blog posts, products, discussion threads, video pages, etc. -- you can test them out and/or sign-up for a free trial account at http://www.diffbot.com.
Our approach relies on computer-vision techniques (in conjunction with machine learning, NLP and markup inspection) as the primary engine in identifying the proper content to extract from a page. What this means: when analyzing a web page or submitted document, our system renders the page fully, as it appears in a browser -- including images, CSS, even Ajax-delivered content -- and then analyzes its visual layout.
A full rendering allows the page to be broken-down into its constituent visual components. Then using machine-learning-trained algorithms, these elements will be weighted for their likelihood in being various components of a page: title, date, author, product price(s), related image(s), full text, sharing icons, next-page link, breadcrumb, comment, etc. As part of this the content and markup within and surrounding each element will also be evaluated to help further identify the correct elements. Within each of these, again our machine-learning-trained algorithms will be used to identify likelihood of each block based on element content, surrounding markup, etc.
Finally the unrelated components will be discarded and the identified elements will be processed (extraneous text or inline elements removed; HTML normalized; date normalized; image-headers scanned; etc.). Then these are collected and returned as a JSON response.
A visual approach allows for better identification of extraneous content (inline advertising links / links to related content; sharing links; attribution elements; etc.) and the difficult-to-identify metadata (author, date, title, etc.) and to work very well on non-English pages, since visual structure of a page tends to be similar regardless of a page's written language. For instance, on the following Chinese-language article (http://www.worldjournal.com/view...), a visual-based extraction works very well:",,https://www.quora.com/How-does-Diffbot-work,,
"Diffbot Releases Product Pages API, Uses Robot Learning To Supercharge Shopping And Collecting Sites",d2013-07-31T00:00,Darrell Etherington,TechCrunch,,"Diffbot is a startup that’s trying to make sense of the mass of information available on the web via robotic vision and computer learning, and it’s doing so one chunk at a time. Previously, the company released a comprehensive API for identifying and deriving key info from article pages on the web, and now it’s launching a Product Page API to do the same for ecommerce and shopping sites.
The new API will allow Diffbot to crawl the web and parse information such as price, discounts, shipping, images, descriptions and SKUs, and then translate that into an immediately usable database format for devs to mine and repurpose however they wish. This is incredible useful for comparison shopping sites, for instance, but Diffbot CEO and founder Mike Tung says they’ve also had a lot of interest in the product from collecting, bookmarking and listing sites similar to Pinterest.
“Product discovery type services where the users themselves are submitting links to products [is a use case],” he said. “We did some data analysis last year and 8 percent of the links that people are sharing on Twitter are products, and there are a lot of sites where the entire concept of the site is just to share links to products with other users on the site. With the product API now it’s not just a link, with a picture; you know the price and all the product details.”
Like the Articles API before it, Diffbot will offer the Product API on a usage-based software-as-a-service model, and this should allow everyone from small companies just starting out to big brands to take advantage. Diffbot’s current clients include AOL (disclosure: they own TC), as well as Betaworks, CBS Interactive, StumbleUpon and more. The Product API opens up a whole new category of potential customer for the Palo Alto-based startup, which has raised just over $2 million to date and is not currently looking around for anymore, according to Tung, as they’re already happy with their own current revenue being generated by products.
Diffbot plans to release a whole slew of APIs to target different page categories, and Tung says that the engine behind it can easily learn new categories without much in the way of additional engineering. Preparing a new category for general release involves helping the Diffbot robotic brain to essentially learn to spot pertinent information on its own, and that means talking to stakeholders to identify exactly what kind of information they should be looking for. Sometimes that’s obvious, as with price and description for products, but other, like SKU and manufacturer ID are less so.
Adding to Diffbot’s existing library of Home page, Article page and Image page identification APIs, the Product page release is a key new addition to its platform, and one that should see high demand. Diffbot’s progress is impressive, and this is definitely a startup to watch as it continues to lay pipes working in the background to identify and make sense of the Internet’s mass of available data.",,https://techcrunch.com/2013/07/31/diffbot-releases-product-pages-api-uses-robot-learning-to-supercharge-shopping-and-collecting-sites/,,
Launching the Largest Database of Human Knowledge: Diffbot Knowledge Graph,d2018-08-30T11:00,,Business Wire,"data,artificial intelligence,Mountain View,database,business,Diffbot,World Wide Web,knowledge,Knowledge Graph,launch","MOUNTAIN VIEW, Calif.--(BUSINESS WIRE)--Diffbot today announced the launch of Diffbot Knowledge Graph (DKG): all of the knowledge on the Web, collected and connected into a single, structured source of data, answers, insights, and truth. Using a sophisticated combination of machine learning, computer vision, and natural language processing, the DKG is a fully autonomous, AI-curated database of more than 1 trillion facts and 10 billion entities. This represents a repository of knowledge that is nearly 500 times larger than the Google Knowledge Graph, and growing every day.
“What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
Tweet this
Diffbot is the first company to turn broad-application Artificial Intelligence into a profitable business, powering applications for customers including Salesforce, Cisco, eBay, Yandex, and more. Far from a theoretical research project in search of a business application, Artificial Intelligence is the backbone of Diffbot and the company uses state of the art AI methods to deploy profitable products at scale while also furthering the field by funding extensive research.
In contrast to other solutions marketed as Knowledge Graphs, the DKG is:
Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labor.
Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models.
Web-wide, regardless of originating language. Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as English.
Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.
Starting today, any business that wants instant access to all of the world’s knowledge can simply sign up for the DKG and turn the entire Web into their personal database for business intelligence across:
People: skills, employment history, education, social profiles
Companies: rich profiles of companies and the workforce globally, from Fortune 500 to SMB’s
Locations: mapping data, addresses, business types, zoning information
Articles: Every news article, dateline, byline from anywhere on the Web, in any language
Products: pricing, specifications, and, reviews for every SKU across major ecommerce engines and individual retailers
Discussions: chats, social sharing, and conversations everywhere from article comments to web forums like Reddit
Images: billions of images on the web organized using image recognition and meta data collection
“A Web-wide, comprehensive, and interconnected Knowledge Graph has the power to transform how enterprises do business. Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers,” said Mike Tung, founder and CEO of Diffbot. “What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
DKG data is easily accessible to businesses and can be integrated via API into any internal business process or application, from business intelligence and analytics to marketing campaigns and CRM systems. Users can also create custom queries using Diffbot’s DQL syntax. Users simply enter a query and the DKG instantly generates a comprehensive set of results with every single item on the internet that relates to it, with links to all existing connections between those results. Results can be viewed in a list, map or table layout, with the ability to easily expand or refine results based on connections captured by the Knowledge Graph.
“Simply put, Diffbot is using the power of AI on a scale we’ve never seen before,” said Aydin Senkut, founder and managing director of Felicis Ventures, one of Diffbot’s investors. “It’s the first profitable AI company on record, they are the ‘secret ingredient’ powering applications from many of the largest companies in tech, and the launch of the Knowledge Graph is going to further elevate Diffbot’s status as a clear leader in the space.”
Companies interested in accessing the Diffbot Knowledge Graph can contact Diffbot at sales@diffbot.com or visit www.diffbot.com/knowledge-graph
About Diffbot:
The first profitable AI startup on record, Diffbot provides knowledge-as-a-service to power intelligent applications for some of the world’s most prolific tech companies. It uses AI, computer vision, machine learning and natural language processing to provide businesses and developers with tools to effectively extract and understand facts from any web page. Diffbot also built the world’s first true knowledge graph, enabling businesses to extract valuable insights from all of the information on the web in milliseconds. The company is based in Mountain View, Calif.",,https://www.businesswire.com/news/home/20180830005204/en/Launching-Largest-Database-Human-Knowledge-Diffbot-Knowledge,,
Diffbot: Crawling with Visual Machine Learning,d2014-07-27T00:00,Bruno Skvorc,SitePoint,"Diffbot,Machine learning,Source code,Directory (computing),Uniform Resource Locator,News aggregator,Application programming interface","Have you ever wondered how social networks do URL previews so well when you share links? How do they know which images to grab, whom to cite as an author, or which tags to attach to the preview? Is it all crawling with complex regexes over source code? Actually, more often than not, it isn't. Meta information defined in the source can be unreliable, and sites with less than stellar reputation often use them as keyword carriers, attempting to get search engines to rank them higher. Isn't what we, the humans, see in front of us what matters anyway?
If you want to build a URL preview snippet or a news aggregator, there are many automatic crawlers available online, both proprietary and open source, but you seldom find something as niche as visual machine learning. This is exactly what Diffbot is - a ""visual learning robot"" which renders a URL you request in full and then visually extracts data, helping itself with some metadata from the page source as needed.
After covering some theory, in this post we'll do a demo API call at one of SitePoint's posts.
PHP Library
The PHP library for Diffbot is somewhat out of date, and as such we won't be using it in this demo. We'll be performing raw API calls, and in some future posts we'll build our own library for API interaction.
If you'd like to take a look at the PHP library nonetheless, see here, and if you're interested in libraries for other languages, Diffbot has a directory.
Update, July 2015: A PHP library has been developed since this article was published. See its entire development process here, or the source code here.
JavaScript Content
We said in the introductory section that Diffbot renders the request in full and then analyzes it. But, what about JavaScript content? Nowadays, websites often render some HTML above the fold, and then finish the CSS, JS, and dynamic content loading afterwards. Can the Diffbot API see that?
As a matter of fact, yes. Diffbot literally renders the page in full, and then inspects it visually, as explained in my StackOverflow Q&A here. There are some caveats, though, so make sure you read the answer carefully.
Pricing and API Health
Diffbot has several usage tiers. There's a free trial tier which kills your API token after 7 days or 10000 calls, whichever comes first. The commercial tokens can be purchased at various prices, and never expire, but do have limitations. A special case by case approach is afforded to open source and/or educational projects which provides an older model of the free token - 10k calls per month, once per second max, but never expires. You need to contact them directly if you think you qualify.
Diffbot guarantees a high uptime, but failures sometimes do happen - especially in the most resource intensive API of the bunch: Crawlbot. Crawlbot is used to crawl entire domains, not just individual pages, and as such has a lower reliability rate than other APIs. Not by a lot, but enough to be noticeable in the API Health screen - the screen you can check to see if an API is up and running or currently unavailable if your calls run into issues or return error 500.
Demo
To prepare your environment, please boot up a Homestead Improved instance.
Create a starter Laravel project by SSHing into the VM with vagrant ssh, going into the Code folder, and executing composer create-project laravel/laravel Laravel --prefer-dist. This will let you access the Laravel greeting page via http://homestead.app:8000 from the host's browser.
In app/routes.php add the following route:
Route::get('/diffbot', 'HomeController@diffbotDemo');
In app/controllers/HomeController add the following action:
public function diffbotDemo() {
        die(""hi"");
    }
If http://homestead.app:8000/diffbot now outputs ""hi"" on the screen, we're ready to start playing with the API.
To interact with the Diffbot API, you need a token. Sign up for one on their pricing page. For the sake of this demo, let's call our token $TOKEN, and we'll refer to it as such in URLs. Replace $TOKEN with your own value where appropriate.
We'll be using Guzzle as our HTTP client. It's not required, but I do recommend you get familiar with it through a past article of ours.
Add the ""guzzlehttp/guzzle"": ""4.1.*@dev"" to your composer.json so the require block looks like this:
""require"": {
		""laravel/framework"": ""4.2.*"",
        ""guzzlehttp/guzzle"": ""4.1.*@dev""
	},
In the project root, run composer update.
In the first example, we'll crawl a SitePoint post with the default Article API from Diffbot. To do this, we refer to the docs which do an excellent job at explaining the workflow. Change the body of the diffbotDemo action to the following code:
public function diffbotDemo() {

        $token = ""$TOKEN"";
        $version = 'v3';

        $client = new GuzzleHttp\Client(['base_url' =| 'http://api.diffbot.com/']);

        $response = $client-|get($version.'/article', ['query' =| [
            'token' =| $token,
            'url' =| 'http://www.sitepoint.com/7-mistakes-commonly-made-php-developers/'
        ]]);

        die(var_dump($response-|json()));
    }
First, we set our token. Then, we define a variable that'll hold the API version. Next, it's up to us to create a new Guzzle client, and we also give it a base URL so we don't have to type it in every time we make another request.
Next up, we create a response object by sending a GET request to the API's URL, and we add in an array of query parameters in key =| value format. In this case, we only pass in the token and the URL, the most basic of parameters.
Finally, since the Diffbot API returns JSON data, we use Guzzle's json() method to automatically decode it into an array. We then pretty-print this data:
As you can see, we got some information back rather quickly. There's the icon that was used, a preview of the text, the title, even the language, date and HTML have been returned. You'll notice there's no author, however. Let's change this and request some more values.
If we add the ""fields"" parameter to the query params list and give it a value of ""tags"", Diffbot will attempt to extract tags/categories from the URL provided. Add this line to the query array:
'fields' =| 'tags'
and then change the die part to this:
$data = $response-|json();
die(var_dump($data['objects'][0]['tags']));
Refreshing the screen now gives us this:
But, the source code of the article notes several other tags:
Why is the result so very different? It's precisely due to the reason we mentioned at the end of the very first paragraph of this post: what we humans see takes precedence. Diffbot is a visual learning robot, and as such its AI deducts the tags from the actual rendered content - what it can see - rather than from looking at the source code which is far too easily spiced up for SEO purposes.
Is there a way to get the tags from the source code, though, if one really needs them? Furthermore, can we make Diffbot recognize the author on SitePoint articles? Yes. With the Custom API.
The Custom API is a feature which allows you to not only tweak existing Diffbot API to your liking by adding new fields and rules for content extraction, but also allows you to create completely new APIs (accessed via a dedicated URL, too) for custom content processing.
Go to the dev dashboard and log in with your token. Then, go into ""Custom API"". Activate the ""Create a Rule"" tab at the bottom, and input the URL of the article we're crawling into the URL box, then click Test. Your screen should look something like this:
You'll immediately notice the Author field is empty. You can tweak the author-searching rule by clicking Edit next to it, and finding the Author element in the live preview window that opens, then click on it to get the desired result. However, due to some, well, less than perfect CSS on SitePoint's end, it's very difficult to provide Diffbot's API with a consistent path to the author name, especially by clicking on elements. Instead, add the following rule manually: .contributor--large .contributor_name a and click Save.
You'll notice the Preview window now correctly populates the Author field:
In fact, this new rule is automatically applied to all SitePoint links for your token. If you try to preview another SitePoint article, like this one, you'll notice Peter Nijssen is successfully extracted:
Ok, let's modify the API further. We need the article:tag values that are visible in source code. Doing this requires a two-step process.
A collection is exactly what it sounds like - a collection of values grabbed via a specific ruleset. We'll call our collection ""MetaTags"", and give it the following selector: meta[property=article:tag]. This means ""find all meta elements in the HTML that have the property attribute with the value article:tag"".
Collection fields are individual entries in a collection - in our case, the various tags. Click on ""Add a custom field to this collection"", and add the following values:
Click Save. You'll immediately have access to the list of Tags in the result window:
Change the final output of the diffbotDemo() action to this:
die(var_dump($data['objects'][0]['metaTags']));
If you now refresh the URL we tested with (http://homestead.app:8000/diffbot), you'll notice the author and meta tags values are there. Here's the output the above line of code produces:
We have our tags!
Conclusion
Diffbot is a powerful data extractor for the web - whether you need to consolidate many sites into a single search index without combining their back-ends, want to build a news aggregator, have an idea for a URL preview web component, or want to regularly harvest the contents of competitors' public pricing lists, Diffbot can help. With dead simple API calls and highly structured responses, you'll be up and running in next to no time. In a later article, we'll build a brand new API for using Diffbot with PHP, and redo the calls above with it. We'll also host the library on Packagist, so you can easily install it with Composer. Stay tuned!",-0.0979,http://www.sitepoint.com/diffbot-crawling-visual-machine-learning/,,
"Diffbot raises $2 million to help apps understand the open, unstructured web",d2012-05-31T14:27,Ben Popper,The Verge,"Machine learning,Understanding,Diffbot,World Wide Web,Mobile app","Diffbot is a machine learning system that can read any web page and extract useful, structured data from it. Today it announced a $2 million series A round of funding from an impressive list of names, including the founders of Earthlink and Sun Microsystems alongside executives from Twitter, Facebook and AOL.
The main function of Diffbot is to turn the open web into an easily digestible API. So for example, AOL is one of Diffbot's clients. It has numerous online media properties using different content management systems. Rather than trying to figure out how to organize all of that, it uses Diffbot to read new articles from all of those properties and extract that data into one simple API. That way its new tablet magazine app, Editions, can pull stories from across all AOL properties and display them on the iPad in real time.
""Diffbot understands a web page no matter how often it is redesigned.""
So far Diffbot has been limited to understanding front pages and article pages, but founder Michael Tung told us that the company is now expanding to cover a much wider range. ""So for example we can now understand recipes,"" Tung explained. ""So you could build an app that lets users bookmark recipes from any page on the web, and Diffbot could pull the recipe and break it down into ingredients and instructions.""
Right now the company has two basic services. It can scan URLs that a customer sends them or it can monitor a URL for a customer and alert them to changes, something Tung says many clients are using to keep an eye on their competition. The company works on a freemium model, with the first 10,000 API calls per month being free and tiered pricing after that. ""Right now we are processing over 100 million API calls per month,"" said Tung.
The new funds, says Tung, will be used to scale the companies servers to keep up with demand and hire machine learning experts in a crowded and competitive marketplace.",,http://www.theverge.com/2012/5/31/3054444/diffbot-raises-2-million-apps-open-web,,
Diffbot: Crawling with Visual Machine Learning,d2014-07-27T00:00,Bruno Skvorc,SitePoint,"Diffbot,Machine learning,Source code,Directory (computing),Uniform Resource Locator,News aggregator,Application programming interface","Have you ever wondered how social networks do URL previews so well when you share links? How do they know which images to grab, whom to cite as an author, or which tags to attach to the preview? Is it all crawling with complex regexes over source code? Actually, more often than not, it isn’t. Meta information defined in the source can be unreliable, and sites with less than stellar reputation often use them as keyword carriers, attempting to get search engines to rank them higher. Isn’t what we, the humans, see in front of us what matters anyway?
If you want to build a URL preview snippet or a news aggregator, there are many automatic crawlers available online, both proprietary and open source, but you seldom find something as niche as visual machine learning. This is exactly what Diffbot is – a “visual learning robot” which renders a URL you request in full and then visually extracts data, helping itself with some metadata from the page source as needed.
After covering some theory, in this post we’ll do a demo API call at one of SitePoint’s posts.
PHP Library
The PHP library for Diffbot is somewhat out of date, and as such we won’t be using it in this demo. We’ll be performing raw API calls, and in some future posts we’ll build our own library for API interaction.
If you’d like to take a look at the PHP library nonetheless, see here, and if you’re interested in libraries for other languages, Diffbot has a directory.
Update, July 2015: A PHP library has been developed since this article was published. See its entire development process here, or the source code here.
JavaScript Content
We said in the introductory section that Diffbot renders the request in full and then analyzes it. But, what about JavaScript content? Nowadays, websites often render some HTML above the fold, and then finish the CSS, JS, and dynamic content loading afterwards. Can the Diffbot API see that?
As a matter of fact, yes. Diffbot literally renders the page in full, and then inspects it visually, as explained in my StackOverflow Q&A here. There are some caveats, though, so make sure you read the answer carefully.
Pricing and API Health
Diffbot has several usage tiers. There’s a free trial tier which kills your API token after 7 days or 10000 calls, whichever comes first. The commercial tokens can be purchased at various prices, and never expire, but do have limitations. A special case by case approach is afforded to open source and/or educational projects which provides an older model of the free token – 10k calls per month, once per second max, but never expires. You need to contact them directly if you think you qualify.
Diffbot guarantees a high uptime, but failures sometimes do happen – especially in the most resource intensive API of the bunch: Crawlbot. Crawlbot is used to crawl entire domains, not just individual pages, and as such has a lower reliability rate than other APIs. Not by a lot, but enough to be noticeable in the API Health screen – the screen you can check to see if an API is up and running or currently unavailable if your calls run into issues or return error 500.
Demo
To prepare your environment, please boot up a Homestead Improved instance.
Create a starter Laravel project by SSHing into the VM with vagrant ssh, going into the Code folder, and executing composer create-project laravel/laravel Laravel --prefer-dist. This will let you access the Laravel greeting page via http://homestead.app:8000 from the host’s browser.
Add a Route and Action
In app/routes.php add the following route:
Route::get('/diffbot', 'HomeController@diffbotDemo');
In app/controllers/HomeController add the following action:
public function diffbotDemo() {
        die(""hi"");
    }
If http://homestead.app:8000/diffbot now outputs “hi” on the screen, we’re ready to start playing with the API.
To interact with the Diffbot API, you need a token. Sign up for one on their pricing page. For the sake of this demo, let’s call our token $TOKEN, and we’ll refer to it as such in URLs. Replace $TOKEN with your own value where appropriate.
We’ll be using Guzzle as our HTTP client. It’s not required, but I do recommend you get familiar with it through a past article of ours.
Add the ""guzzlehttp/guzzle"": ""4.1.*@dev"" to your composer.json so the require block looks like this:
""require"": {
		""laravel/framework"": ""4.2.*"",
        ""guzzlehttp/guzzle"": ""4.1.*@dev""
	},
In the project root, run composer update.
In the first example, we’ll crawl a SitePoint post with the default Article API from Diffbot. To do this, we refer to the docs which do an excellent job at explaining the workflow. Change the body of the diffbotDemo action to the following code:
public function diffbotDemo() {

        $token = ""$TOKEN"";
        $version = 'v3';

        $client = new GuzzleHttp\Client(['base_url' => 'http://api.diffbot.com/']);

        $response = $client->get($version.'/article', ['query' => [
            'token' => $token,
            'url' => 'http://www.sitepoint.com/7-mistakes-commonly-made-php-developers/'
        ]]);

        die(var_dump($response->json()));
    }
First, we set our token. Then, we define a variable that’ll hold the API version. Next, it’s up to us to create a new Guzzle client, and we also give it a base URL so we don’t have to type it in every time we make another request.
Next up, we create a response object by sending a GET request to the API’s URL, and we add in an array of query parameters in key => value format. In this case, we only pass in the token and the URL, the most basic of parameters.
Finally, since the Diffbot API returns JSON data, we use Guzzle’s json() method to automatically decode it into an array. We then pretty-print this data:
As you can see, we got some information back rather quickly. There’s the icon that was used, a preview of the text, the title, even the language, date and HTML have been returned. You’ll notice there’s no author, however. Let’s change this and request some more values.
If we add the “fields” parameter to the query params list and give it a value of “tags”, Diffbot will attempt to extract tags/categories from the URL provided. Add this line to the query array:
'fields' => 'tags'
and then change the die part to this:
$data = $response->json();
die(var_dump($data['objects'][0]['tags']));
Refreshing the screen now gives us this:
But, the source code of the article notes several other tags:
Why is the result so very different? It’s precisely due to the reason we mentioned at the end of the very first paragraph of this post: what we humans see takes precedence. Diffbot is a visual learning robot, and as such its AI deducts the tags from the actual rendered content – what it can see – rather than from looking at the source code which is far too easily spiced up for SEO purposes.
Is there a way to get the tags from the source code, though, if one really needs them? Furthermore, can we make Diffbot recognize the author on SitePoint articles? Yes. With the Custom API.
The Custom API is a feature which allows you to not only tweak existing Diffbot API to your liking by adding new fields and rules for content extraction, but also allows you to create completely new APIs (accessed via a dedicated URL, too) for custom content processing.
Go to the dev dashboard and log in with your token. Then, go into “Custom API”. Activate the “Create a Rule” tab at the bottom, and input the URL of the article we’re crawling into the URL box, then click Test. Your screen should look something like this:
You’ll immediately notice the Author field is empty. You can tweak the author-searching rule by clicking Edit next to it, and finding the Author element in the live preview window that opens, then click on it to get the desired result. However, due to some, well, less than perfect CSS on SitePoint’s end, it’s very difficult to provide Diffbot’s API with a consistent path to the author name, especially by clicking on elements. Instead, add the following rule manually: .contributor--large .contributor_name a and click Save.
You’ll notice the Preview window now correctly populates the Author field:
In fact, this new rule is automatically applied to all SitePoint links for your token. If you try to preview another SitePoint article, like this one, you’ll notice Peter Nijssen is successfully extracted:
Ok, let’s modify the API further. We need the article:tag values that are visible in source code. Doing this requires a two-step process.
A collection is exactly what it sounds like – a collection of values grabbed via a specific ruleset. We’ll call our collection “MetaTags”, and give it the following selector: meta[property=article:tag]. This means “find all meta elements in the HTML that have the property attribute with the value article:tag“.
Collection fields are individual entries in a collection – in our case, the various tags. Click on “Add a custom field to this collection”, and add the following values:
Click Save. You’ll immediately have access to the list of Tags in the result window:
Change the final output of the diffbotDemo() action to this:
die(var_dump($data['objects'][0]['metaTags']));
If you now refresh the URL we tested with (http://homestead.app:8000/diffbot), you’ll notice the author and meta tags values are there. Here’s the output the above line of code produces:
We have our tags!
Conclusion
Diffbot is a powerful data extractor for the web – whether you need to consolidate many sites into a single search index without combining their back-ends, want to build a news aggregator, have an idea for a URL preview web component, or want to regularly harvest the contents of competitors’ public pricing lists, Diffbot can help. With dead simple API calls and highly structured responses, you’ll be up and running in next to no time. In a later article, we’ll build a brand new API for using Diffbot with PHP, and redo the calls above with it. We’ll also host the library on Packagist, so you can easily install it with Composer. Stay tuned!",,http://www.sitepoint.com/diffbot-crawling-visual-machine-learning/,,
Is there any open-source web scraping tool such as Scrapinghub or DiffBot?,d2018-01-08T00:00,,quora.com,"THIS IS PERFECT LTD.,tool,open-source software,open-source model,web scraping,structured programming,website,Source,application programming interface,kimono","I’ve gotta recommend Kimono here (Turn websites into structured APIs from your browser in seconds). While it’s not open source, it is a free web scraping tool where you can click on the data properties you want rather than having to define the selectors in the code. It’s very easy to use (one of our founders’ mom can make an api in about 60 seconds!) and lets you scrape almost any site. Kimono even hosts your data for you and lets you update whenever you tell it to.
This is perfect if you have one site with a list of hundreds of links to blog articles, and you want the content inside each article. Kimono lets you scrape the list and use it as a source of links for another API to crawl. You just have to make one API out of the detail and set the list of links from the index API, and then you’re done! (Fox vs. CNN: Who's got Obama on the mind?)
I should tell you that I work for Kimono, but
I truly think the product is awesome. It’s really easy to use and it can save you a ton of work.",,https://www.quora.com/Is-there-any-open-source-web-scraping-tool-such-as-Scrapinghub-or-DiffBot,,
Powerful Custom Entities with the Diffbot PHP Client,d2015-10-30T00:00,Bruno Skvorc,SitePoint,"custom,PHP,Custom,lithium,entity,SitePoint,Diffbot,client,application programming interface,lexical analysis","A while back, we looked at Diffbot, the machine learning AI for processing web pages, as a means to extract SitePoint author portfolios. That tutorial focused on using the Diffbot UI only, and consuming the API created would entail pinging the API endpoint manually. Additionally, since then, the design of the pages we processed has changed, and thus the API no longer reliably works.
In this tutorial, apart from rebuilding the API so that it works again, we’ll use the official Diffbot client to build custom entities that correspond to the data we seek (author portfolios).
Bootstrapping
We’ll be using Homestead Improved as usual. The following few commands will bootstrap the Vagrant box, create the project folder, and install the Diffbot client.
git clone https://github.com/swader/homestead_improved hi_diffbot_authorfolio; cd hi_diffbot_authorfolio
./bin/folderfix.sh
vagrant up; vagrant ssh
mkdir -p Code/Project/public; cd Code/Project; touch public/index.php
composer require swader/diffbot-php-client
Additionally, we can install Symfony’s vardumper as a development requirement, just to get prettier debug outputs.
composer require symfony/var-dumper --dev
If we now give index.php the following content, provided we added homestead.app to our host machine’s /etc/hosts file, we should see “Hello world” if we visit http://homestead.app in our browser:
<?php
// index.php

require '../vendor/autoload.php';

echo ""Hello World"";
Diffbot Initialization
Note that to follow along, you’ll need a free Diffbot token – get one here.
define('TOKEN', 'token');
use Swader\Diffbot\Diffbot;

$d = new Diffbot(TOKEN);
This is all we need to init Diffbot. Let’s test it on a sample article.
echo $d->createArticleAPI('http://www.sitepoint.com/crawling-searching-entire-domains-diffbot')->call()->getAuthor(); // Bruno Skvorc
Custom API
First, we need to rebuild our API from the last post, so that it can become operational again. We do this by logging into the dev panel and going to https://www.diffbot.com/dev/customize/.
Let’s create a new API:
After entering a sample URL like www.sitepoint.com/author/bskvorc/, we can add some custom fields, like author:
We can use this same approach to define fields like bio, and nextPage, in order to activate Diffbot’s automatic pagination:
We also need to define a collection which would gather all the article cards and process them. Making a collection entails selecting an element the selector of which is repeated multiple times. In our case, that’s the li element of the .article-list class.
Within that collection, we define fields for each card (when in doubt, the browser’s dev tools can help us identify the classes and elements we need to specify as selectors to get the desired result):
Besides title and primary category, we should also to extract the date of publication, primary category URL, article URLs, number of likes, etc. For the sake of brevity, we’ll skip defining those here.
If we now access our endpoint directly rather than in the API toolkit, we should get the fully merged 9 pages of posts back, processed just the way we want them.
http://api.diffbot.com/v3/diffpoint?token=token&url=http://www.sitepoint.com/author/bskvorc/
We can see that the API successfully found all the pages in the set and returned even the oldest of posts.
Extending the Client
Let’s see if the Custom API behaves as expected.
echo $d->createCustomAPI('http://www.sitepoint.com/author/bskvorc', 'diffpoint')->call()->getBio();
This should echo the correct bio.
This step is, in a way, optional. We could consume the returned data as is, and just iterate through keys and arrays, but let’s pretend our data is much more complex than a simple portfolio page and do it right regardless.
We need two new classes: an Entity Factory, and an Entity. Let’s create them at /src/AuthorFolio.php and src/CustomFactory.php, relative to the root of our project (src is in the root folder).
Let’s start with the new entity. As per the docs, we have an abstract class we can extend.
<?php

// src/AuthorFolio.php

namespace My\Custom;

use Swader\Diffbot\Abstracts\Entity;

class AuthorFolio extends Entity
{

}
We extend the abstract entity and give our new entity its own namespace. This is optional, but useful. At this point, the entity would already be usable – it is essentially identical to the Wildcard entity which uses magic methods to resolve requests for various properties of the returned data (which is why the getBio method in the example above worked without us having to define anything). But the goal is to have the AuthorFolio class verbose, with support for custom, SitePoint-specific data and maybe some shortcut methods. Let’s do this now.
The API will return the full list of an author’s articles – but not their count. To find out how many posts an author has, we’d have to count the articles property, so let’s wrap that process in a shortcut method. We can also tell PHPStorm that the class will have an articles property using the @property tag, so it stops complaining about accessing the field with magic methods:
<?php

// src/AuthorFolio.php

namespace My\Custom;

use Swader\Diffbot\Abstracts\Entity;

/**
 * Class AuthorFolio
 * @property array articles
 * @package My\Custom
 */
class AuthorFolio extends Entity
{
    public function getType()
    {
        return 'authorfolio';
    }

    public function getNumPosts()
    {
        return count($this->articles);
    }
}
Other methods we could define are totalLikes, activeSince, favoredCategory, etc.
The entity being ready, it’s time to define a custom factory to bind it to the type of return data we’re getting from our custom API. We’re writing an alternative to the default factory, but the original class already contains some methods we can use – it’s designed to be reused by its children. As such, we merely need to extend the original, map the new type to our custom entity, and we’re done.
<?php

// src/CustomFactory.php

namespace My\Custom;

use Swader\Diffbot\Factory\Entity;

class CustomFactory extends Entity
{
    public function __construct()
    {
        $this->apiEntities = array_merge(
            $this->apiEntities,
            ['diffpoint' => '\My\Custom\AuthorFolio']
        );
    }
}
We merged the original API-to-entity list with our own custom binding, thereby telling the Factory class to both keep an eye on the standard types and APIs, and our new ones. This means we can keep using this factory for default Diffbot APIs as well.
To make our classes autoloadable, we should probably add them to composer.json:
  ""autoload"": {
    ""psr-4"": {
      ""My\\Custom\\"": ""src""
    }
  }
We activate these new autoload mappings by running composer dump-autoload.
Next, we instantiate the new factory, plug it into our Diffbot instance, and test the API:
$d = new Diffbot(TOKEN);

$d->setEntityFactory(new My\Custom\CustomFactory());

$api = $d->createCustomAPI('http://www.sitepoint.com/author/bskvorc', 'diffpoint');
$api->setTimeout(120000);

$result = $api->call();

dump($result->getNumPosts());
Note that we increased the timeout because a heavily paginated set of posts can take a while to render on Diffbot’s end.
Conclusion
In this tutorial, by using the official Diffbot client, we constructed custom entities and built a custom API which returns them. We saw how easy it is to leverage machine learning and optical content processing for grabbing arbitrary data from websites of any type, and we saw how heavily customizable the Diffbot client is.
While this was a rather simple example, it isn’t difficult to imagine advanced use cases on more complex entities, or perhaps several of them spread over multiple APIs, all processed through a single EntityFactory, each custom API corresponding to a special Entity type. With a well trained visual neural network, the only processing limit is one’s imagination.
If you’d like to read more about the Diffbot client, check out the full docs and play around for yourself – just don’t forget to fetch a fresh free two-week demo token!
Bruno is a blockchain developer and code auditor from Croatia with Master’s Degrees in Computer Science and English Language and Literature. He's been a web developer for 10 years until JavaScript drove him away. He now runs a cryptocurrency business at Bitfalls.com via which he makes blockchain tech approachable to the masses, and runs Coinvendor, an on-boarding platform for people to easily buy cryptocurrency. He’s also a developer evangelist for Diffbot.com, a San Francisco-based AI-powered machine vision web scraper.",,http://www.sitepoint.com/powerful-custom-entities-with-the-diffbot-php-client/,,
Diffbot: Repeated Collections and Merged APIs,d2014-08-22T00:00,Bruno Skvorc,SitePoint,"Diffbot,Uniform Resource Locator,SitePoint,Application programming interface","In the previous post on Analyzing SitePoint Authors' Profiles with Diffbot we built a Custom API that automatically paginates an author's list of work and extracts his name, bio and a list of posts with basic data (URL, title and date stamp). In this post, we'll extract the links to the author's social networks.
Introduction
If you look at the social network icons inside an author's bio frame on their profile page, you'll notice they vary. There can be none, or there can be eight, or anything in between. What's worse, the links aren't classed in any semantically meaningful way - they're just links with an icon and a href attribute.
This makes turning them into an extractable pattern difficult, and yet that's exactly what we'll be doing here because hey, who doesn't love a challenge?
To get set up, please read and go through the first part. When you're done, re-enter the dev dashboard.
Repeated Collections Problem
The logical approach would be to define a new collection just like for posts, but one that targets the social network links. Then, just target the href attribute on each and we're set, right? Nope.
Observe below:
As you can see, we get all the social links. But we get them all X times, where X is the number of pages in an author's profile. This happens because the Diffbot API concatenates the HTML of all the pages into a single big one, and our collection rule finds several sets of these social network icon-links.
Intuition might lead you to use a :first-child pseudo element on the parent of the collection on the first page, but the API doesn't work like that. The HTML contents of the individual pages are concatenated, yes, but the rules are executed on them first. In reality, only the result is being concatenated. This is why it isn't possible to use main:first-child to target the first page only. Likewise, at this moment the Diffbot API does not have any :first-page custom pseudo elements, but them appearing at a later stage is not out of the question. How, then, do we do this?
Custom Domain Regex and API Dupes
Diffbot allows you to define several custom rulesets for the same API endpoint, differing by domain regex. When an API endpoint is called, all the rulesets that match the URL are executed, the results are concatenated, and you get a unique set back, as if it was all in a single API. This is what we're going to do, too.
Start off by going to ""Create a rule"" and selecting a Custom API, so you get asked for a name. Enter the same name as the one in the first part (in my case, AuthorFolio). Enter the typical test url (http://sitepoint.com/author/bskvorc/) and run the Test. Then, change the domain regex to this:
(http(s)?://)?(.*\.)?sitepoint.com/author/[^/]+/
This tells the API to only target the first page of any author profile - it ignores pagination completely.
Next, define a new collection. Call it ""social"" and give it a custom field with the selector of .contributor_social li. Name the field ""link"", and give it a selector of ""a"" with an attribute filter of href. Save, wait for the reload, and notice that you now have the four links extracted:
But having just the links there kind of sucks, doesn't it? It would be nice if we had a social network name, too. SitePoint's design, however, doesn't class them in any semantically meaningful way, so there's no easy way to get the network name. How can we tackle this?
Regex Rewrite Filters to the rescue!
Custom fields have three available filters:
attribute: extracts an HTML element's attribute
ignore: ignores certain HTML elements based on a css selector
replace: replaces the content of the output with the given content if a regex pattern matches
We'll be using the third one - read more about them here.
Add a new field to our ""social"" collection. Give it the name ""network"", the selector a, and an attribute filter of href so it extracts the link just like the ""link"" field. Then, add a new ""replace"" filter.
SitePoint author profiles can have the following social networks attached to their profiles: Google+, Twitter, Facebook, Reddit, Youtube, Flickr, Github and Linkedin. Luckily, each of those has pretty straightforward URLs with full domain names, so regexing the names out is a piece of cake. The correct regex is ^.*KEYWORD.*$:
Save, wait for the reload, and notice that you now have a well formed collection of an author's social links.
Bringing the APIs together
Finally, let's fetch all this data at once. According to what we said above, executing a call to an author page with the AuthorFolio API should now give us a single JSON response containing the sum of everything we've defined so far, including the fields from the first post. Let's see if that's true. Visit the following link in your browser:
http://diffbot.com/api/AuthorFolio?token=xxxxxxxxx&url=http://www.sitepoint.com/author/bskvorc/
This is the result I get:
As you can see, we successfully merged the two APIs and got back a single result of everything we wanted. We can now consume this API URL at will from any third party application, and pull in the portfolio of an author, easily grouping by date, detecting changes in the bio, registering newly added social networks, and much more.
Conclusion
In this post we looked at some trickier aspects of visual crawling with Diffbot like repeated collections and duplicate APIs on custom domain regexes. We built an endpoint that allows us to extract valuable information from an author's profile, and we learned how to apply this knowledge to any similar situation.
Did you crawl something interesting using these techniques? Did you run into any trouble? Let us know in the comments below!",-0.3062,https://www.sitepoint.com/?p=87434,,
Diffbot: Repeated Collections and Merged APIs,d2014-08-22T00:00,Bruno Skvorc,SitePoint,"Diffbot,Pagination,Application programming interface","Minimal. Clean. Simple. Lightweight. Responsive.
Download Our FREE Base Theme!
In the previous post on Analyzing SitePoint Authors' Profiles with Diffbot we built a Custom API that automatically paginates an author's list of work and extracts his name, bio and a list of posts with basic data (URL, title and date stamp). In this post, we'll extract the links to the author's social networks.
Introduction
If you look at the social network icons inside an author's bio frame on their profile page, you'll notice they vary. There can be none, or there can be eight, or anything in between. What's worse, the links aren't classed in any semantically meaningful way - they're just links with an icon and a href attribute.
This makes turning them into an extractable pattern difficult, and yet that's exactly what we'll be doing here because hey, who doesn't love a challenge?
To get set up, please read and go through the first part. When you're done, re-enter the dev dashboard.
Repeated Collections Problem
The logical approach would be to define a new collection just like for posts, but one that targets the social network links. Then, just target the href attribute on each and we're set, right? Nope.
Observe below:
As you can see, we get all the social links. But we get them all X times, where X is the number of pages in an author's profile. This happens because the Diffbot API concatenates the HTML of all the pages into a single big one, and our collection rule finds several sets of these social network icon-links.
Intuition might lead you to use a :first-child pseudo element on the parent of the collection on the first page, but the API doesn't work like that. The HTML contents of the individual pages are concatenated, yes, but the rules are executed on them first. In reality, only the result is being concatenated. This is why it isn't possible to use main:first-child to target the first page only. Likewise, at this moment the Diffbot API does not have any :first-page custom pseudo elements, but them appearing at a later stage is not out of the question. How, then, do we do this?
Custom Domain Regex and API Dupes
Diffbot allows you to define several custom rulesets for the same API endpoint, differing by domain regex. When an API endpoint is called, all the rulesets that match the URL are executed, the results are concatenated, and you get a unique set back, as if it was all in a single API. This is what we're going to do, too.
Start off by going to ""Create a rule"" and selecting a Custom API, so you get asked for a name. Enter the same name as the one in the first part (in my case, AuthorFolio). Enter the typical test url (http://sitepoint.com/author/bskvorc/) and run the Test. Then, change the domain regex to this:
(http(s)?://)?(.*\.)?sitepoint.com/author/[^/]+/
This tells the API to only target the first page of any author profile - it ignores pagination completely.
Next, define a new collection. Call it ""social"" and give it a custom field with the selector of .contributor_social li. Name the field ""link"", and give it a selector of ""a"" with an attribute filter of href. Save, wait for the reload, and notice that you now have the four links extracted:
But having just the links there kind of sucks, doesn't it? It would be nice if we had a social network name, too. SitePoint's design, however, doesn't class them in any semantically meaningful way, so there's no easy way to get the network name. How can we tackle this?
Regex Rewrite Filters to the rescue!
Custom fields have three available filters:
attribute: extracts an HTML element's attribute
ignore: ignores certain HTML elements based on a css selector
replace: replaces the content of the output with the given content if a regex pattern matches
We'll be using the third one - read more about them here.
Add a new field to our ""social"" collection. Give it the name ""network"", the selector a, and an attribute filter of href so it extracts the link just like the ""link"" field. Then, add a new ""replace"" filter.
SitePoint author profiles can have the following social networks attached to their profiles: Google+, Twitter, Facebook, Reddit, Youtube, Flickr, Github and Linkedin. Luckily, each of those has pretty straightforward URLs with full domain names, so regexing the names out is a piece of cake. The correct regex is ^.*KEYWORD.*$:
Save, wait for the reload, and notice that you now have a well formed collection of an author's social links.
Bringing the APIs together
Finally, let's fetch all this data at once. According to what we said above, executing a call to an author page with the AuthorFolio API should now give us a single JSON response containing the sum of everything we've defined so far, including the fields from the first post. Let's see if that's true. Visit the following link in your browser:
http://diffbot.com/api/AuthorFolio?token=xxxxxxxxx&url=http://www.sitepoint.com/author/bskvorc/
This is the result I get:
As you can see, we successfully merged the two APIs and got back a single result of everything we wanted. We can now consume this API URL at will from any third party application, and pull in the portfolio of an author, easily grouping by date, detecting changes in the bio, registering newly added social networks, and much more.
Conclusion
In this post we looked at some trickier aspects of visual crawling with Diffbot like repeated collections and duplicate APIs on custom domain regexes. We built an endpoint that allows us to extract valuable information from an author's profile, and we learned how to apply this knowledge to any similar situation.
Did you crawl something interesting using these techniques? Did you run into any trouble? Let us know in the comments below!",-0.06485,https://www.sitepoint.com/diffbot-repeated-collections-merged-apis/,,
Title: Architecture of the Diffbot Knowledge Graph,,,Stanford Report,"Diffbot,architecture,Knowledge Graph",,0.287,https://web.stanford.edu/class/cs520/abstracts/tung.html,,
What is the algorithm used by Diffbot for extracting web data?,d2014-12-16T00:00,John Davi,quora.com,"data,natural language processing,background process,algorithm,Our Approach,video on demand,Diffbot,World Wide Web,application programming interface,extract","(Quick background: Diffbot's been working on automatic extraction of web data since 2010, and provides a number of on-demand APIs to automatically structure articles and blog posts, products, discussion threads, video pages, etc. -- you can test them out and/or sign-up for a free trial account at http://www.diffbot.com.)
Our approach relies on computer-vision techniques (in conjunction with machine learning, NLP and markup inspection) as the primary engine in identifying the proper content to extract from a page. What this means: when analyzing a web document, our system renders the page f...
(more)",,https://www.quora.com/What-is-the-algorithm-used-by-Diffbot-for-extracting-web-data,,
Diffbot Aims To Build The Intel Of Data For Artificial Intelligence,d2016-02-11T20:07:30,Jonathan Shieber,TechCrunch,"artificial intelligence,Mike Tung,People's Republic of China,To Build,Diffbot,Intel,Data,Tencent,World Wide Web,company","With a new $10 million commitment led by Tencent, one of China’s largest Internet companies, Diffbot chief executive Mike Tung has come a long way from his days of eating beans and rice in the dark and solving the math problems that would form the core of his groundbreaking artificial intelligence software.
Diffbot, which raised its first seed money in 2012, has set itself the lofty goal of being the “Intel of data” for independent artificial intelligence application developers.
Companies like Google, Facebook, and Baidu — which are all working on artificial intelligence — have the benefit of massive amounts of data at their fingertips that they and their data entry employees can use to categorize and define the web in a language that AI software can later feed into their algorithms .
Small companies who don’t have the benefit of that data can turn to Diffbot.
“We’ve been working on this technology for quite a few years. It was really last year that 90% to 95% accuracy was reached. And hitting profitability last year as one of the first AI startups to do so was a turning point,” says Tung.
The major expenses for Diffbot had been electricity and bandwidth, Tung says. Unlike other artificial intelligence deep learning projects that rely on humans to classify web pages, Diffbot uses only the proprietary algorithms that it created itself and has refined over the years, according to Tung.
“We want to build the world’s largest database of structured knowledge,” he says.
If artificial intelligence is to achieve the promise (and potential peril) inherent in the technology, it still needs to be taught.
Tung compares it to teaching a child. “The technology is scouring the web and is trying to simulate what a human being is doing when they’re on the page,” he says.
Research into artificial intelligence, and the ability to develop sentience in machines, sits at the intersection of a few very large trends in computing. It combines the development of new, and newly powerful, chipsets that can process complex increasingly quickly; the development of new kinds of database software that can organize massive amounts of data more flexibly, and the development of a nearly ubiquitous arrays of sensors and systems to collect that data.
The problem with the data that these would be intelligences would learn and process is that it needs to be structured in a way that the systems can recognize and that’s exactly what Diffbot does.
“We’re taking the Internet and converting it into semantic knowledge,” says Tung. And, in a strategy that drives down the cost of developing the massive trillions of facts that comprise the taxonomy that Diffbot is creating, the company’s secret weapon is its own AI software.
“Google has this knowledge graph using human curation and it’s the same with Watson. There’s a lot of human beings behind the scenes creating the rules the way the algorithm works,” says Tung. And humans cost money that Diffbot simply doesn’t need to spend.
Tung calls it the Manhattan project for AI — except computers are the researchers developing the bomb.
The Path Seldom Taken
Diffbot was always going to make money. The question of profitability wasn’t one that Tung ever wanted to address, nor was relying on fundraising as a necessity, the founder and chief executive said.
To make money to support the development of the software, Tung pinched pennies and took on a second job after dropping out of Stanford’s graduate school, learning patent law and filing patents in the wee small hours of the morning to make rent money.
“For each patent I was able to get 20K,” says Tung. “I would be good to get rent for a few months.”
He lived on a diet of beans and rice and ramen, alternating working on the math at the core of the software with filing patent applications for money.
Once the initial product was baked, Diffbot had the singular honor of being the first company to be accelerated in the program that would become Stanford’s premiere source for getting graduates to exit velocity with their business — StartX (where Tung is still a mentor).
With the initial seed money from StartX, Diffbot was able to continue its research and launch its first, revenue generating, products.
“From day one we made it an on-demand service,” Tung recalls. “You pass us a URL and we will process that. For every hit to our server we earn .008 cents…
In retrospect it was a decision that Tung was happiest about. “Our on-demand customers were paying us to structure the web,” he says.
Many of those on-demand customers are still on board. AOL (the parent company and owner of TechCrunch), Yandex, eBay, Microsoft’s Bing search service, Cisco and Adobe all pay Diffbot for its taxonomical services — and Diffbot got to increase the scope of its learning.
A Thin, Premeditated Rig
While Diffbot couldn’t spider the web from day one, by 2015 its situation had changed. The company was profitable, confident in its ability to raise money, its AI software was identifying data on the web with a 90% to 95% reliability. It was time.
So the company started spidering the web to speed up its data collection. The goal, ultimately is to get to trillions of discrete data points to provide a structured taxonomy for the entire internet (it’s a small goal).
Since the company began its spidering project last year, it’s taxonomy already contains more than 1.2 billion objects and is adding 10 million objects per day.
By comparison, Google’s Knowledge Graph only recently passed 1 billion objects, the company notes.
Show Me The Money
Lofty goals attract big investors, and Diffbot has attracted some of the biggest.
For its seed round the company attracted a who’s who of the Silicon Valley’s biggest names including: EarthLink founder Sky Dayton; Andy Bechtolsheim, co-founder of Sun Microsystems; Joi Ito, Director of MIT Media Lab; Brad Garlinghouse, CEO of YouSendIt (and formerly of TechCrunch parent company AOL),Maynard Webb, Chairman of the Board at LiveOps, formerly eBay COO; Elad Gil, VP of Corporate Strategy at Twitter; Jonathan Heiliger, former VP of Technical Operations at Facebook; Redbeacon co-founder Aaron Lee; and founder of VitalSigns Montgomery Kersten.
The latest round brought in a strategic investor in Tencent, one of China’s largest Internet companies in one of the world’s largest markets. And Felicis Ventures, which is building a sizable portfolio of artificial intelligence companies.
A coterie of new angels and other institutions joined as well — all of them also bold-faced names in the Valley. Among the superstar new names are: Andy Bechtolsheim, the founder of Sun Microsystems and the first investor in Google; Amplify Ventures, Valor Capital, and Bill Lee — an early investor in SpaceX and Tesla.",,http://techcrunch.com/2016/02/11/diffbot-aims-to-build-the-intel-of-data-for-artificial-intelligence/,"We’ve been working on this technology for quite a few years. It was really last year that 90% to 95% accuracy was reached. And hitting profitability last year as one of the first AI startups to do so was a turning point.,We’re taking the Internet and converting it into semantic knowledge.,Google has this knowledge graph using human curation and it’s the same with Watson. There’s a lot of human beings behind the scenes creating the rules the way the algorithm works.,For each patent I was able to get 20K.,I would be good to get rent for a few months.",
Diffbot Aims To Build The Intel Of Data For Artificial Intelligence,d2016-02-11T00:00,Jonathan Shieber,TechCrunch,"Chief executive officer,Diffbot,Internet,Seed money,Deep learning,Tencent,Google,Algorithm,Human,Artificial intelligence","With a new $10 million commitment led by Tencent, one of China’s largest Internet companies, Diffbot chief executive Mike Tung has come a long way from his days of eating beans and rice in the dark and solving the math problems that would form the core of his groundbreaking artificial intelligence software.
Diffbot, which raised its first seed money in 2012, has set itself the lofty goal of being the “Intel of data” for independent artificial intelligence application developers.
Companies like Google, Facebook, and Baidu — which are all working on artificial intelligence — have the benefit of massive amounts of data at their fingertips that they and their data entry employees can use to categorize and define the web in a language that AI software can later feed into their algorithms .
Small companies who don’t have the benefit of that data can turn to Diffbot.
“We’ve been working on this technology for quite a few years. It was really last year that 90% to 95% accuracy was reached. And hitting profitability last year as one of the first AI startups to do so was a turning point,” says Tung.
The major expenses for Diffbot had been electricity and bandwidth, Tung says. Unlike other artificial intelligence deep learning projects that rely on humans to classify web pages, Diffbot uses only the proprietary algorithms that it created itself and has refined over the years, according to Tung.
“We want to build the world’s largest database of structured knowledge,” he says.
If artificial intelligence is to achieve the promise (and potential peril) inherent in the technology, it still needs to be taught.
Tung compares it to teaching a child. “The technology is scouring the web and is trying to simulate what a human being is doing when they’re on the page,” he says.
Research into artificial intelligence, and the ability to develop sentience in machines, sits at the intersection of a few very large trends in computing. It combines the development of new, and newly powerful, chipsets that can process complex increasingly quickly; the development of new kinds of database software that can organize massive amounts of data more flexibly, and the development of a nearly ubiquitous arrays of sensors and systems to collect that data.
The problem with the data that these would be intelligences would learn and process is that it needs to be structured in a way that the systems can recognize and that’s exactly what Diffbot does.
“We’re taking the Internet and converting it into semantic knowledge,” says Tung. And, in a strategy that drives down the cost of developing the massive trillions of facts that comprise the taxonomy that Diffbot is creating, the company’s secret weapon is its own AI software.
“Google has this knowledge graph using human curation and it’s the same with Watson. There’s a lot of human beings behind the scenes creating the rules the way the algorithm works,” says Tung. And humans cost money that Diffbot simply doesn’t need to spend.
Tung calls it the Manhattan project for AI — except computers are the researchers developing the bomb.
The Path Seldom Taken
Diffbot was always going to make money. The question of profitability wasn’t one that Tung ever wanted to address, nor was relying on fundraising as a necessity, the founder and chief executive said.
To make money to support the development of the software, Tung pinched pennies and took on a second job after dropping out of Stanford’s graduate school, learning patent law and filing patents in the wee small hours of the morning to make rent money.
“For each patent I was able to get 20K,” says Tung. “I would be good to get rent for a few months.”
He lived on a diet of beans and rice and ramen, alternating working on the math at the core of the software with filing patent applications for money.
Once the initial product was baked, Diffbot had the singular honor of being the first company to be accelerated in the program that would become Stanford’s premiere source for getting graduates to exit velocity with their business — StartX (where Tung is still a mentor).
With the initial seed money from StartX, Diffbot was able to continue its research and launch its first, revenue generating, products.
“From day one we made it an on-demand service,” Tung recalls. “You pass us a URL and we will process that. For every hit to our server we earn .008 cents…
In retrospect it was a decision that Tung was happiest about. “Our on-demand customers were paying us to structure the web,” he says.
Many of those on-demand customers are still on board. AOL (the parent company and owner of TechCrunch), Yandex, eBay, Microsoft’s Bing search service, Cisco and Adobe all pay Diffbot for its taxonomical services — and Diffbot got to increase the scope of its learning.
While Diffbot couldn’t spider the web from day one, by 2015 its situation had changed. The company was profitable, confident in its ability to raise money, its AI software was identifying data on the web with a 90% to 95% reliability. It was time.
So the company started spidering the web to speed up its data collection. The goal, ultimately is to get to trillions of discrete data points to provide a structured taxonomy for the entire internet (it’s a small goal).
Since the company began its spidering project last year, it’s taxonomy already contains more than 1.2 billion objects and is adding 10 million objects per day.
By comparison, Google’s Knowledge Graph only recently passed 1 billion objects, the company notes.
Show Me The Money
Lofty goals attract big investors, and Diffbot has attracted some of the biggest.
For its seed round the company attracted a who’s who of the Silicon Valley’s biggest names including: EarthLink founder Sky Dayton; Andy Bechtolsheim, co-founder of Sun Microsystems; Joi Ito, Director of MIT Media Lab; Brad Garlinghouse, CEO of YouSendIt (and formerly of TechCrunch parent company AOL),Maynard Webb, Chairman of the Board at LiveOps, formerly eBay COO; Elad Gil, VP of Corporate Strategy at Twitter; Jonathan Heiliger, former VP of Technical Operations at Facebook; Redbeacon co-founder Aaron Lee; and founder of VitalSigns Montgomery Kersten.
The latest round brought in a strategic investor in Tencent, one of China’s largest Internet companies in one of the world’s largest markets. And Felicis Ventures, which is building a sizable portfolio of artificial intelligence companies.
A coterie of new angels and other institutions joined as well — all of them also bold-faced names in the Valley. Among the superstar new names are: Andy Bechtolsheim, the founder of Sun Microsystems and the first investor in Google; Amplify Ventures, Valor Capital, and Bill Lee — an early investor in SpaceX and Tesla.
Featured Image: agsandrew/Shutterstock",,http://techcrunch.com/2016/02/11/diffbot-aims-to-build-the-intel-of-data-for-artificial-intelligence/,,
Has anybody ever used Diffbot for a web scraping solution?,d2017-08-22T00:00,John Davi,quora.com,,,,https://www.quora.com/Has-anybody-ever-used-Diffbot-for-a-web-scraping-solution,,
"Diffbot launches AI-powered knowledge graph of 1 trillion facts about people, places, and things",d2018-08-30T08:00,Kyle Wiggers,VentureBeat,"Artificial intelligence,Knowledge Graph,People,Orders of magnitude (numbers),Aydin Senkut,Diffbot,Google Search","If you’ve ever performed a Google search for a celebrity, a famous landmark, or a product before, you’ve likely encountered the infoboxes that sometimes sit to the right of the results page. They’re filled with information from Google’s Knowledge Graph, an entities database used to enhance search results on the web and in smart speakers like Google Home. Most of the Knowledge Graph‘s more than 1.6 billion facts are crowdsourced from human teams, who regularly comb through millions of websites for answers to common questions about people, places, and things.
But if you ask Mike Tung, there’s a better way to do it.
He’s the founder of Diffbot, a Mountain View, California-based startup whose mission is to convert the web’s unstructured data into structured data — or, as Tung put it, “extracting knowledge in an automated way from documents.” Diffbot is publicly launching this week after a years-long private pilot program.
“We’re trying to build the first comprehensive map of human knowledge … by analyzing every page on the internet,” Tung told VentureBeat in a phone interview.
It’s a lofty goal, but Diffbot, which grew out of Tung’s artificial intelligence (AI) work at Stanford, spent five years building the tools necessary to accomplish it. Leveraging a combination of computer vision and natural language processing, Diffbot’s web crawler can parse the layout and structure of virtually any webpage — about 90 percent of the web and 20 or so page types, Tung claims — for facts, figures, and abstract relationships between objects. (Typical examples include a product page on Amazon.com or an executive bio on a company’s webpage.)
“We call it knowledge-as-a-service,” Tung said. “Right now, 30 percent of a knowledge workers’ job is data gathering. There’s a big opportunity in the market for a horizontal knowledge graph — a database of information about people, businesses, and things.”
Data extracted by Diffbot’s crawler feeds into an enormous database called the Diffbot Knowledge Graph, or DKG, comprising more than a trillion facts and 10 billion entities. (Tung said it’s adding facts at a rate of 130 million per month.) Core categories include people (skills, employment history, education, social profile), companies, locations (mapping data, addresses, business types, zoning information), articles (every news article, dateline, byline from anywhere on the web, in any language), discussions (chats, social sharing, and conversations), and images (organized using image recognition and metadata collection).
All of this is accessible via API calls and manipulable with Diffbot DQL, the company’s custom query syntax. Clients can view results from the DKG in a list, map, or table layout in Diffbot’s web-based UI, or from within third-party content management systems or analytics platforms.
Among those clients are Microsoft, eBay, Yandex, and DuckDuckGo, which are using it to enhance the quality of their search results. Other customers include Cisco, Salesforce, Crunchbase, Hubspot, Adobe, Instapaper, and Onswipe.
“Simply put, Diffbot is using the power of AI on a scale we’ve never seen before,” said Aydin Senkut, founder and managing director of Felicis Ventures, one of Diffbot’s investors. “It’s the first profitable AI company on record; they are the ‘secret ingredient’ powering applications from many of the largest companies in tech.”
In a demo, Tung showed me how it worked. Say you wanted to perform a one-off search for a brand of shoe. In Diffbot’s web dashboard, you’d type the sneaker brand into a Google-like search bar and hit enter; within milliseconds you’d get a product profile synthesized from sources around the web.
Looking for news articles instead? Same process: Typing in an author’s name yields every article they’ve ever published online (across languages, too). Searching for a person, on the other hand, pulls up a CV-like work history pieced together from dozens (or hundreds) of bios, articles, and publicly available profiles.
One of Diffbot’s unique strengths is its ability to quickly drill down by entity, Tung explained. It’s helpful in tasks like job recruitment — the appropriate DQL string (e.g., “type:Person employments.employer.name:’Diffbot'”) can collate every employee at a given company, along with their job title, skills, educational background, and social media profiles all in one place.
“This is the holy grail of machine learning — capturing all the world’s knowledge in one place,” Tung said.
Google’s Knowledge Graph has historically faced criticism for lacking attribution and omitting sources of conflicting information, but Tung said that Diffbot’s automated approach kills two birds with one stone. Not only is Diffbot more comprehensive than manually curated databases like Google’s Knowledge Graph, but it’s more accurate, too — Diffbot’s crawler regularly refreshes the DKG with new information and its machine learning algorithms are smart enough to pass over sites with histories of producing “logically inconsistent” facts.
“That’s one of the reasons why we fuse information together from different sources,” Tung said. “Our scale is such that there’s minimal potential for errors. We’d bet the business on it.”
Diffbot launched in 2008 and counts 28 employees among its core staff of engineers and data scientists. It previously raised $10 million in a funding round led by VC Tencent, Felicis Ventures, and Amplify Ventures.",-0.36241,https://venturebeat.com/2018/08/30/diffbot-launches-ai-powered-knowledge-graph-of-1-trillion-people-places-and-things/,,
Diffbot brings big-time search poobah aboard to help it scale,d2013-09-10T13:00,Barb Darrow,Gigaom,"Diffbot,web search engine","Diffbot is a small company with a big plan: to convert gazillions of web pages into machine-readable format that can be used and reused by lots of applications. Now, it’s bringing on some serious search engine firepower to get that job done. The new hire? Matt Wells, who created Gigablast, a pioneering search engine.
Some background: Gigablast was one of the first search engines to do real-time web indexing. At one point, back in the in the mid 2000s, its index hit the 12 billion page mark, second only to Google(s goog) but while Google was built with lots of people and resources, Gigablast was essentially a one-man show. Gigablast “from coding to crawling to marketing, it’s all Matt Wells,” according to SearchEngineWatch.
Prior to Gigablast, Wells was at Infoseek, where he worked with Robin Li, who went off to co-found a little company named Baidu, now China’s largest search engine and Wells created Gigablast.
To accomplish Diffbot’s ambitious goal of - to put it simply - converting the worlds’ web pages into databases, it needs machine learning and web-scale architecture experience which is what Wells brings to the table, according to Diffbot CEO Mike Tung.
Wells, he said, “is probably the only person in the world that has single-handedly written a commercial full-web search engine - even Larry and Sergey were a pair,” said Tung, referring to Google co-founders Larry Page and Sergey Brin.",-0.17133,https://gigaom.com/2013/09/10/diffbot-brings-big-time-search-poobah-aboard-to-help-it-scale/,,
Diffbot Raises $2 Million Angel Round For Web Content Extraction Technology,d2012-04-16T00:00,Sarah Perez,TechCrunch,"Andy Bechtolsheim,Matrix Partners,Sky Dayton,Maynard Webb,Sun Microsystems,Brad Garlinghouse,TechCrunch,Jonathan Heiliger,Diffbot,technology","Diffbot, the super-geeky/awesome visual learning robot technology which aims to “see” the web the way that people do, is today announcing a new infusion of capital. The company has closed $2 million in funding from a number of technology veterans, including EarthLink founder Sky Dayton; Andy Bechtolsheim, co-founder of Sun Microsystems; Joi Ito, Director of MIT Media Lab; Brad Garlinghouse, CEO of YouSendIt (and formerly of TechCrunch parent company AOL), Maynard Webb, Chairman of the Board at LiveOps, formerly eBay COO; Elad Gil, VP of Corporate Strategy at Twitter; Jonathan Heiliger, former VP of Technical Operations at Facebook; Redbeacon co-founder Aaron Lee; and founder of VitalSigns Montgomery Kersten.
Matrix Partners also participated in the round. Of the new investors, Sky Dayton will be the first to join Diffbot’s board and will be taking an active role in the company, including plans to go hands-on with various Diffbot projects.
Last August, the company publicly debuted its first APIs, which allow developers to build apps that can automatically extract meaning from web pages. For example, the Front Page API is able to analyze site homepages, and understands the difference between article text, headlines, bylines, ads, etc. The Article API can then extract clean article text, images and videos. Another example of Diffbot in action is the “follow API,” which can track the changes made to a website.
Today, Diffbot has categorized the web into about 20 different page types, including homepages and article pages, which are the first two types it can now identity. Going forward, Diffbot plans train its bots to recognize all the other types of pages, including product pages, social networking profiles, recipe pages, review pages, and more.
Its APIs have been put to use by AOL (again: disclosure, TC parent) in its news magazine AOL Editions, as well as by companies like Nuance, SocMetrics, and others. Diffbot says it’s now processing 100 million API calls per month on behalf of its customers. Thousands of developers are using the APIs, the company notes, but paying customers are only in the “tens.” Correction: we’re now told they have “a lot more!”
Diffbot founder and CEO Michael Tung (aka “Diffbot Mike”) says the new funding will be put towards new hires and expanding its resources. “More than that, we’re receiving a huge vote of confidence from veterans who have built massive companies and understand the fine points of building for scale, maintaining uptime and delivering the absolute highest standards of service.”
Tung is a patent attorney and Stanford PhD student who left the doctoral program to pursue Diffbot, thanks to seed funding from Stanford’s incubator, StartX. Diffbot was StartX’s first investment. With today’s funding, Diffbot total raise is $2 million and change.",,http://tcrn.ch/Jw7ZKw,,
Funding Daily: Diffbot raises $10 million for its data-gathering artificial intelligence,d2016-02-11T08:48,Sindy Nanclares,VentureBeat,"Artificial intelligence,Series A round,Diffbot","Artificial intelligence startup Diffbot announced today that it scored a $10 million series A round led by Chinese VC Tencent, Felicis Ventures, and Amplify Ventures.
The Palo Alto-based company says its technology “uses computer vision and NLP algorithms to extract and structure any web page into the world’s largest structured database… with no human curation or oversight.”
The startup has raised at least $12.5 million since its seed round in 2012.",,https://venturebeat.com/2016/02/11/funding-daily-diffbot-raises-10-million-for-its-artificial-intelligence-technology,,
"Diffbot Sees The Web Like People Do, Now Free For Developers",d2011-08-25T08:41:51,Sarah Perez,TechCrunch,"World Wide Web,Diffbot","Diffbot is a geeky and incredibly interesting technology that uses bots, algorithms, computer vision and artificial intelligence to process the content on the Web the way a human being can. “The entire Internet can be broken down into 30 different page types” explains Co-founder Mike Tung, also known as “Diffbot Mike,” and “Diffbot can identify them all.” Diffbot knows the difference between a social network profile, a blog post, a site’s front page, a product page, an event page and dozens more.
Today, Diffbot is releasing its first set of APIs, now open to all developers for free. The launch has the potential to dramatically impact the types of applications developers can build, and for consumers, it means a whole host of intelligent applications are about to emerge.
The New APIs: On-Demand & Follow
With the two API’s available now, developers can build apps that automatically extract meaning from pages, apps that understand what’s trending and who’s talking about it, apps that provide RSS feeds where none were available before and apps that read just the relevant parts of webpages aloud, ignoring ads, header and footer copy.
And that’s just for starters. Future API’s will enable developers to automatically turn event pages into calendar appointments, social network profiles into vCards or automatically extract shipping prices or reviews from product pages, among other things. While Diffbot doesn’t have a set roadmap, it expects to launch these additional API’s over the new few months.
Today, the first 2 API’s available are:
On-Demand API: This API is divided into page types “Frontpage” and “Article.” The former is used to analyze site homepages and index pages using common layout markers like headlines, bylines, images, articles, ads, etc. The Article API extracts clean article text, pictures and tags. (For example, see Readably.)
Follow API: This is used to track the changes or updates made to any webpage. Diffbot automatically determines the part of the page that the developer wants to follow and extracts metadata like title, images, text summary and more, then segments the page into meaningful sections (See above photo).
What Can Diffbot Actually Do?
These same APIs are already being used by companies like speech recognition system maker Nuance, AOL (disclaimer: TechCrunch is owned by AOL), social media monitoring firm SocMetrics, and others.
AOL uses Diffbot to extract the title, author, image, text, videos, topics and other metadata for its new iPad mag, AOL Editions. Nuance uses the technology to improve its natural language processing in a product for doctors, which requires comprehension of complex medical terminology. SocMetrics sends bit.ly shortened links to Diffbot to get the full article text and topics, so it can determine which social media users are talking about which topics the most.
These are just a few big-name examples. There are smaller, but just as innovative use cases out there, too. Like Hacker News Radio, for example, which reads Hacker News and comments to you. Or FeedBeater, which makes it easy to turn any URL into an RSS feed automatically (one of Diffbot’s first creations). Or this Diffbot-generated Twitter feed, which tracks changes to the webpage for the city of São Paulo, Brazil (as it lacks RSS), and tweets the updates.
The new self-serve platform for developers is free up to 50,000 API calls per month. The cloud plan provides 100,000 calls for $500, then is $0.002/call afterwards. The Managed plan for Enterprise requires custom pricing.
Diffbot was founded by Mike Tung and Leith Abdulla, both Stanford PhD students on a leave of absence to build the company. The idea sprung from Tung’s desire to automatically track new assignments on the class website automatically, through the use of technology. Diffbot was also the first startup funded by Stanford’s incubator program, now called StartX (formerly SSE Labs).",,https://techcrunch.com/2011/08/25/diffbot-sees-the-web-like-people-do-now-free-for-developers/,,
Diffbot: Repeated Collections and Merged APIs,d2014-08-22T00:00,Bruno Skvorc,SitePoint,"rerun,art collection,Uniform Resource Locator,HTML element,SitePoint,Diffbot,author,application programming interface,social network,collection","In the previous post on Analyzing SitePoint Authors’ Profiles with Diffbot we built a Custom API that automatically paginates an author’s list of work and extracts his name, bio and a list of posts with basic data (URL, title and date stamp). In this post, we’ll extract the links to the author’s social networks.
Introduction
If you look at the social network icons inside an author’s bio frame on their profile page, you’ll notice they vary. There can be none, or there can be eight, or anything in between. What’s worse, the links aren’t classed in any semantically meaningful way – they’re just links with an icon and a href attribute.
This makes turning them into an extractable pattern difficult, and yet that’s exactly what we’ll be doing here because hey, who doesn’t love a challenge?
To get set up, please read and go through the first part. When you’re done, re-enter the dev dashboard.
Repeated Collections Problem
The logical approach would be to define a new collection just like for posts, but one that targets the social network links. Then, just target the href attribute on each and we’re set, right? Nope.
Observe below:
As you can see, we get all the social links. But we get them all X times, where X is the number of pages in an author’s profile. This happens because the Diffbot API concatenates the HTML of all the pages into a single big one, and our collection rule finds several sets of these social network icon-links.
Intuition might lead you to use a :first-child pseudo element on the parent of the collection on the first page, but the API doesn’t work like that. The HTML contents of the individual pages are concatenated, yes, but the rules are executed on them first. In reality, only the result is being concatenated. This is why it isn’t possible to use main:first-child to target the first page only. Likewise, at this moment the Diffbot API does not have any :first-page custom pseudo elements, but them appearing at a later stage is not out of the question. How, then, do we do this?
Custom Domain Regex and API Dupes
Diffbot allows you to define several custom rulesets for the same API endpoint, differing by domain regex. When an API endpoint is called, all the rulesets that match the URL are executed, the results are concatenated, and you get a unique set back, as if it was all in a single API. This is what we’re going to do, too.
Start off by going to “Create a rule” and selecting a Custom API, so you get asked for a name. Enter the same name as the one in the first part (in my case, AuthorFolio). Enter the typical test url (http://sitepoint.com/author/bskvorc/) and run the Test. Then, change the domain regex to this:
(http(s)?://)?(.*\.)?sitepoint.com/author/[^/]+/
This tells the API to only target the first page of any author profile – it ignores pagination completely.
Next, define a new collection. Call it “social” and give it a custom field with the selector of .contributor_social li. Name the field “link”, and give it a selector of “a” with an attribute filter of href. Save, wait for the reload, and notice that you now have the four links extracted:
But having just the links there kind of sucks, doesn’t it? It would be nice if we had a social network name, too. SitePoint’s design, however, doesn’t class them in any semantically meaningful way, so there’s no easy way to get the network name. How can we tackle this?
Regex Rewrite Filters to the rescue!
Custom fields have three available filters:
attribute: extracts an HTML element’s attribute
ignore: ignores certain HTML elements based on a css selector
replace: replaces the content of the output with the given content if a regex pattern matches
We’ll be using the third one – read more about them here.
Add a new field to our “social” collection. Give it the name “network”, the selector a, and an attribute filter of href so it extracts the link just like the “link” field. Then, add a new “replace” filter.
SitePoint author profiles can have the following social networks attached to their profiles: Google+, Twitter, Facebook, Reddit, Youtube, Flickr, Github and Linkedin. Luckily, each of those has pretty straightforward URLs with full domain names, so regexing the names out is a piece of cake. The correct regex is ^.*KEYWORD.*$:
Save, wait for the reload, and notice that you now have a well formed collection of an author’s social links.
Bringing the APIs together
Finally, let’s fetch all this data at once. According to what we said above, executing a call to an author page with the AuthorFolio API should now give us a single JSON response containing the sum of everything we’ve defined so far, including the fields from the first post. Let’s see if that’s true. Visit the following link in your browser:
http://diffbot.com/api/AuthorFolio?token=xxxxxxxxx&url=http://www.sitepoint.com/author/bskvorc/
This is the result I get:
As you can see, we successfully merged the two APIs and got back a single result of everything we wanted. We can now consume this API URL at will from any third party application, and pull in the portfolio of an author, easily grouping by date, detecting changes in the bio, registering newly added social networks, and much more.
Conclusion
In this post we looked at some trickier aspects of visual crawling with Diffbot like repeated collections and duplicate APIs on custom domain regexes. We built an endpoint that allows us to extract valuable information from an author’s profile, and we learned how to apply this knowledge to any similar situation.
Did you crawl something interesting using these techniques? Did you run into any trouble? Let us know in the comments below!
Bruno is a blockchain developer and code auditor from Croatia with Master’s Degrees in Computer Science and English Language and Literature. He's been a web developer for 10 years until JavaScript drove him away. He now runs a cryptocurrency business at Bitfalls.com via which he makes blockchain tech approachable to the masses, and runs Coinvendor, an on-boarding platform for people to easily buy cryptocurrency. He’s also a developer evangelist for Diffbot.com, a San Francisco-based AI-powered machine vision web scraper.",,http://www.sitepoint.com/diffbot-repeated-collections-merged-apis/,,
"Diffbot aims to convert the web into one big database, one page at a time",d2013-07-31T13:00,Barb Darrow,Gigaom,"startup company,Mike Tung,Brad Garlinghouse,Diffbot","Diffbot the itty-bitty startup with some big, bad backers is about to release a new API that will let users convert product web pages into reusable data for pricing analysis and tracking and whatever other uses they can come up with.
The idea behind Diffbot’s visual robot is to scan and recognize different types of web pages — which are largely unstructured — for what they are — rich data sources for other applications. “We take pages and analyze them and make a structure out of them using sophisticated techniques,” Diffbot founder and CEO Mike Tung told me recently.
Most web pages fall into a handful of broad categories — news articles, front page, images, events and extracts. Diffbot recognizes them for what they are and turns them into what Tung calls a “true database representation.” The company already released APIs for front pages and articles. What’s new here is the product API.
Customers include Instapaper which can take that structured data and repurpose it for use on mobile devices, he said.
Academics and big vendors including Google(s goog), Microsoft(s msft) and Yahoo(s yhoo) are all working to better understand web pages. Google Research and Microsoft Research are no doubt doing similar work, the difference being they are keeping it as a black box,Tung said. Diffbot is making its APIs and web-scanning SaaS service available to the masses.
Diffbot is backed by such tech stars as Andy Bechtolsheim, Sky Dayton, Joi Ito, Brad Garlinghouse, and Jonathan Heiliger",,https://gigaom.com/2013/07/31/diffbot-aims-to-convert-the-web-into-one-big-database-one-page-at-a-time/,We take pages and analyze them and make a structure out of them using sophisticated techniques.,
Diffbot helps apps read the web like humans,d2011-08-25T17:30,Ryan Kim,Gigaom,"Human,World Wide Web,Diffbot,Robotics,Mobile app","Diffbot, a Palo Alto, Calif. startup, is trying help developers build apps that read the web like humans. What that means is that Diffbot’s technology uses visual learning robotics and artificial intelligence to view content on the web visually, instead of parsing the underlying HTML code. It sounds a little geeky, but it provides a way for developers and publishers to analyze the web and organize it in ways that are very easy to digest for people, especially mobile users.
The company, which has just opened its first API to the public, is encouraging developers to start using it for a variety of applications. What might these apps look like? Well, AOL(s aol) joined a private beta earlier this year and is now using Diffbot to help personalize its Editions news reader iPad app, (s aapl) by grabbing content and pulling out the top news stories from other sources. Diffbot is able to look at a content source and determine what kind of page it is, what the elements are such as headlines, images, advertisements and contextually understand the content on the page. It can determine what the top story is on a news site.
That’s helpful for news aggregators in determining what to present and creating a clean experience. Co-founder Michael Tung told me he’s talking to tablet news aggregator apps that compete with Editions and I’m guessing some will look at implementing Diffbot’s API. These companies are often doing this kind of work themselves but will now have an option in Diffbot.
But it’s not just news apps that can use Diffbot. Nuance uses the Diffbot API to build large domain-specific text corpuses to train its natural language processing system to recognize speech more accurately in specific areas like medicine. Hacker News Radio uses Diffbot to take the top Hacker News stories and turn them into text-to-speech content for an online radio station. You can see some other apps that use Diffbot’s API here.
Tung said Diffbot can really be useful in mobile applications, which have limited screen real estate and can use extra intelligence to help present content.
“We have hundreds of beta developers and a lot of them are working on mobile apps,” he said. “Web pages don’t look very good on mobile devices, they’re designed for large screens. A lot of the apps are using us because that want to incorporate web data and display it in a better way on mobile devices or do something custom.”
Diffbot is releasing two APIs: There’s an On-Demand API that looks at two types of pages, front pages and articles. The Frontpage API analyzes home pages and indexes things like headlines, bylines, images, articles and ads, while the Article API can extract article text, pictures, and tags from news pages. A second main API called Follow can be used to follow any changes or updates made to a web page and pulls out the useful data. Developers will get 50,000 API calls per month for free on a self-serve basis, while a cloud plan allocates 100,000 free calls a month, then is $0.002 per call after that.
Tung said there are about 30 different page types on the web and Diffbot will be opening APIs for those over time, including profile pages, event pages and product pages. As those become available, expect more developers to give Diffbot a try. Developers can grab more data from a variety of web sources and put them together into some interesting apps that should be useful for consumers.
Diffbot was built by Tung and Leith Abdulla, two Stanford students who were able to land funding from Stanford’s venture fund, SSE ventures. Tung said the company is profitable and is in the process of expanding beyond its five people.",,https://gigaom.com/2011/08/25/diffbot-helps-apps-read-the-web-like-humans/,We have hundreds of beta developers and a lot of them are working on mobile apps.,
AWS Case Study: Diffbot,,,"Amazon Web Services, Inc.","CBS Interactive,Stanford University,San Francisco Bay Area,Palo Alto, California,Diffbot,Mike Tung,Amazon Web Services","Diffbot is a San Francisco Bay Area startup that provides developers with APIs and other tools to understand and extract data from any web page. Diffbot’s APIs can extract the title, author, date, text, images, videos, captions, categories, entities, and other metadata from an article page to enhance readability on mobile applications. In addition, Diffbot provides a programmatic crawler that can be combined with page analysis APIs to extract and index databases of information from entire websites in real-time.
Diffbot enables software companies of all sizes—whether it’s a large company wanting to mine information from an entire website or a small, product-focused team with limited resources—to access nearly any page on the web as a source of structured data with a simple API call. Instapaper, Digg, AOL, Salesforce, CBS Interactive, and The New York Times use Diffbot’s APIs to power their content engines and analyze competitors. Large firms such as Salesforce’s Radian6 use Diffbot to monitor social media conversations while startups such as FindTheBest use Diffbot to check product pricing information on the web.
Diffbot, located in Palo Alto, CA, was founded by Mike Tung, then a graduate student in Artificial Intelligence at Stanford University, and was the first company to take part in StartX, Stanford’s on-campus accelerator.
Diffbot's technology applies computer vision and natural language processing algorithms to web pages, executing all of the styling, scripting, and layout needed to produce visual information. The processes are CPU-intensive and users tend to submit content in bursts from news streams, social media channels, and other sources. As a result, Diffbot has to be able to scale to handle frequent, real-time spikes in demand.
The company runs its own data center and was using custom software to handle deployment and scaling. ""When we were first started out as a small company, running the operations of our data center consumed an enormous amount of my time and attention,"" says Founder and CEO, Mike Tung. ""In the startup stage, focus is critical—anything that distracts you from delivering your company's core and unique value can be fatal to your venture's success. As we started to ramp up API call volumes, it was clear that we needed a better strategy for scaling our computing resources. Diffbot handles hundreds of millions of API calls per month, but as a startup, it was not capital efficient to build out a large-scale on-premises infrastructure.""
Diffbot considered a variety of solutions, but chose Amazon Web Services (AWS) because of the scalability of the platform and the ability to leverage Amazon EC2 Spot Instances as a cost-effective way to purchase compute capacity. Diffbot designed a solution that integrated the use of Amazon Elastic Compute Cloud (Amazon EC2) instances with existing on-premises resources. Diffbot uses the compute-optimized c1.xlarge Amazon EC2 instance types for its most compute-intensive machine learning loads. The high core count of these instance types means that multi-threaded code can utilize static objects more efficiently in memory. The higher clock speeds means that latency can be reduced.
By switching from Berkeley Internet Name Domain (BIND) DNS servers to Amazon Route 53, a globally distributed DNS, Diffbot can utilize the geographical distribution and the higher hit rate of a shared cache, removing a single-point-of-failure and lowering the average roundtrip latency. Diffbot uses Amazon Machine Images (AMIs) to define images of worker roles, greatly simplifying deployment and rollback and Amazon Simple Storage Service (Amazon S3) to store the AMIs.
Diffbot APIs analyze a web page and return a JavaScript Object Notation (JSON) object in real-time. The on-demand nature of some of its APIs means that traffic can spike throughout the day as new web pages are created across the web. Diffbot monitors resources with Amazon CloudWatch and utilizes Auto Scaling with custom predictive logic in order to scale up its analysis fleet during periods of high demand. This allows Diffbot to maintain high performance regardless of the amount of traffic it receives.
Diffbot processes hundreds of millions of web pages per month, and using Amazon EC2 Spot Instances lets the company flexibly prioritize and shift computing resources, depending on the level of requests. “Using Amazon EC2 Spot Instances helps Diffbot realize a 70 percent cost savings while the flexible prioritization increases reliability,” says Tung.
By running on the AWS Cloud, Diffbot is able to focus resources on developing cutting-edge machine learning algorithms, rather than worrying about hardware failure. Tung estimates that Diffbot can scale its infrastructure as needed in five minutes. “Utilizing AWS allows Diffbot to run on the same kind of world-class infrastructure that big companies use to operate their businesses. The resulting level of reliability, performance, and scale gained as a result would have been impossible to achieve by building out our own servers.”
To learn more about how AWS can help lower your computing costs, visit the http://aws.amazon.com/ec2/spot-instances/.",,https://aws.amazon.com/solutions/case-studies/diffbot/,,
What is your review of Diffbot?,d2016-12-30T00:00,,quora.com,Diffbot,"From trying it on their website, It looks like they are miles ahead of everybody else. Though I have to specify the type of the website to increase the accuracy, they are the only tool that can extract you all the post from any blog (Not limited to low-level Wordpress ones) and display them in a structured way with author and date. Price is a little bit high and I wish they could classify websites automatically because if you try to scrape a news website while classifying as a discussion-type website, the output is going to be off. Overall great product",-0.451,https://www.quora.com/What-is-your-review-of-Diffbot,,
Diffbot Is Using Computer Vision to Reinvent the Semantic Web,d2012-07-25T00:00,Wade Roush,Xconomy,"FaceTime,Skype,Google Hangouts,AT&T,HuffPost,Diffbot,Computer vision,Andy Bechtolsheim,Semantic Web","You know how the Picturephone, a half-billion-dollar project at AT&T back in the 1960s and 1970s, turned out to be a huge commercial flop, but two-way video communication eventually came back with a vengeance in the form of Skype and FaceTime and Google Hangouts? Well, something similar is going on with the Semantic Web.
That’s the proposal, dating back almost to the invention of the Web in the 1990s, that the various parts of Web pages should be tagged so that machines, as well as people, can make inferences based on the information they contain. The idea has never gotten very far, mainly because the burden of tagging all that content would fall to humans, which makes it expensive and tedious. But now it looks like the original goal of making digital content more comprehensible to computers might be achievable at far lower cost, thanks to better software.
Diffbot is building that software. This unusual startup—the first ever to emerge from the Stanford-based accelerator StartX, back in 2009—is using computer vision technology similar to that used for robotics applications such as self-driving cars to classify the parts of Web pages so that they can be reassembled in other forms. AOL is one of the startup’s first big customers and its landlord. It’s using Diffbot’s technology to assemble Editions by AOL, the personalized, iPad-based magazine comprised of content culled from AOL properties like the Huffington Post, TechCrunch, and Engadget.
I went down to AOL’s Palo Alto campus last month to meet the company’s founder and CEO Mike Tung and its vice president of products John Davi. They didn’t deliberately set out to solve the Semantic Web problem, any more than the founders of Skype set out to build an affordable Picturephone. But their venture, which has attracted about $2 million in backing from Andy Bechtolsheim and a raft of other angel investing stars, is already on its way to creating one of the world’s largest structured indexes of unstructured Web content.
Without relying on HTML tags (which can actually be used to trick traditional Web crawling software), Diffbot can look at a news page and tell what’s a headline, what’s a byline, where the article text begins and ends, what’s an advertisement, and so forth. What practical use can companies make of that, and where’s the profit in it for Diffbot? Well, aside from AOL, the startup’s software is already being used in some interesting places: reading app maker Pocket (formerly Read It Later) uses it to extract article text from websites, and content discovery service StumbleUpon employs it to screen out spam.
In fact, companies pay Diffbot to analyze more than 100 million unique URLs per month. And that’s just the beginning. Building outward from its early focus on news articles, the startup is creating new algorithms that could make sense of many kinds of sites, such as e-commerce catalogs. The individual elements of those sites could then be served up in almost any context. Imagine a Siri for shopping, to take just one example. “We’re building a series of wedges that will add up to a complete view of the Web,” says Davi. “We are excited about having them all under our belt, so there can be a fully indexed, reverse-engineered Semantic Web.”
What follows is a highly compressed version of my conversation with Tung and Davi.
Xconomy: Where did you guys meet, and how did you end up working on Diffbot?
Mike Tung: I worked at Microsoft on Windows Vista right out of high school, then went to college at Cal and studied electrical engineering for two years, then went to Stanford to start a PhD in computer science, specializing in AI. When I first moved to Silicon Valley, I also worked at a bunch of startups. I was engineer number four at TheFind, which was a product search company that built the world’s largest product index. I worked on search at Yahoo and eBay, and also did a bunch of contract work. I took the patent bar and worked as a patent lawyer for a couple of years, writing 3G and 4G patents for Panasonic and Matsushita. I first met John when we were working at a startup called ClickTV, which was a video-player-search-engine thing. It was pretty advanced for its time.
Diffbot began when I was in grad school at Stanford [in 2005]. There was this one quarter where I was taking a lot of classes, so I made this tool for myself to keep track of all of them. I would put in the URL for the class website, and whenever a professor would upload new slides or content, Diffbot would find that and download it to my phone. I always felt like I knew what was going on in my classes without having to attend every single one.
It was useful, and my friends started asking me whether they could use it. So I turned it into a Web service and … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy
(Page 2 of 4)
started running it out of a dorm at Stanford. And people started adding a bunch of different kinds of URLs to Diffbot outside of classes, like they might add Craigslist if they were searching for a job or a product, or Facebook if they wanted to see if their ex’s profile had changed.
X: So I assume the name “Diffbot” related to comparing the old and new versions of a website and detecting the differences?
MT: Yes, but just doing deltas on Web pages doesn’t work too well. It turns out that on the modern Web, every page refresh changes the ads and the counters. You have to be a little more intelligent.
That’s where understanding the page comes into play. I was studying machine learning at Stanford, and in particular one project I had worked on was the vision system for the self-driving car [Stanford’s entry in the 2007 DARPA Urban Challenge]. This was the stereo camera system that would compute the depth of a scene and say, ‘This is a cactus, this is drivable dirt, this is not drivable dirt, this is a cliff, this is a very narrow passageway.’ I realized that one way of making Diffbot generalizable was to apply computer vision to Web pages. Not to say, ‘This is a cactus and this is a pedestrian,’ but to say, ‘This is an advertisement and this is a footer and this is a product.’
A human being can look at Web page and very easily tell what type of page it is without even looking at the text, and that is what we are teaching Diffbot to do. The goal is to build a machine-readable version of the entire Web.
X: Isn’t that what Tim Berners-Lee has been talking about for years—building a Semantic Web that’s machine-readable?
MT: It seems that every three years or so a new Semantic Web technology gets hyped up again. There was RSS, RDF, OWL, and now it’s Open Graph and the Knowledge Graph. The central problem—why none of these have really gone mainstream—is that you are requiring humans to tag the content twice, once for the machine’s benefit and once for the actual humans. Because you are placing so much onus on the content creators, you are never going to have all of the content in any given system. So it will be fragmented into different Semantic Web file formats, and because of that you will never have an app that allows you to search and evaluate all that information.
But what if you analyze the page itself? That is where we have an opportunity, by applying computer vision to eliminate the problem of manual tagging. And we have reached a certain point in the technology continuum where it is actually possible—where the CPUs are fast enough and the machine learning technology is good enough that we have a good shot of doing it with high accuracy.
X: Why are you so convinced that a human-tagged Semantic Web would never work?
MT: The number one point is that people are lazy. The second is that people lie. Google used to read the meta tags and keywords at the top of a Web page, and so people would start stuffing those areas with everything. It didn’t correspond to what actual humans saw. The same thing holds for Semantic Web formats. Whenever you have things indexed separately, you start to see spam. By using a robot to look at the page, you are keeping it above that.
X: Talk about the computer vision aspect of Diffbot. How literal is the comparison to the cameras and radar on robot cars?
MT: We use the very same techniques used in computer vision, for example object detection and edge detection. If you are a customer, you give us a URL to analyze. We render the page using a virtual Webkit browser in the cloud. It will render the page, run the Javascript, and lay everything out with the CSS rules and everything. Then we have these hooks into Webkit that … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy
(Page 3 of 4)
allow us to get all of the visual and geometric information out of the page. For every rectangle, we pull out things like the x and y coordinates, the heights and widths, the positioning relative to everything else, the font sizes, the colors, and other visual cues. In much the same way, when I was working on the self-driving car, we would look at a patch and do edge detection to determine the shape of a thing or find the horizon.
X: Once you identify those shapes and other elements, how do you say, “This is a headline, this is an article,” et cetera?
MT: We have an ontology. Other people have done good work defining what those ontologies should be—there are many of them at schema.org, which reflects what the search engines have proposed as ontologies. We also have human beings who draw rectangles on the pages and teach Diffbot “this is what an author field looks like, this is what a product looks like, this is what a price looks like,” and from those rectangles we can generalize. It’s a machine learning system, so it lives and breathes on the training data that is fed into it.
X: Do you actually do all the training work yourselves, or do you crowdsource it out somehow?
John Davi: We have done a combination of things. We always have a cold-start problem firing up new type of pages—products versus articles, or a new algorithm for press releases, for example. We leverage both grunt work internally—just grinding out our own examples, which has the side benefit of keeping us informed about the real world—but yeah, also crowdsourcing, which gives us a much broader variety of input and opinion. We have used everything, including off-the-shelf crowdsourcing tools like Mechanical Turk and Crowdflower, and we have build up our own group of quasi-contract crowdsourcers.
Our basic effort is to cold-start it ourselves, then get an alpha-level product into the hands of our customer, which will then drastically increase the amount of training data we have. Sometimes we look at the stream of content and eyeball it and manually tweak and correct. In a lot of cases our customer gets involved. If they have an interest in helping to train the algorithm—it not only makes it better for them, but if they are first out of the gate they can tailor the algorithm to their very particular needs.
X: How much can your algorithms tell about a Web page just from the way it looks? Are you also analyzing the actual text?
MT: First we take a URL and determine what type of page it is. We’ve identified roughly 20 types of pages that all the Web can fall into. Article pages, people pages, product pages, photos, videos, and so on. So one of the fields we return will be what is the type of this thing. Then, depending on the type, there are other fields. For the article API [application programming interface], which is one we have out publicly, we can tell you the title, the author, the images, the videos, and the text that go with that article. And we not only identify where the text is, but we can tell you the topics. We do some natural language processing on the text and we can tell you “This is about Apple,” and we can tell it’s about Apple Computer and not the fruit.
JD: Another opportunity we are excited about his how Diffbot can help augment what is natively on the page. Just by dint of following so many pages through our system, we can augment [the existing formatting] and increase the value for whoever is reading. In the case of an article, the fact that we see so many articles means it’s relatively easy for us to generate tags for any given text.
X: How do you turn this all into a business?
MT: We are actually selling something. We are trying to build the Semantic Web, but in a profitable way. We analyze the pages that people pay us to analyze. That’s currently over 100 million URLs per month, which is a good slice of the Web. Other startups have taken the approach of starting by crawling and indexing the Web, and that is very capital-intensive. By doing it this way, another benefit is that people only send us the best parts of … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy
(Page 4 of 4)
the Web. Most of the stuff a typical Web crawler goes through never appears in any search results. Most of the Web is crap.
X: Are people finding uses for the technology that you may not have thought of?
MT: We had a hackathon last year where a guy came in and built an app for his father, who is blind. It runs Diffbot on a page and makes it into a radio station. For someone who is blind, browsing a news site is usually a really poor experience. The usual screen readers will read the entire page, including the nav bars and the ads and the text. The screen readers have no context about what is important on the page. Using Diffbot to be his father’s eyes, this guy could parse the page and read it in a way that is much more natural.
JD: AOL’s Editions app is one of the more interesting use cases that I’ve seen. It’s an iPad app that features both their own content as well as snippets from across the Web, in a daily issue. I spent five years running engineering for the media solutions group at Cisco, selling a Web platform for media companies, and the biggest problem we faced was dealing with the excess of content management systems that all media companies have. In the case of Editions, AOL has myriad properties that they want to merge into this single app. But rather than consolidate TechCrunch and Engadget and the Huffington Post and a half dozen other sites, they use Diffbot to build a kind of content management system on the fly from the rendered Web pages. They extract the content and deliver it on the fly as if it came from a CMS right to the iPad magazine.
StumbleUpon is another interesting one. They use Diffbot as their moderation queue. Whenever a new website is submitted to their index, they want to make sure it’s legitimate before it’s available for stumbling. They have to rule out people who stumble a page, then swap it out for spam. So they run Diffbot on the source page, pipe that into their moderation queue, and if it looks like a legitimate page they can monitor that and keep checking on a regular basis to see how much it changes. If it has changed much between day 1 and day 10, it might warrant human intervention.
X: Aren’t there are a lot of news reader app these days that are doing the same thing you’re doing when it comes to identifying and isolating the text of a news article? That’s what Instapaper and Pocket and Readability and Zite are all doing.
MT: We power a lot of those apps. Our audience is the developers who work at those companies, who use our API to create their experience.
JD: We make it a lot more affordable to make those kinds of forays. When you look at building your own customized extraction tools, you are talking about multiple developers over weeks or months, to build something that is more brittle than what we offer out of the gate. Our ultimate goal is to be not only better but a lot cheaper than what you could build.
X: It’s not totally clear yet, though, whether publications or apps that aggregate lots of content from elsewhere, like Editions or even Flipboard, are going to be profitable in the long term, and where publishing is going as a business. Don’t you guys feel there’s some risk in tying your fortunes to such a troubled industry?
MT: The more interesting question is how do you monetize the Semantic Web, and where is the money in building the structured information. Articles are only one page type. Another that I mentioned is products. If you could show products on a cell phone, and people could buy the product and we could make that transaction happen, that is one very tangible way of making money. I think there is a lot of value in having structured information, because you can connect people more directly to what they want. Once we have the entire Web in machine-readable format, anybody who wants to use any sort of data can use the Diffbot view of it, and I think a lot of those apps can make money. Look at Siri—it’s great but it only works with the 10 or so sources that it’s hard-coded to work with. If you were able to combine Siri with Diffbot, Siri could operate on the Web and take a query and actually do it for you.
X: What page types will you move on to next? Did you start with articles because those are easiest?
MT: I wouldn’t say they were easiest, but they are pretty prevalent on the Web. A variety of factors help us prioritize what we should do next. One signal is what is the prevalence of that type of page on the Web. If doing one page type lets us knock out 30 percent of the Web, maybe we will go for it.
X: Will there always be a need for Diffbot, or with the transition to HTML 5, will Web pages gradually get more structure on their own?
MT: If you look at the ratio of unstructured pages to structured, it’s actually going in the opposite direction. I think human beings are creative, and they design pages for other humans. No matter what, people will find a way to create documents that lie outside of the well-defined tags, whether it’s HTML 5 or Flash or PDF or Xbox. What they all have in common is that they are just vessels that we can easily train and adapt Diffbot to work with.
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy",-0.12528,http://www.xconomy.com/san-francisco/2012/07/25/diffbot-is-using-computer-vision-to-reinvent-the-semantic-web/,,
Diffbot Challenges Google Supremacy With Rival Knowledge Graph,d2015-06-04T00:00,Bernadette Tansey,Xconomy,"artificial intelligence,Bloomberg Beta,Google,Google Search,Palo Alto,Diffbot,web page,Mike K Tung,Knowledge Graph,ontology","When you do a Google search at your desktop for a common health condition, you’ll get links to tons of webpages you can sift through in hopes of finding the specific facts you want.
But if you’ve searched Google from a mobile device recently, you may have been rewarded with a summary of important facts about the disorder, culled for you from many websites. You’re tapping into what Google calls its Knowledge Graph.
Palo Alto, CA-based artificial intelligence startup Diffbot has based its whole business on that second kind of search—ferreting out data points scattered across many websites and pulling them together into Big Data resources that can be queried, combined, and rearranged. The upstart company—14 engineers in a backyard bungalow—now says its own data mega-map, called its Global Index, is a bigger database than Google’s Knowledge Graph of billions of facts.
This kind of structured data—Web facts organized into a searchable database—is the resource behind the most popular mobile apps, says Diffbot founder and CEO Mike Tung. Such apps can answer questions like, “What is the best Thai restaurant in this neighborhood?” Diffbot’s mission is to capture everything online—articles, images, videos, comments, reviews, the works—and keep it updated.
“We are working to create a structured version of the Web,” Tung says. “We’re quite serious about that.”
The company developed elements of its Web-crawling methods as it served customer needs over the past few years, but only started proactively spidering the Web for its own purposes in the past few months. Its Global Index now contains more than 600 million objects (this can be anything from a celebrity to an Ikea chair model) and 19 billion facts. Diffbot clocks Google’s Knowledge Graph at about 570 million objects and 18 billion facts.
Diffbot, founded in 2008, is already covering its operating expenses by enhancing other search engines including Microsoft’s Bing and DuckDuckGo, and by powering apps for companies such as Cisco and AOL, Tung says. Diffbot subscribers can build apps based on narrowly targeted searches that answer questions such as, “What’s the best price in my region for Nike cross trainers?”
But Diffbot has larger ambitions, and it’s raising money to support them. The company just banked $500,000 from Bloomberg Beta, bringing an angel round up to $3 million. It wouldn’t be surprising to see a Series A round raised this year, Tung says. Just a hint about Diffbot’s ultimate interests: According to its CEO, Diffbot may help answer the long-debated question: Can computers ever duplicate human intelligence?
Diffbot has been exploring the art of teaching machines to function like a human researcher—compiling facts from multiple online sources so they can be combined and compared for many purposes. The company began building its Global Index by storing results from URL searches requested by customers, but in recent months Diffbot has been analyzing websites to build its index at a rate of up to 15 million pages a day.
Its artificial intelligence bots are doing the work without human supervision. Tung says Google’s Knowledge Graph, by contrast, has relied significantly on human curation.
“Our approach is fairly radical in that there’s no human behind the curtain,” Tung says. “This is why we were able to catch up in such a short time.”
Diffbot assembles its own servers at its Palo Alto bungalow. They’re not the kind you can rent from a cloud storage outfit. The company’s standard crawling and indexing machines use 32 terabytes of solid-state storage, have 192GB of RAM, and 40 CPU cores. Diffbot now has 100 servers in a guarded co-location space in Fremont, CA, where fiber optic cables link them to all the Internet service providers in the world, Tung says.
The crawlbots are adding millions of new objects to Diffbot’s index every day. Tung is envisioning adding thousands more servers, or tens of thousands.
“If we just throw more resources at it, we can generate structured data at true scale,” Tung says.
Last year, Xconomy’s much-missed San Francisco editor Wade Roush asked the question, “Could a Little Startup Called Diffbot Be the Next Google?” in his article about Diffbot’s mission to cover the Web more fully by taking search further than conventional search engines. Diffbot had developed bots that can “read” a webpage the way humans do, distinguishing among different parts of the layout such as headlines, main text, side columns, and so on.
With this computer vision, the machines can tell the difference between types of Web pages—article pages, home pages, and product offerings where prices are displayed. They can reshuffle these layout elements to reformat a Web page for mobile device screens—a chore that companies pay Diffbot to take on, and one of its early sources of revenue.
The bots also “learn” where they’re likely to find certain information on a page, such as prices or author names on articles. They can extract information from images, videos, blogs, and the discussion threads that follow published articles. The company’s newest product, Discussion API, has become a tool for marketers who want to check brand reputations, Tung says.
More from Xconomy
Trending on Xconomy
(Page 2 of 2)
assemble information about competitors and suppliers, or people they might want to hire.
“All these entities leave footprints on the Web,” Tung says. Diffbot is now developing frameworks for machine analysis of new kinds of pages, such as events, locations, and profile pages, to further expand its Global Index.
If you’re a high school freshman with an English term paper due tomorrow, you might be wondering how you can log in to this new type of search to get all the facts you need about Charles Dickens, by midnight tonight, painlessly assembled by a machine.
For the most part, though, consumers can’t yet directly tap into the structured Web data compiled by Diffbot and Google. Diffbot shares its data resources with consumers indirectly by selling its services to search engines and app developers. A Bing search about a product, for example, will show the traditional list of website links, but in the upper right-hand corner, it may display an image, price, and other facts assembled about the product—structured data.
Google is also making some of its Knowledge Graph findings available through mobile searches. But Tung speculates that Google may limit these kinds of search returns in favor of traditional Web page listings, because they expose consumers to more advertising, the tech titan’s source of revenue.
In his 2014 article, Roush pondered what would happen if Diffbot remained an independent company, grew to 10,000 employees, and vied with Google to control our online existence. But Roush, wistfully, found it more likely that Diffbot would be “acqui-hired” by Google at some point.
Tung brought up this prediction when I talked to him this week—mainly to refute it. He didn’t sound like a guy who was ready to let somebody else discover his company’s full potential.
“We have received a lot of acquisition offers from pretty much all of the large technology companies,” Tung says. Diffbot’s response to the offers, he says, is to convert its suitors into customers. He declines to say whether Google belongs to either category.
So what future is Tung aiming an independent Diffbot towards?
My sense is that Tung is an artificial intelligence researcher at heart, captivated by questions about what machines could do if humans knew how to equip their silicon brains and train them expertly.
Here’s part of his shorter-term vision: Technologists are not just organizing Web data to better inform humans so they can decide what to do or to think. They’re also structuring the Web to better inform machines, so they can take action themselves and work with other machines.
The first products along these lines may be as cozy as recipe apps. You might be able to point your mobile phone at an unfurnished corner of your new living room, so that in a few minutes it will tell you what chair on the market would fit in the space, and look good with the rest of your decor, Tung says.
He gave another example: The printer in your office runs out of ink, knows whether it needs a black or color cartridge, knows which manufacturers’ products are compatible, taps into the Web, compares prices, and executes the order.
“We think that’s the exciting future,” Tung says. “Everything’s intelligent, and they all need access to information.”
But beyond those limited chores, there’s still a bg question out there: Will machines ever duplicate human intelligence? For example, could they exercise judgment by balancing the benefits and consequences of two different courses of action, such as choices between medical treatments or business strategies?
“Progress toward human intelligence is still quite a rocky road ahead,” Tung says. The artificial intelligence community hasn’t yet hit on the missing link that would make that possible, he says. “It’s going to require a breakthrough that’s still unknown.”
But the Diffbot team is trying to make training computers more sophisticated by rendering the Web—the digital repository of a growing swath of human knowledge—readable by machines.
The big successes already gained in artificial intelligence have not necessarily come from new programming wizardry, but through the application of old algorithms to dense data sets, Tung and fellow Diffbot executive John Davi point out.
Achievements in computer image classification have built on the burgeoning population of digitized images that were not available in earlier decades, they say. The IBM computer Watson performed feats of medical diagnosis by drawing from a concentrated trove of human-curated knowledge in that relatively narrow realm of data, Tung and Davi add.
“Our working theory is, if we can assemble enough structured, labeled data, we can simulate all aspects of human intelligence,” Tung says. The inflection point in artificial intelligence may be assembling trillions of objects from the Web that machines can read, he says.
“We’re working on what could be the missing piece, which is the data,” Tung says.
More from Xconomy
Trending on Xconomy",,http://www.xconomy.com/national/2015/06/04/diffbot-challenges-google-supremacy-with-rival-knowledge-graph/,"We are working to create a structured version of the Web.,We’re quite serious about that.,Our approach is fairly radical in that there’s no human behind the curtain.,This is why we were able to catch up in such a short time.",
Diffbot Is Using Computer Vision to Reinvent the Semantic Web,d2012-07-25T00:00,Wade Roush,Xconomy,"Diffbot,Autonomous car,Panasonic,Computer vision,Semantic Web","You know how the Picturephone, a half-billion-dollar project at AT&T back in the 1960s and 1970s, turned out to be a huge commercial flop, but two-way video communication eventually came back with a vengeance in the form of Skype and FaceTime and Google Hangouts? Well, something similar is going on with the Semantic Web.
That’s the proposal, dating back almost to the invention of the Web in the 1990s, that the various parts of Web pages should be tagged so that machines, as well as people, can make inferences based on the information they contain. The idea has never gotten very far, mainly because the burden of tagging all that content would fall to humans, which makes it expensive and tedious. But now it looks like the original goal of making digital content more comprehensible to computers might be achievable at far lower cost, thanks to better software.
Diffbot is building that software. This unusual startup—the first ever to emerge from the Stanford-based accelerator StartX, back in 2009—is using computer vision technology similar to that used for robotics applications such as self-driving cars to classify the parts of Web pages so that they can be reassembled in other forms. AOL is one of the startup’s first big customers and its landlord. It’s using Diffbot’s technology to assemble Editions by AOL, the personalized, iPad-based magazine comprised of content culled from AOL properties like the Huffington Post, TechCrunch, and Engadget.
I went down to AOL’s Palo Alto campus last month to meet the company’s founder and CEO Mike Tung and its vice president of products John Davi. They didn’t deliberately set out to solve the Semantic Web problem, any more than the founders of Skype set out to build an affordable Picturephone. But their venture, which has attracted about $2 million in backing from Andy Bechtolsheim and a raft of other angel investing stars, is already on its way to creating one of the world’s largest structured indexes of unstructured Web content.
Without relying on HTML tags (which can actually be used to trick traditional Web crawling software), Diffbot can look at a news page and tell what’s a headline, what’s a byline, where the article text begins and ends, what’s an advertisement, and so forth. What practical use can companies make of that, and where’s the profit in it for Diffbot? Well, aside from AOL, the startup’s software is already being used in some interesting places: reading app maker Pocket (formerly Read It Later) uses it to extract article text from websites, and content discovery service StumbleUpon employs it to screen out spam.
In fact, companies pay Diffbot to analyze more than 100 million unique URLs per month. And that’s just the beginning. Building outward from its early focus on news articles, the startup is creating new algorithms that could make sense of many kinds of sites, such as e-commerce catalogs. The individual elements of those sites could then be served up in almost any context. Imagine a Siri for shopping, to take just one example. “We’re building a series of wedges that will add up to a complete view of the Web,” says Davi. “We are excited about having them all under our belt, so there can be a fully indexed, reverse-engineered Semantic Web.”
What follows is a highly compressed version of my conversation with Tung and Davi.
Xconomy: Where did you guys meet, and how did you end up working on Diffbot?
Mike Tung: I worked at Microsoft on Windows Vista right out of high school, then went to college at Cal and studied electrical engineering for two years, then went to Stanford to start a PhD in computer science, specializing in AI. When I first moved to Silicon Valley, I also worked at a bunch of startups. I was engineer number four at TheFind, which was a product search company that built the world’s largest product index. I worked on search at Yahoo and eBay, and also did a bunch of contract work. I took the patent bar and worked as a patent lawyer for a couple of years, writing 3G and 4G patents for Panasonic and Matsushita. I first met John when we were working at a startup called ClickTV, which was a video-player-search-engine thing. It was pretty advanced for its time.
Diffbot began when I was in grad school at Stanford [in 2005]. There was this one quarter where I was taking a lot of classes, so I made this tool for myself to keep track of all of them. I would put in the URL for the class website, and whenever a professor would upload new slides or content, Diffbot would find that and download it to my phone. I always felt like I knew what was going on in my classes without having to attend every single one.
It was useful, and my friends started asking me whether they could use it. So I turned it into a Web service and … Next Page »
More from Xconomy
Trending on Xconomy
(Page 2 of 4)
started running it out of a dorm at Stanford. And people started adding a bunch of different kinds of URLs to Diffbot outside of classes, like they might add Craigslist if they were searching for a job or a product, or Facebook if they wanted to see if their ex’s profile had changed.
X: So I assume the name “Diffbot” related to comparing the old and new versions of a website and detecting the differences?
MT: Yes, but just doing deltas on Web pages doesn’t work too well. It turns out that on the modern Web, every page refresh changes the ads and the counters. You have to be a little more intelligent.
That’s where understanding the page comes into play. I was studying machine learning at Stanford, and in particular one project I had worked on was the vision system for the self-driving car [Stanford’s entry in the 2007 DARPA Urban Challenge]. This was the stereo camera system that would compute the depth of a scene and say, ‘This is a cactus, this is drivable dirt, this is not drivable dirt, this is a cliff, this is a very narrow passageway.’ I realized that one way of making Diffbot generalizable was to apply computer vision to Web pages. Not to say, ‘This is a cactus and this is a pedestrian,’ but to say, ‘This is an advertisement and this is a footer and this is a product.’
A human being can look at Web page and very easily tell what type of page it is without even looking at the text, and that is what we are teaching Diffbot to do. The goal is to build a machine-readable version of the entire Web.
X: Isn’t that what Tim Berners-Lee has been talking about for years—building a Semantic Web that’s machine-readable?
MT: It seems that every three years or so a new Semantic Web technology gets hyped up again. There was RSS, RDF, OWL, and now it’s Open Graph and the Knowledge Graph. The central problem—why none of these have really gone mainstream—is that you are requiring humans to tag the content twice, once for the machine’s benefit and once for the actual humans. Because you are placing so much onus on the content creators, you are never going to have all of the content in any given system. So it will be fragmented into different Semantic Web file formats, and because of that you will never have an app that allows you to search and evaluate all that information.
But what if you analyze the page itself? That is where we have an opportunity, by applying computer vision to eliminate the problem of manual tagging. And we have reached a certain point in the technology continuum where it is actually possible—where the CPUs are fast enough and the machine learning technology is good enough that we have a good shot of doing it with high accuracy.
X: Why are you so convinced that a human-tagged Semantic Web would never work?
MT: The number one point is that people are lazy. The second is that people lie. Google used to read the meta tags and keywords at the top of a Web page, and so people would start stuffing those areas with everything. It didn’t correspond to what actual humans saw. The same thing holds for Semantic Web formats. Whenever you have things indexed separately, you start to see spam. By using a robot to look at the page, you are keeping it above that.
X: Talk about the computer vision aspect of Diffbot. How literal is the comparison to the cameras and radar on robot cars?
MT: We use the very same techniques used in computer vision, for example object detection and edge detection. If you are a customer, you give us a URL to analyze. We render the page using a virtual Webkit browser in the cloud. It will render the page, run the Javascript, and lay everything out with the CSS rules and everything. Then we have these hooks into Webkit that … Next Page »
More from Xconomy
Trending on Xconomy
(Page 3 of 4)
allow us to get all of the visual and geometric information out of the page. For every rectangle, we pull out things like the x and y coordinates, the heights and widths, the positioning relative to everything else, the font sizes, the colors, and other visual cues. In much the same way, when I was working on the self-driving car, we would look at a patch and do edge detection to determine the shape of a thing or find the horizon.
X: Once you identify those shapes and other elements, how do you say, “This is a headline, this is an article,” et cetera?
MT: We have an ontology. Other people have done good work defining what those ontologies should be—there are many of them at schema.org, which reflects what the search engines have proposed as ontologies. We also have human beings who draw rectangles on the pages and teach Diffbot “this is what an author field looks like, this is what a product looks like, this is what a price looks like,” and from those rectangles we can generalize. It’s a machine learning system, so it lives and breathes on the training data that is fed into it.
X: Do you actually do all the training work yourselves, or do you crowdsource it out somehow?
John Davi: We have done a combination of things. We always have a cold-start problem firing up new type of pages—products versus articles, or a new algorithm for press releases, for example. We leverage both grunt work internally—just grinding out our own examples, which has the side benefit of keeping us informed about the real world—but yeah, also crowdsourcing, which gives us a much broader variety of input and opinion. We have used everything, including off-the-shelf crowdsourcing tools like Mechanical Turk and Crowdflower, and we have build up our own group of quasi-contract crowdsourcers.
Our basic effort is to cold-start it ourselves, then get an alpha-level product into the hands of our customer, which will then drastically increase the amount of training data we have. Sometimes we look at the stream of content and eyeball it and manually tweak and correct. In a lot of cases our customer gets involved. If they have an interest in helping to train the algorithm—it not only makes it better for them, but if they are first out of the gate they can tailor the algorithm to their very particular needs.
X: How much can your algorithms tell about a Web page just from the way it looks? Are you also analyzing the actual text?
MT: First we take a URL and determine what type of page it is. We’ve identified roughly 20 types of pages that all the Web can fall into. Article pages, people pages, product pages, photos, videos, and so on. So one of the fields we return will be what is the type of this thing. Then, depending on the type, there are other fields. For the article API [application programming interface], which is one we have out publicly, we can tell you the title, the author, the images, the videos, and the text that go with that article. And we not only identify where the text is, but we can tell you the topics. We do some natural language processing on the text and we can tell you “This is about Apple,” and we can tell it’s about Apple Computer and not the fruit.
JD: Another opportunity we are excited about his how Diffbot can help augment what is natively on the page. Just by dint of following so many pages through our system, we can augment [the existing formatting] and increase the value for whoever is reading. In the case of an article, the fact that we see so many articles means it’s relatively easy for us to generate tags for any given text.
X: How do you turn this all into a business?
MT: We are actually selling something. We are trying to build the Semantic Web, but in a profitable way. We analyze the pages that people pay us to analyze. That’s currently over 100 million URLs per month, which is a good slice of the Web. Other startups have taken the approach of starting by crawling and indexing the Web, and that is very capital-intensive. By doing it this way, another benefit is that people only send us the best parts of … Next Page »
More from Xconomy
Trending on Xconomy
(Page 4 of 4)
the Web. Most of the stuff a typical Web crawler goes through never appears in any search results. Most of the Web is crap.
X: Are people finding uses for the technology that you may not have thought of?
MT: We had a hackathon last year where a guy came in and built an app for his father, who is blind. It runs Diffbot on a page and makes it into a radio station. For someone who is blind, browsing a news site is usually a really poor experience. The usual screen readers will read the entire page, including the nav bars and the ads and the text. The screen readers have no context about what is important on the page. Using Diffbot to be his father’s eyes, this guy could parse the page and read it in a way that is much more natural.
JD: AOL’s Editions app is one of the more interesting use cases that I’ve seen. It’s an iPad app that features both their own content as well as snippets from across the Web, in a daily issue. I spent five years running engineering for the media solutions group at Cisco, selling a Web platform for media companies, and the biggest problem we faced was dealing with the excess of content management systems that all media companies have. In the case of Editions, AOL has myriad properties that they want to merge into this single app. But rather than consolidate TechCrunch and Engadget and the Huffington Post and a half dozen other sites, they use Diffbot to build a kind of content management system on the fly from the rendered Web pages. They extract the content and deliver it on the fly as if it came from a CMS right to the iPad magazine.
StumbleUpon is another interesting one. They use Diffbot as their moderation queue. Whenever a new website is submitted to their index, they want to make sure it’s legitimate before it’s available for stumbling. They have to rule out people who stumble a page, then swap it out for spam. So they run Diffbot on the source page, pipe that into their moderation queue, and if it looks like a legitimate page they can monitor that and keep checking on a regular basis to see how much it changes. If it has changed much between day 1 and day 10, it might warrant human intervention.
X: Aren’t there are a lot of news reader app these days that are doing the same thing you’re doing when it comes to identifying and isolating the text of a news article? That’s what Instapaper and Pocket and Readability and Zite are all doing.
MT: We power a lot of those apps. Our audience is the developers who work at those companies, who use our API to create their experience.
JD: We make it a lot more affordable to make those kinds of forays. When you look at building your own customized extraction tools, you are talking about multiple developers over weeks or months, to build something that is more brittle than what we offer out of the gate. Our ultimate goal is to be not only better but a lot cheaper than what you could build.
X: It’s not totally clear yet, though, whether publications or apps that aggregate lots of content from elsewhere, like Editions or even Flipboard, are going to be profitable in the long term, and where publishing is going as a business. Don’t you guys feel there’s some risk in tying your fortunes to such a troubled industry?
MT: The more interesting question is how do you monetize the Semantic Web, and where is the money in building the structured information. Articles are only one page type. Another that I mentioned is products. If you could show products on a cell phone, and people could buy the product and we could make that transaction happen, that is one very tangible way of making money. I think there is a lot of value in having structured information, because you can connect people more directly to what they want. Once we have the entire Web in machine-readable format, anybody who wants to use any sort of data can use the Diffbot view of it, and I think a lot of those apps can make money. Look at Siri—it’s great but it only works with the 10 or so sources that it’s hard-coded to work with. If you were able to combine Siri with Diffbot, Siri could operate on the Web and take a query and actually do it for you.
X: What page types will you move on to next? Did you start with articles because those are easiest?
MT: I wouldn’t say they were easiest, but they are pretty prevalent on the Web. A variety of factors help us prioritize what we should do next. One signal is what is the prevalence of that type of page on the Web. If doing one page type lets us knock out 30 percent of the Web, maybe we will go for it.
X: Will there always be a need for Diffbot, or with the transition to HTML 5, will Web pages gradually get more structure on their own?
MT: If you look at the ratio of unstructured pages to structured, it’s actually going in the opposite direction. I think human beings are creative, and they design pages for other humans. No matter what, people will find a way to create documents that lie outside of the well-defined tags, whether it’s HTML 5 or Flash or PDF or Xbox. What they all have in common is that they are just vessels that we can easily train and adapt Diffbot to work with.
Trending on Xconomy",-0.12528,http://www.xconomy.com/san-francisco/2012/07/25/diffbot-is-using-computer-vision-to-reinvent-the-semantic-web/,,
Diffbot launches world’s largest Knowledge Graph with 500 times more data than Google,d2018-08-30T00:00,Duncan Riley,SiliconANGLE,"Knowledge Graph,EBay,Cisco Systems,Google,Diffbot,Data","Palo Alto-based artificial intelligence startup Diffbot Inc. today launched what it claims is the world’s largest structured, enterprise-ready database of human knowledge ever created.
According to the company, the Diffbot Knowledge Graph is 500 times larger than Google LLC’s own Knowledge Graph and growing autonomously every day. It’s launching with more than 1 trillion facts and 10 billion entities and it’s growing by 130 million facts every month.
Managing and curating that amount of data involves the use of machine learning, computer vision and natural language processing with the service built specifically to provide knowledge as the end product, paid for and owned by the customer.
Diffbot raised a relatively small $10 million in a Series A venture capital round back in 2016 despite having been founded in 2010. That’s because it was somewhat remarkably both self-funded and profitable, a rarity among startups. Two years later, that hasn’t changed.
“Far from a theoretical research project in search of a business application, artificial intelligence is the backbone of Diffbot and the company uses state of the art AI methods to deploy profitable products at scale while also furthering the field by funding extensive research,” a Diffbot spokesperson told SiliconANGLE.
In 2018 Diffbot powers application for customers that include Salesforce.com Inc., Cisco Systems Inc., eBay Inc. and Yandex N.V. It remains profitable as it launches what potentially is a groundbreaking knowledge graph.
For all that, the scale of the Diffbot Knowledge Graph may cause concern in some quarters over its potential Big Brother nature. Although there’s no suggestion that Diffbot or its customers will use the service for nefarious purposes, the scope of the knowledge it contains about just everyone and everything on the planet is staggering.
The datasets within the Diffbot Knowledge Graph include personal information, including skills, employment history, education and social profiles. For corporations, it includes rich profiles of companies and their workforces globally, ranging from “Fortune 500 to SMBs.”
Location data includes mapping data, addresses, business types and zoning information. It’s also said to index every news article, dateline and byline from anywhere on the web in any language, along with scraping and indexing social media chats, social sharing and conversations everywhere from article comments to web forums such as Reddit.
“A Web-wide, comprehensive and interconnected Knowledge Graph has the power to transform how enterprises do business,” Mike Tung, founder and chief executive officer of Diffbot, said in a statement. “Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers.”
Tung said that Diffbot’s Knowledge Graph is the first one that organizations can use to access the full breadth of information contained on the web. “Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it,” he said.
Since you’re here …
… We’d like to tell you about our mission and how you can help us fulfill it. SiliconANGLE Media Inc.’s business model is based on the intrinsic value of the content, not advertising. Unlike many online publications, we don’t have a paywall or run banner advertising, because we want to keep our journalism open, without influence or the need to chase traffic.
The journalism, reporting and commentary on SiliconANGLE — along with live, unscripted video from our Silicon Valley studio and globe-trotting video teams at theCUBE — take a lot of hard work, time and money. Keeping the quality high requires the support of sponsors who are aligned with our vision of ad-free journalism content.
If you like the reporting, video interviews and other ad-free content here, please take a moment to check out a sample of the video content supported by our sponsors, tweet your support, and keep coming back to SiliconANGLE:",0.11308,https://siliconangle.com/2018/08/30/diffbot-launches-worlds-largest-knowledge-graph-500x-times-data-google/,,
Profitable AI startup Diffbot raises $10m Series A,d2016-02-11T00:00,DUNCAN RILEY,SiliconANGLE,"artificial intelligence,Knowledge Graph,Series A round,Diffbot,startup company,Tencent Holdings,Andy Bechtolsheim","Artificial intelligence startup Diffbot, Inc. has raised $10 million Series A in a round led by Tencent Holdings Ltd. that included Felicis Ventures, Valor Capital, Amplify Ventures, Andy Bechtolsheim, Bill Lee and Georges Harik.
Founded in 2010, Diffbot uses artificial intelligence (AI) technology to build a structured database of objects and facts from across the entire internet, which they claim is bigger than Google’s Knowledge Graph.
The company’s technology visually recognizes, reads, understands, and monitors Web pages and components including product pages, news articles, discussions/comments/forums, videos, pictures, and more, with each element then extracted, organized, tagged, cross-referenced, and stored as an “object” in the Global Index.
This organized data allows applications and machines to easily access every piece of information contained on the web.
Diffbot doesn’t provide a search service but instead offers businesses a host of turnkey APIs, automatic web-crawling, and bulk data processing capabilities, to allow applications to leverage previously unstructured Web data.
The company says that because it offers a structured, constantly updated database of information pulled from the internet, they can treat all of this information as a searchable “Big Data” set – pulling out only the pieces of information that are relevant to their company or product.
Even if you’ve not heard of them before, Diffbot is already providing services to a range of companies most would know, including Microsoft (Bing), Cisco, Adobe, Yandex, eBay, and many others.
Profitable
It’s rare to see a unicorn that is profitable, but Diffbot goes into the round with a proven business model that is making serious money.
“What truly makes [Diffbot] an amazing investment is that, unlike many ‘unicorns,’ Diffbot has identified a high-margin, high-leverage business model in AI that is only limited in its data collection capabilities by the size of an easily expanded data center,” Aydin Senkut of Felicis Ventures said in a statement sent to SiliconANGLE.
Dig below the surface and the numbers are also truly amazing, with Diggbot’s “Global Index” containing more than 1.2 billion objects, and is adding another 10 million every single day, and all using AI without being manually curated.
Including the new round Diffbot has raised $12.5 million to date.
The company said it would use the new funding to expand their already profitable technology across the entire Web and to add more world-class AI experts to their 12 person team.
Image credit: Diffbot/ screenshot",0.35768,https://siliconangle.com/blog/2016/02/11/profitable-ai-startup-diffbot-raises-10m-series-a/,,
Diffbot Raises $10M To Expand AI Engine That Mines The Web,d2016-02-11T00:00,Bernadette Tansey,Xconomy,"artificial intelligence,Tencent,Allen Institute for Artificial Intelligence,Felicis Ventures,Palo Alto,Diffbot,World Wide Web,Mike Tung,Knowledge Graph,company","Diffbot, an artificial intelligence company that helps clients extract and combine data from multiple Web sources, announced today it raised $10 million from investors including Tencent and Felicis Ventures to expand its “knowledge-as-a-service” offerings to businesses and consumer apps.
The Palo Alto, CA-based startup, founded in 2009, still has a tiny staff of 14. But Diffbot’s ambition is huge: to catalog trillions of facts across the Web—many of them drawn from page elements such as comment forums, which can’t be mined by traditional search engines. The startup says it has made a significant start on that goal, having indexed 1.2 billion entities such as people, products, and places since the middle of last year. Its Global Index also encompasses 10 to 20 times that number of facts, says Diffbot founder and CEO Mike Tung. Last June, the company said its database had surpassed the size of Google’s Knowledge Graph.
Some big customers are putting Diffbot’s technology to use. Cisco and Adobe are sifting through comment sections to monitor customer feedback about their products, Tung says. Other companies use Diffbot to comb online professional profiles and assemble lists of hiring prospects. Apps powered by Diffbot can address specific consumer needs, such as a comparison of prices for a coveted mountain bike. Businesses also use Diffbot’s services to track their competitors and develop sales leads. Search engines such as Microsoft’s Bing use it to enhance the results from user queries.
Cybersecurity may also emerge as one of Diffbot’s client areas.
Diffbot works in any language, so it can parse message forums in Arabic, for example, Tung says. “It can tell you who the speakers are, and what they’re saying,” he says. Tung declined to name any customers Diffbot may have attracted from the information security sector. But he says the company’s technology is “sufficiently powerful to reduce information asymmetry.”
With more than 250 customers—including Amazon, CBS Interactive, eBay, Instapaper, Microsoft, Salesforce, and Samsung—Diffbot became profitable at the end of 2015, Tung says.
“We’ve proven it’s possible to build a profitable AI business model,” Tung says.
Diffbot isn’t disclosing its revenues at this point, but Tung says they cover operating expenses. These costs are low because Diffbot’s automated data collection and analysis technology requires no human curation, he says.
The startup’s key early innovation was to extend the search function into previously uncharted territory by teaching computers how to recognize the various sub-sections of Web pages, including headlines, ad boxes, pictures, and discussion threads. Diffbot could then classify each page by type, such as news articles and product pages. That knowledge allows the computers to find and assemble related information, such as product prices across various retailers, and consumer opinions across many social media platforms and comment sections. The technology creates “structured data” that machines can read and interpret, as the company describes it.
Diffbot has been scaling up its data center, adding to its bank of proprietary servers with specialized hardware, and integrating Web-based processing power into the system to meet surges of demand. The company’s new money will accelerate the scale-up and fund an expansion of its R&D team, Tung says.
“We wanted to see the future happen quicker,” Tung says. “Money contracts time.”
The second goal for Diffbot’s $10 million Series A financing round was to make alliances with investors experienced in artificial intelligence, Tung says. The round was led by Tencent, China’s leading Internet service provider, and Felicis Ventures. Other participants include Andy Bechtolsheim, the co-founder of Sun Microsystems; Amplify Ventures; Valor Capital; Bill Lee, an early investor in SpaceX and Tesla; and artificial intelligence expert Georges Harik, an early Google staffer.
Tencent is one of the big Asian companies investing in U.S. tech companies, both for financial and strategic reasons. Tencent is not a customer of Diffbot’s, Tung says. He adds the word “now.”
In 2015, Tencent invested in a number of Bay Area companies, including mobile game company PocketGems and app finder Vurb—both based in San Francisco—and Redwood City-based virtual reality startup AltspaceVR.
Tung declined to disclose the company’s valuation for the Series A round, which brings Diffbot’s total fundraising to $13 million.
The research groups at Google and Facebook are Diffbot’s closest rivals in the development of methods to gather and synthesize Web data using artificial intelligence technology, Tung says. But rather than keeping the knowledge in-house, Diffbot is making it available to outside companies, Tung says.
“We’re sort of like Switzerland in the AI wars,” Tung says.
Other startups are pursuing a similar AI-as-a-service model, recognizing that while the Internet giants have the resources to push the envelope in things like computer vision and natural language understanding, lots of companies can benefit from these technologies. One example is Seattle-based KITT.ai, a spinout from the Allen Institute for Artificial Intelligence that is working on natural language understanding.
Web-mining can be a competitive advantage for apps as well as the proliferating devices of the Internet of Things, Tung says.
“Everything’s becoming intelligent, but the limiting factor of intelligence is access to structured data,” Tung says.
Bernadette Tansey is Xconomy's San Francisco Editor. You can reach her at btansey@xconomy.com. Follow @Tansey_Xconomy
More from Xconomy
Trending on Xconomy",,http://www.xconomy.com/san-francisco/2016/02/11/diffbot-raises-10m-to-expand-ai-engine-that-mines-the-web/,,
Data Extractor Diffbot Wants To Turn The Web Into The Semantic Web,d2016-02-11T11:00,Barry Levine,MarTech Today,"software release life cycle,semantics,Palo Alto,web content,Diffbot,Semantic Web,many companies,Global Index,Knowledge Graph,John Davi","Many companies grab web content, analyze it and return stats on sentiment, product mentions and the like.
But startup Diffbot says it takes a different approach that can automatically sort the web into human-like categories of knowledge.
And today the Palo Alto, California-based company is announcing a new Series A round of $10 million in investment funding, which will back the expansion within a few weeks of its newest tool — a massive structured database called the Global Index — from a closed beta phase to general availability.
Founded in 2008, the company specializes in automatically extracting the unstructured content on web pages, categorizing it using artificial intelligence, computer vision and natural language processing, and then storing it by data type in a structured database.
It may be obvious to a human that, for example, an image on a retailer’s page is of a pair of shoes, this number on the page is the price and this abbreviation is the color. But unless the page has been marked up in XML or other semantic marking to identify which info is a color, a crawler and the processing engine won’t be able to store “BR” as the color for this pair of sneakers or “$100” as its price.
Semantic Web Content
Diffbot grabs the info at the URL, renders the page inside its system and employs computer vision to visually analyze the page’s structure.
Essentially, Diffbot is creating semantic Web content — that is, information that is characterized by its meaning — even though the page hasn’t been formatted that way. It can automatically detect product, article, image, video, author, date, discussion threads, pricing info, product IDs like SKU, brand, video thumbnail and other categories.
VP of product John Davi told me it can also scan images and find, for instance, all photos of Barack Obama wearing a blue tie.
Each page element — headline, photo, SKU and so on — is stored separately and made available for searching. Here, for instance, is a Diffbot-generated breakdown of a story I posted yesterday:
Diffbot has been providing what Davi called a “web reading robot” in support of specific applications. Instapaper, for instance, utilizes Diffbot to capture articles, identify and store its elements (title, story, images and so on), and then make them available for offline reading later.
Similarly, Cisco has used its service to monitor forums to automatically capture, store and categorize comments about products and those of its competitors. Other customers include Microsoft’s Bing, Duck Duck Go, eBay and Adobe.
“It’s A Big Web Out There”
Davi said the company has been beta testing the Global Index since last summer. One test, for example, ranked travel brands according to the kinds of sentiments found on forums.
The idea of the Index is to build a huge structured database of sorted, web-based knowledge that developers can tap for marketing or other uses, or for applications. Eventually, he indicated, the company would like to make it available via a dashboard as a searchable knowledge base of web content for marketers and other non-technical users.
In many ways, the Global Index is comparable to Google’s Knowledge Graph, which also categorizes info on the web into usable and related knowledge. But, Davi said, the Google effort is based on Wikipedia, the database from its Metaweb acquisition, several other sources and human efforts. It’s also available only through Google’s search engine, while the Global Index will shortly be open to the public.
Diffbot says its index, which has been autonomously spidering only since the summer, already contains more than 1.2 billion objects, where an object is an assembly of data representing some useful piece of knowledge, like a product. Google’s Knowledge Graph, it says, has only recently passed a billion objects after some years.
The initial focus of the Index has been on news and information, but the company has a much bigger ambition: to categorize most of the business-valuable information on the web. That will take at least three to five years, Davi acknowledges.
“It’s a big web out there,” he pointed out.
About The Author",,http://marketingland.com/data-extractor-diffbot-wants-to-turn-the-web-into-the-semantic-web-163936,,
Diffbot Makes The Web Machine-Readable,d2012-03-23T00:00,,WebProNews,"World Wide Web,technology","Microsoft's Bing service has a cool promotion vehicle known as Bing Booster. It helps tech startups connect and collaborate with other startups and people in their field. One of the events Bing helps sponsor is LAUNCH, an event that give startups a platform to launch from.
One of the startups that Bing brought to LAUNCH this year is really interesting. The company is called diffbot, and they have a mission. They want to make the entire Web machine-readable. What does that mean? According to the official Web site, diffbot is a ""visual learning robot that enables developers to easily use Web content in their apps.""
So diffbot lets a robot read the Web regardless of layout, design or language. That's pretty cool, but what kind of applications would it have for the Web. The diffbot team lays out its potential uses on the BingBooster Web site:
Using Diffbot’s existing Article API (which automatically parses blog posts or news articles into machine- app-friendly XML or JSON) to migrate users’ blogs — regardless of existing platform — to a new blog-platform provider.
Providing Diffbot’s forthcoming Product API (which can parse product pages of any type across the web) with additional training data or edge cases from existing product-search and price-comparison applications.
Leveraging Diffbot’s language agnosticism to help power a multi-language tablet newsreader focused on global content and stories.
Powering a forthcoming event and activity search engine using Diffbot’s impending events page functionality.
Using our image-identification and extraction capabilities to power a personal photo-book printing service, allowing the inclusion of photos from any gallery software or photo-hosting services.
The LAUNCH event was a great chance for the diffbot team to meet various people that were interested in using their API as well as helping them find new uses for their technology.
The BingBooster Web site says this is just the first in a series of startups that showed off their product at LAUNCH. There will be more development technologies being shown during the coming days.
Here's a presentation of diffbot at the DEMO Enterprise Disruption 2012 from a few months ago:",0.123,http://www.webpronews.com/diffbot-makes-the-web-machine-readable-2012-03/,,
Diffbot Challenges Google Supremacy With Rival Knowledge Graph,d2015-06-04T00:00,Bernadette Tansey,Xconomy,"search and seizure,Knowledge Graph,web page,mobile device,Google,Bloomberg Beta,1000000000,price,World Wide Web,Diffbot","When you do a Google search at your desktop for a common health condition, you’ll get links to tons of webpages you can sift through in hopes of finding the specific facts you want.
But if you’ve searched Google from a mobile device recently, you may have been rewarded with a summary of important facts about the disorder, culled for you from many websites. You’re tapping into what Google calls its Knowledge Graph.
Palo Alto, CA-based artificial intelligence startup Diffbot has based its whole business on that second kind of search—ferreting out data points scattered across many websites and pulling them together into Big Data resources that can be queried, combined, and rearranged. The upstart company—14 engineers in a backyard bungalow—now says its own data mega-map, called its Global Index, is a bigger database than Google’s Knowledge Graph of billions of facts.
This kind of structured data—Web facts organized into a searchable database—is the resource behind the most popular mobile apps, says Diffbot founder and CEO Mike Tung. Such apps can answer questions like, “What is the best Thai restaurant in this neighborhood?” Diffbot’s mission is to capture everything online—articles, images, videos, comments, reviews, the works—and keep it updated.
“We are working to create a structured version of the Web,” Tung says. “We’re quite serious about that.”
The company developed elements of its Web-crawling methods as it served customer needs over the past few years, but only started proactively spidering the Web for its own purposes in the past few months. Its Global Index now contains more than 600 million objects (this can be anything from a celebrity to an Ikea chair model) and 19 billion facts. Diffbot clocks Google’s Knowledge Graph at about 570 million objects and 18 billion facts.
Diffbot, founded in 2008, is already covering its operating expenses by enhancing other search engines including Microsoft’s Bing and DuckDuckGo, and by powering apps for companies such as Cisco and AOL, Tung says. Diffbot subscribers can build apps based on narrowly targeted searches that answer questions such as, “What’s the best price in my region for Nike cross trainers?”
But Diffbot has larger ambitions, and it’s raising money to support them. The company just banked $500,000 from Bloomberg Beta, bringing an angel round up to $3 million. It wouldn’t be surprising to see a Series A round raised this year, Tung says. Just a hint about Diffbot’s ultimate interests: According to its CEO, Diffbot may help answer the long-debated question: Can computers ever duplicate human intelligence?
Diffbot has been exploring the art of teaching machines to function like a human researcher—compiling facts from multiple online sources so they can be combined and compared for many purposes. The company began building its Global Index by storing results from URL searches requested by customers, but in recent months Diffbot has been analyzing websites to build its index at a rate of up to 15 million pages a day.
Its artificial intelligence bots are doing the work without human supervision. Tung says Google’s Knowledge Graph, by contrast, has relied significantly on human curation.
“Our approach is fairly radical in that there’s no human behind the curtain,” Tung says. “This is why we were able to catch up in such a short time.”
Diffbot assembles its own servers at its Palo Alto bungalow. They’re not the kind you can rent from a cloud storage outfit. The company’s standard crawling and indexing machines use 32 terabytes of solid-state storage, have 192GB of RAM, and 40 CPU cores. Diffbot now has 100 servers in a guarded co-location space in Fremont, CA, where fiber optic cables link them to all the Internet service providers in the world, Tung says.
The crawlbots are adding millions of new objects to Diffbot’s index every day. Tung is envisioning adding thousands more servers, or tens of thousands.
“If we just throw more resources at it, we can generate structured data at true scale,” Tung says.
Last year, Xconomy’s much-missed San Francisco editor Wade Roush asked the question, “Could a Little Startup Called Diffbot Be the Next Google?” in his article about Diffbot’s mission to cover the Web more fully by taking search further than conventional search engines. Diffbot had developed bots that can “read” a webpage the way humans do, distinguishing among different parts of the layout such as headlines, main text, side columns, and so on.
With this computer vision, the machines can tell the difference between types of Web pages—article pages, home pages, and product offerings where prices are displayed. They can reshuffle these layout elements to reformat a Web page for mobile device screens—a chore that companies pay Diffbot to take on, and one of its early sources of revenue.
The bots also “learn” where they’re likely to find certain information on a page, such as prices or author names on articles. They can extract information from images, videos, blogs, and the discussion threads that follow published articles. The company’s newest product, Discussion API, has become a tool for marketers who want to check brand reputations, Tung says.
Bernadette Tansey is Xconomy's San Francisco Editor. You can reach her at btansey@xconomy.com. Follow @Tansey_Xconomy
Trending on Xconomy
(Page 2 of 2)
assemble information about competitors and suppliers, or people they might want to hire.
“All these entities leave footprints on the Web,” Tung says. Diffbot is now developing frameworks for machine analysis of new kinds of pages, such as events, locations, and profile pages, to further expand its Global Index.
If you’re a high school freshman with an English term paper due tomorrow, you might be wondering how you can log in to this new type of search to get all the facts you need about Charles Dickens, by midnight tonight, painlessly assembled by a machine.
For the most part, though, consumers can’t yet directly tap into the structured Web data compiled by Diffbot and Google. Diffbot shares its data resources with consumers indirectly by selling its services to search engines and app developers. A Bing search about a product, for example, will show the traditional list of website links, but in the upper right-hand corner, it may display an image, price, and other facts assembled about the product—structured data.
Google is also making some of its Knowledge Graph findings available through mobile searches. But Tung speculates that Google may limit these kinds of search returns in favor of traditional Web page listings, because they expose consumers to more advertising, the tech titan’s source of revenue.
In his 2014 article, Roush pondered what would happen if Diffbot remained an independent company, grew to 10,000 employees, and vied with Google to control our online existence. But Roush, wistfully, found it more likely that Diffbot would be “acqui-hired” by Google at some point.
Tung brought up this prediction when I talked to him this week—mainly to refute it. He didn’t sound like a guy who was ready to let somebody else discover his company’s full potential.
“We have received a lot of acquisition offers from pretty much all of the large technology companies,” Tung says. Diffbot’s response to the offers, he says, is to convert its suitors into customers. He declines to say whether Google belongs to either category.
So what future is Tung aiming an independent Diffbot towards?
My sense is that Tung is an artificial intelligence researcher at heart, captivated by questions about what machines could do if humans knew how to equip their silicon brains and train them expertly.
Here’s part of his shorter-term vision: Technologists are not just organizing Web data to better inform humans so they can decide what to do or to think. They’re also structuring the Web to better inform machines, so they can take action themselves and work with other machines.
The first products along these lines may be as cozy as recipe apps. You might be able to point your mobile phone at an unfurnished corner of your new living room, so that in a few minutes it will tell you what chair on the market would fit in the space, and look good with the rest of your decor, Tung says.
He gave another example: The printer in your office runs out of ink, knows whether it needs a black or color cartridge, knows which manufacturers’ products are compatible, taps into the Web, compares prices, and executes the order.
“We think that’s the exciting future,” Tung says. “Everything’s intelligent, and they all need access to information.”
But beyond those limited chores, there’s still a bg question out there: Will machines ever duplicate human intelligence? For example, could they exercise judgment by balancing the benefits and consequences of two different courses of action, such as choices between medical treatments or business strategies?
“Progress toward human intelligence is still quite a rocky road ahead,” Tung says. The artificial intelligence community hasn’t yet hit on the missing link that would make that possible, he says. “It’s going to require a breakthrough that’s still unknown.”
But the Diffbot team is trying to make training computers more sophisticated by rendering the Web—the digital repository of a growing swath of human knowledge—readable by machines.
The big successes already gained in artificial intelligence have not necessarily come from new programming wizardry, but through the application of old algorithms to dense data sets, Tung and fellow Diffbot executive John Davi point out.
Achievements in computer image classification have built on the burgeoning population of digitized images that were not available in earlier decades, they say. The IBM computer Watson performed feats of medical diagnosis by drawing from a concentrated trove of human-curated knowledge in that relatively narrow realm of data, Tung and Davi add.
“Our working theory is, if we can assemble enough structured, labeled data, we can simulate all aspects of human intelligence,” Tung says. The inflection point in artificial intelligence may be assembling trillions of objects from the Web that machines can read, he says.
“We’re working on what could be the missing piece, which is the data,” Tung says.
Bernadette Tansey is Xconomy's San Francisco Editor. You can reach her at btansey@xconomy.com. Follow @Tansey_Xconomy
Trending on Xconomy",,http://www.xconomy.com/national/2015/06/04/diffbot-challenges-google-supremacy-with-rival-knowledge-graph/?utm_source=related-content&utm_medium=link&utm_campaign=related-content,,
Diffbot Turns Online Comments Into Market Intelligence Databases,d2015-03-31T00:00,Bernadette Tansey,Xconomy,"revenue,database,comment,Internet forum,Diffbot,Market intelligence","Let’s say you’re a tech enterpreneur whose groundbreaking gizmo got rave reviews in influential publications, spurring months of lucrative sales. But later, sales slide downward, and you don’t know why.
Turns out, a rumor is circulating in discussion forums that your product has a high failure rate after six months of use-a mistaken impression or a deliberate lie. How would you detect those inaccurate comments as soon as they appear, so you can publicly refute them?
It’s the kind of challenge that Diffbot, a small Palo Alto, CA-based artificial intelligence company, set out to help companies meet. Diffbot announced today that it has created a new search tool, Discussions API, that digs for product mentions in comment threads, community forums and online reviews-the hidden crannies of the “deep Web” it says Google doesn’t fully explore.
“Engaged commenters could be misrepresenting a brand,” Diffbot product executive John Davi says. “Now that’s easy to find.”
While traditional search engines routinely pull up lists of published articles about products, Diffbot indexes the online conversation that follows below those stories. The volume of this user-generated commentary could be as much as 400 times larger than the “surface Web” of mainstream media, organizational websites, and other content easily accessible with conventional search engines, Diffbot says.
While companies often search Twitter feeds as part of their media monitoring routines, Davi says some of the most influential consumer backchat may take place in other forums where writers aren’t limited to 140-character messages. And those other comment sections don’t have their own built-in search functions, as Twitter does. Diffbot is now making those forums searchable.
While a Google keyword search might turn up a few individual comments found on the Web, Diffbot scans the discussion sections across multiple websites and returns the results to the customer in a database format that summarizes many comments, Davi says. Through the use of artificial intelligence and robot technology, Diffbot extracts key details from each comment, including the author, author url, the site where the comment appeared, and the nature of the opinion expressed.
The database can identify positive trends in public opinion as well as damaging misconceptions, Davi says. Product makers gain the ability to reach out to consumers whose favorable comments could be used in company marketing campaigns, he says.
Discussion API searches opinion forums including Facebook Comments, Disqus, WordPress, Blogger, and Reddit.
“We actually expect this to be a pretty hot commodity,” Davi says. The idea to search comment sections arose both from Diffbot’s staff and from its existing customers, he says.
Diffbot’s new Discussions tool builds on other innovations the company has made to expand the searchable territory on the Web. Founded in 2009, Diffbot first attacked a blind spot in the way a robot brain “reads” a website. Unlike a human being, a traditional search engine can’t distinguish well between the different sections of a webpage, such as a story headline, the author’s byline, an image, and the body of the story.
Using machine learning and human trainers, Diffbot taught its robot brain to recognize these layout components in a variety of different webpage types, such as a front page or home page, an article, an e-commerce site displaying product information, and pages containing images or videos.
Customers such as Instapaper use this Diffbot layout-reading function to reformat Web content for use on mobile devices. Diffbot automatically reshuffles the positions of headlines, text blocks, and other components to fit the different dimensions of a smartphone or tablet screen.
But Diffbot also uses the layout-reading function to find data such as product prices-its robot brain now knows which page areas to search for that information. Customers like Pinterest might use the price information Diffbot extracts from multiple locations to help its users with comparison shopping. Product wholesalers use Diffbot’s aggregate reports of current price data to make sure stores aren’t violating agreements to charge customers the suggested manufacturer’s retail price, Davi says.
“We’ve built a fairly tidy business on that,” Davi says of the page-reading capabilities that were Diffbot’s first commercial services. He declined to disclose company revenue numbers. Customers of the 12-employee company include Adobe, CBS Interactive, Cisco, eBay, Salesforce, and Samsung.
Diffbot has raised a total of $2 million since it was founded, and doesn’t need to raise capital for a while, Davi says.
“We’ve had break-even months, but then we keep hiring people,” he says.
Bernadette Tansey is Xconomy's San Francisco Editor. You can reach her at btansey@xconomy.com. Follow @Tansey_Xconomy
Trending on Xconomy",-0.15068,http://www.xconomy.com/san-francisco/2015/03/31/diffbot-turns-online-comments-into-market-intelligence-databases/,,
Diffbot attempts to create smarter AI that can discern between fact and misinformation,d2020-09-07T01:15,The Financial Express,The Financial Express,"artificial intelligence,Diffbot","The better part of the early 2000s was spent in creating artificial intelligence (AI) systems that could beat the Turing Test; the test is designed to determine if an AI can trick a human into believing that it is a human. Now, companies are in a race to create a smarter AI that is more knowledgeable and trustworthy. A few months ago, Open AI showcased GPT-3, a much smarter version of its AI bot, and now as per a report in MIT Technology Review, Diffbot is working on a system that can surpass the capabilities of GPT-3.
Diffbot is expected to be a smarter system as it works by reading a page as a human does. Using this technology, it can create knowledge graphs, which will contain verifiable facts. One of the problems that constant testing of GPT-3 reveals is that you still need a human to cross-verify information it is collecting. The Diffbot is trying to make the process more autonomous. The use of knowledge graphs is not unique to Diffbot; Google also uses them. The success of Diffbot will depend on how accurately it can differentiate between information and misinformation.
Give it will apply natural language processing and image recognition to virtually billions of web-pages, the knowledge graph it will build will be galactic. It will join Google and Microsoft in crawling nearly the entire web. Its non-stop crawling of the web means it knocks down its knowledge graph periodically, incorporating new information. If it can sift through data to verify information, it will indeed be a victory for internet companies looking to make their platforms more reliable.",0.435,https://www.financialexpress.com/industry/technology/diffbot-attempts-to-create-smarter-ai-that-can-discern-between-fact-and-misinformation/2076463/,,
http://www.outlookindia.com/magazine/story/shahrukh-khan/230120?utm_r=false:777 in global code Status: 200 OK X-Diffbot-Render-Hostname: morpheus Content-Type: text/html; charset=utf-8 X-Location: http://www.outlookindia.com/magazine/story/shahrukh-khan/230120,,,http://www.outlookindia.com/,,,,http://www.outlookindia.com/magazine/story/shahrukh-khan/230120,,
Diffbot’s A.I. Engine Draws Global Map of Machine Learning Expertise,d2018-12-05T00:00,Bernadette Tansey,Xconomy,"expert,artificial intelligence,People's Republic of China,Diffbot,machine learning,engine,United States of America,India,United Kingdom,New York","(Page 2 of 2)
expertise of a candidate through traditional means such as interviews and testing, Tung says.
Once the machine learning experts were compiled as a cohort, Diffbot’s group data could be sorted out by various factors, such as national origin, place of current employment, gender, education, and professional background, Tung says.
For example, Diffbot found that women make up a bit more than 24 percent of U.S. machine learning experts—a gender diversity score that was lower than that in China and four other countries. More than 51,000 women are employed as machine learning professionals in the United States, Diffbot found. Tung says this identified talent pool could be a hiring resource for companies trying to correct a gender imbalance caused by institutional bias.
With its 221,592 experts, the United States employs 30.8 percent of the global talent pool in machine learning, followed distantly by India, where 59,980 are employed, Diffbot found. Ranked next are the United Kingdom, Canada, China, and France. If California were a country, it would rank above India. It employs 74,791 machine learning professionals—more than New York, Texas, and Massachusetts combined.
In U.S. hiring, Google and Microsoft led the pack, with more than 4,000 machine learning experts each. That’s about four-fold higher than Apple’s count of 1,064, Diffbot reported. Tung says that financial analysts who track the market for technologies involving machine learning might use Diffbot’s engine in a search for correlations between expert staff strength and the performance of new products.
The Diffbot data may also provide some insights to add to the public discussion of a possible “A.I. war” between the United States and Asia, Tung says. The company’s report found that five of the top ten universities producing global talent in machine learning are in China. But the employment pattern suggests a “brain drain” from China. Among graduates from those Chinese university programs, more than 62 percent work in the United States, Diffbot found.
“In the study, most of U.S. A.I. research is being carried out by Chinese or Indian nationals,” Tung says. “A.I. development in the U.S. is very co-dependent on Asia.”
Tung raises a caveat to Diffbot’s findings about staff strength in China, however, because A.I. experts in China are less likely than U.S. professionals to create online materials that Diffbot can scan, such as their own Web pages.
Diffbot’s report doesn’t resolve a burning question about the A.I. job market: How big is the shortfall between the number of trained professionals and the number of open jobs? That widely held perception of a significant candidate shortage has helped drive up compensation for A.I. experts.
Tung says Diffbot doesn’t yet capture structured data about jobs and job postings—but it plans to, he says.
“It’s on our roadmap,” Tung says.
Photo courtesy of Diffbot",,https://xconomy.com/san-francisco/2018/12/05/diffbots-a-i-engine-draws-global-map-of-machine-learning-expertise/2/,,
http://www.outlookindia.com/magazine/story/recommendations/295280?utm_r=false:757 in global code Status: 200 OK X-Diffbot-Render-Hostname: morpheus Content-Type: text/html; charset=utf-8 X-Location: http://www.outlookindia.com/magazine/story/recommendations/295280,,,http://www.outlookindia.com/,,,,http://www.outlookindia.com/magazine/story/recommendations/295280,,
Diffbot’s A.I. Engine Draws Global Map of Machine Learning Expertise,d2018-12-05T00:00,Bernadette Tansey,Xconomy,"San Francisco,Expert,Artificial intelligence,Map,Tencent,Diffbot,Element AI,Machine learning,Engine,Mike Tung","Xconomy San Francisco —
A year ago, the leading Chinese Internet company Tencent Holdings pegged the global number of artificial intelligence researchers and professionals at 300,000 or less—just as the unmet demand for such experts was pushing salary offers to as much as $1 million. In February, the Canadian firm Element AI estimated that talent pool at no more than about 90,000, Bloomberg reported.
Now, Silicon Valley company Diffbot has used its A.I.-powered fact-mining engine to comb the global Web and make its own census of skilled people in the field.
“We found that there’s much more,” Diffbot co-founder and CEO Mike Tung says. Rather than looking for all A.I. talent, Mountain View, CA-based Diffbot searched only for people with expertise in machine learning, a much-valued specialization within A.I. In a report released this week, the company says it found 720,325 professionals with machine learning expertise, and 221,592 of them are in the United States alone. Diffbot says it’s the single largest survey of machine learning skills ever compiled.
The report demonstrates the growing potential of A.I. software to quickly amass data and analyze it, compared with traditional methods such as time-consuming surveys of limited population samples, and the extrapolation of those results to estimate the size of entire groups. Diffbot’s study may also contribute more granular evidence showing which countries are leading the way in A.I. technology development, and whether the field’s much-lamented talent shortage is as deep as hiring companies fear.
The fastest-growing job category in a 2017 study by LinkedIn was machine learning engineer. These experts design advanced software systems that can change their behavior based on “insights” from the results of their earlier actions.
Tung says Diffbot’s search method, which employs software that uses A.I. technologies such as machine learning, computer vision, and natural language processing, allowed the company to turn the Web into a sensor that used A.I. to detect professionals with advanced A.I. skills.
“Like Cerebro in ‘X-Men,’ you can find all the mutants in the world,” he says.
By sharing summary data about the thousands of experts it found, Diffbot is revealing what Tung calls a competitive advantage that his company has long held in its own hiring of A.I. experts. Diffbot has deployed its automated Web-scouring engine to find the types of job candidates who are key to its progress in the development of that core product itself.
“We’ve been using it for this reason for many years,” Tung (pictured in center above) says.
Now, by releasing an analysis of the database of machine learning talent it compiled, Diffbot is hoping to demonstrate the kind of information that clients can derive by the same means—about A.I. workers and many other topics.
Diffbot’s customers can query its “Knowledge Graph” of more than a trillion facts gleaned about 10 billion “entities,” which include people and products.
Diffbot has extended the reach of Web data capture by scanning types of items not usually tracked by search engines, such as advertisements, images, and the reader comments posted below articles. The system finds connections among the facts scooped up from these public sources—like linking a product’s description to all the prices for it found in current ad displays. Diffbot structures the facts within its searchable Knowledge Graph, which is continually updated.
Diffbot, founded in 2008, attracted customers including Cisco (NASDAQ: CSCO), Salesforce (NYSE: CRM), and Crunchbase while operating in beta mode until August, when it opened up its “knowledge-as-a-service” tool to the general public.
The August announcement sparked interest among new customers, Tung says, and Diffbot’s staff of about 30 are working to help these potential clients integrate the company’s services into their existing systems.
The company’s machine learning expertise report is Diffbot’s first major release of a study based on data in its Knowledge Graph, Tung says. Diffbot may produce more such reports if people find them interesting and useful, he says.
The global machine learning expertise map
Tung says Diffbot found a significantly greater number of A.I. experts than Element AI or Tencent (which is one of Diffbot’s investors) because its Web-crawling engine casts a much wider net, and works in multiple languages. Montreal-based Element AI had relied on LinkedIn profiles to estimate the number of A.I. professionals, according to Bloomberg.
To find people who identified themselves as skilled in machine learning techniques, Diffbot’s algorithms scanned an array of document types, including publicly posted resumés, curriculum vitae, personal Web pages, company staff biographies, university faculty directories, news articles, scholarly publications, papers found through searches of Google Scholar, and professional sites such as GitHub’s.
Tung says top academics and machine learning experts at companies are more likely to be found through these sources than through LinkedIn, where new graduates and jobseekers commonly create profiles.
Diffbot’s count includes a wide variety of professionals, including those who are not PhD’s. It encompasses top engineers who can build entire machine learning systems and practitioners who can write code. Diffbot’s Web-crawling engine picks up on terms in addition to “machine learning” that indicate when a person is involved in the field, including “neural networks,” and “TensorFlow.’’
On the other hand, it would not award a place on the experts’ list to the drummer for a band dubbed “Machine Learning,’’ Tung says. Due to the search engine’s natural language processing capablities, it can identify the sense in which a term is being used, he says.
Companies using the database as a recruiting resource would, of course, have to verify the … Next Page »
(Page 2 of 2)
expertise of a candidate through traditional means such as interviews and testing, Tung says.
Once the machine learning experts were compiled as a cohort, Diffbot’s group data could be sorted out by various factors, such as national origin, place of current employment, gender, education, and professional background, Tung says.
For example, Diffbot found that women make up a bit more than 24 percent of U.S. machine learning experts—a gender diversity score that was lower than that in China and four other countries. More than 51,000 women are employed as machine learning professionals in the United States, Diffbot found. Tung says this identified talent pool could be a hiring resource for companies trying to correct a gender imbalance caused by institutional bias.
With its 221,592 experts, the United States employs 30.8 percent of the global talent pool in machine learning, followed distantly by India, where 59,980 are employed, Diffbot found. Ranked next are the United Kingdom, Canada, China, and France. If California were a country, it would rank above India. It employs 74,791 machine learning professionals—more than New York, Texas, and Massachusetts combined.
In U.S. hiring, Google and Microsoft led the pack, with more than 4,000 machine learning experts each. That’s about four-fold higher than Apple’s count of 1,064, Diffbot reported. Tung says that financial analysts who track the market for technologies involving machine learning might use Diffbot’s engine in a search for correlations between expert staff strength and the performance of new products.
The Diffbot data may also provide some insights to add to the public discussion of a possible “A.I. war” between the United States and Asia, Tung says. The company’s report found that five of the top ten universities producing global talent in machine learning are in China. But the employment pattern suggests a “brain drain” from China. Among graduates from those Chinese university programs, more than 62 percent work in the United States, Diffbot found.
“In the study, most of U.S. A.I. research is being carried out by Chinese or Indian nationals,” Tung says. “A.I. development in the U.S. is very co-dependent on Asia.”
Tung raises a caveat to Diffbot’s findings about staff strength in China, however, because A.I. experts in China are less likely than U.S. professionals to create online materials that Diffbot can scan, such as their own Web pages.
Diffbot’s report doesn’t resolve a burning question about the A.I. job market: How big is the shortfall between the number of trained professionals and the number of open jobs? That widely held perception of a significant candidate shortage has helped drive up compensation for A.I. experts.
Tung says Diffbot doesn’t yet capture structured data about jobs and job postings—but it plans to, he says.
“It’s on our roadmap,” Tung says.
Photo courtesy of Diffbot",,https://xconomy.com/san-francisco/2018/12/05/diffbots-a-i-engine-draws-global-map-of-machine-learning-expertise/,,
http://www.outlookindia.com/magazine/story/jaya-bachchan/284659?utm_r=false:603 in global code Status: 200 OK X-Diffbot-Render-Hostname: smith02 Content-Type: text/html; charset=utf-8 X-Location: http://www.outlookindia.com/magazine/story/jaya-bachchan/284659,,,http://www.outlookindia.com/,,,,http://www.outlookindia.com/magazine/story/jaya-bachchan/284659,,
Diffbot Is Using Computer Vision to Reinvent the Semantic Web,d2012-07-25T00:00,Wade Roush,Xconomy,"Wade Roush,John Davi,Skype,Google Hangouts,AT&T,Diffbot,Mike Tung,Computer vision,Semantic Web","You know how the Picturephone, a half-billion-dollar project at AT&T back in the 1960s and 1970s, turned out to be a huge commercial flop, but two-way video communication eventually came back with a vengeance in the form of Skype and FaceTime and Google Hangouts? Well, something similar is going on with the Semantic Web.
That’s the proposal, dating back almost to the invention of the Web in the 1990s, that the various parts of Web pages should be tagged so that machines, as well as people, can make inferences based on the information they contain. The idea has never gotten very far, mainly because the burden of tagging all that content would fall to humans, which makes it expensive and tedious. But now it looks like the original goal of making digital content more comprehensible to computers might be achievable at far lower cost, thanks to better software.
Diffbot is building that software. This unusual startup—the first ever to emerge from the Stanford-based accelerator StartX, back in 2009—is using computer vision technology similar to that used for robotics applications such as self-driving cars to classify the parts of Web pages so that they can be reassembled in other forms. AOL is one of the startup’s first big customers and its landlord. It’s using Diffbot’s technology to assemble Editions by AOL, the personalized, iPad-based magazine comprised of content culled from AOL properties like the Huffington Post, TechCrunch, and Engadget.
I went down to AOL’s Palo Alto campus last month to meet the company’s founder and CEO Mike Tung and its vice president of products John Davi. They didn’t deliberately set out to solve the Semantic Web problem, any more than the founders of Skype set out to build an affordable Picturephone. But their venture, which has attracted about $2 million in backing from Andy Bechtolsheim and a raft of other angel investing stars, is already on its way to creating one of the world’s largest structured indexes of unstructured Web content.
Without relying on HTML tags (which can actually be used to trick traditional Web crawling software), Diffbot can look at a news page and tell what’s a headline, what’s a byline, where the article text begins and ends, what’s an advertisement, and so forth. What practical use can companies make of that, and where’s the profit in it for Diffbot? Well, aside from AOL, the startup’s software is already being used in some interesting places: reading app maker Pocket (formerly Read It Later) uses it to extract article text from websites, and content discovery service StumbleUpon employs it to screen out spam.
In fact, companies pay Diffbot to analyze more than 100 million unique URLs per month. And that’s just the beginning. Building outward from its early focus on news articles, the startup is creating new algorithms that could make sense of many kinds of sites, such as e-commerce catalogs. The individual elements of those sites could then be served up in almost any context. Imagine a Siri for shopping, to take just one example. “We’re building a series of wedges that will add up to a complete view of the Web,” says Davi. “We are excited about having them all under our belt, so there can be a fully indexed, reverse-engineered Semantic Web.”
What follows is a highly compressed version of my conversation with Tung and Davi.
Xconomy: Where did you guys meet, and how did you end up working on Diffbot?
Mike Tung: I worked at Microsoft on Windows Vista right out of high school, then went to college at Cal and studied electrical engineering for two years, then went to Stanford to start a PhD in computer science, specializing in AI. When I first moved to Silicon Valley, I also worked at a bunch of startups. I was engineer number four at TheFind, which was a product search company that built the world’s largest product index. I worked on search at Yahoo and eBay, and also did a bunch of contract work. I took the patent bar and worked as a patent lawyer for a couple of years, writing 3G and 4G patents for Panasonic and Matsushita. I first met John when we were working at a startup called ClickTV, which was a video-player-search-engine thing. It was pretty advanced for its time.
Diffbot began when I was in grad school at Stanford [in 2005]. There was this one quarter where I was taking a lot of classes, so I made this tool for myself to keep track of all of them. I would put in the URL for the class website, and whenever a professor would upload new slides or content, Diffbot would find that and download it to my phone. I always felt like I knew what was going on in my classes without having to attend every single one.
It was useful, and my friends started asking me whether they could use it. So I turned it into a Web service and … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy
(Page 2 of 4)
started running it out of a dorm at Stanford. And people started adding a bunch of different kinds of URLs to Diffbot outside of classes, like they might add Craigslist if they were searching for a job or a product, or Facebook if they wanted to see if their ex’s profile had changed.
X: So I assume the name “Diffbot” related to comparing the old and new versions of a website and detecting the differences?
MT: Yes, but just doing deltas on Web pages doesn’t work too well. It turns out that on the modern Web, every page refresh changes the ads and the counters. You have to be a little more intelligent.
That’s where understanding the page comes into play. I was studying machine learning at Stanford, and in particular one project I had worked on was the vision system for the self-driving car [Stanford’s entry in the 2007 DARPA Urban Challenge]. This was the stereo camera system that would compute the depth of a scene and say, ‘This is a cactus, this is drivable dirt, this is not drivable dirt, this is a cliff, this is a very narrow passageway.’ I realized that one way of making Diffbot generalizable was to apply computer vision to Web pages. Not to say, ‘This is a cactus and this is a pedestrian,’ but to say, ‘This is an advertisement and this is a footer and this is a product.’
A human being can look at Web page and very easily tell what type of page it is without even looking at the text, and that is what we are teaching Diffbot to do. The goal is to build a machine-readable version of the entire Web.
X: Isn’t that what Tim Berners-Lee has been talking about for years—building a Semantic Web that’s machine-readable?
MT: It seems that every three years or so a new Semantic Web technology gets hyped up again. There was RSS, RDF, OWL, and now it’s Open Graph and the Knowledge Graph. The central problem—why none of these have really gone mainstream—is that you are requiring humans to tag the content twice, once for the machine’s benefit and once for the actual humans. Because you are placing so much onus on the content creators, you are never going to have all of the content in any given system. So it will be fragmented into different Semantic Web file formats, and because of that you will never have an app that allows you to search and evaluate all that information.
But what if you analyze the page itself? That is where we have an opportunity, by applying computer vision to eliminate the problem of manual tagging. And we have reached a certain point in the technology continuum where it is actually possible—where the CPUs are fast enough and the machine learning technology is good enough that we have a good shot of doing it with high accuracy.
X: Why are you so convinced that a human-tagged Semantic Web would never work?
MT: The number one point is that people are lazy. The second is that people lie. Google used to read the meta tags and keywords at the top of a Web page, and so people would start stuffing those areas with everything. It didn’t correspond to what actual humans saw. The same thing holds for Semantic Web formats. Whenever you have things indexed separately, you start to see spam. By using a robot to look at the page, you are keeping it above that.
X: Talk about the computer vision aspect of Diffbot. How literal is the comparison to the cameras and radar on robot cars?
MT: We use the very same techniques used in computer vision, for example object detection and edge detection. If you are a customer, you give us a URL to analyze. We render the page using a virtual Webkit browser in the cloud. It will render the page, run the Javascript, and lay everything out with the CSS rules and everything. Then we have these hooks into Webkit that … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy
(Page 3 of 4)
allow us to get all of the visual and geometric information out of the page. For every rectangle, we pull out things like the x and y coordinates, the heights and widths, the positioning relative to everything else, the font sizes, the colors, and other visual cues. In much the same way, when I was working on the self-driving car, we would look at a patch and do edge detection to determine the shape of a thing or find the horizon.
X: Once you identify those shapes and other elements, how do you say, “This is a headline, this is an article,” et cetera?
MT: We have an ontology. Other people have done good work defining what those ontologies should be—there are many of them at schema.org, which reflects what the search engines have proposed as ontologies. We also have human beings who draw rectangles on the pages and teach Diffbot “this is what an author field looks like, this is what a product looks like, this is what a price looks like,” and from those rectangles we can generalize. It’s a machine learning system, so it lives and breathes on the training data that is fed into it.
X: Do you actually do all the training work yourselves, or do you crowdsource it out somehow?
John Davi: We have done a combination of things. We always have a cold-start problem firing up new type of pages—products versus articles, or a new algorithm for press releases, for example. We leverage both grunt work internally—just grinding out our own examples, which has the side benefit of keeping us informed about the real world—but yeah, also crowdsourcing, which gives us a much broader variety of input and opinion. We have used everything, including off-the-shelf crowdsourcing tools like Mechanical Turk and Crowdflower, and we have build up our own group of quasi-contract crowdsourcers.
Our basic effort is to cold-start it ourselves, then get an alpha-level product into the hands of our customer, which will then drastically increase the amount of training data we have. Sometimes we look at the stream of content and eyeball it and manually tweak and correct. In a lot of cases our customer gets involved. If they have an interest in helping to train the algorithm—it not only makes it better for them, but if they are first out of the gate they can tailor the algorithm to their very particular needs.
X: How much can your algorithms tell about a Web page just from the way it looks? Are you also analyzing the actual text?
MT: First we take a URL and determine what type of page it is. We’ve identified roughly 20 types of pages that all the Web can fall into. Article pages, people pages, product pages, photos, videos, and so on. So one of the fields we return will be what is the type of this thing. Then, depending on the type, there are other fields. For the article API [application programming interface], which is one we have out publicly, we can tell you the title, the author, the images, the videos, and the text that go with that article. And we not only identify where the text is, but we can tell you the topics. We do some natural language processing on the text and we can tell you “This is about Apple,” and we can tell it’s about Apple Computer and not the fruit.
JD: Another opportunity we are excited about his how Diffbot can help augment what is natively on the page. Just by dint of following so many pages through our system, we can augment [the existing formatting] and increase the value for whoever is reading. In the case of an article, the fact that we see so many articles means it’s relatively easy for us to generate tags for any given text.
X: How do you turn this all into a business?
MT: We are actually selling something. We are trying to build the Semantic Web, but in a profitable way. We analyze the pages that people pay us to analyze. That’s currently over 100 million URLs per month, which is a good slice of the Web. Other startups have taken the approach of starting by crawling and indexing the Web, and that is very capital-intensive. By doing it this way, another benefit is that people only send us the best parts of … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy
(Page 4 of 4)
the Web. Most of the stuff a typical Web crawler goes through never appears in any search results. Most of the Web is crap.
X: Are people finding uses for the technology that you may not have thought of?
MT: We had a hackathon last year where a guy came in and built an app for his father, who is blind. It runs Diffbot on a page and makes it into a radio station. For someone who is blind, browsing a news site is usually a really poor experience. The usual screen readers will read the entire page, including the nav bars and the ads and the text. The screen readers have no context about what is important on the page. Using Diffbot to be his father’s eyes, this guy could parse the page and read it in a way that is much more natural.
JD: AOL’s Editions app is one of the more interesting use cases that I’ve seen. It’s an iPad app that features both their own content as well as snippets from across the Web, in a daily issue. I spent five years running engineering for the media solutions group at Cisco, selling a Web platform for media companies, and the biggest problem we faced was dealing with the excess of content management systems that all media companies have. In the case of Editions, AOL has myriad properties that they want to merge into this single app. But rather than consolidate TechCrunch and Engadget and the Huffington Post and a half dozen other sites, they use Diffbot to build a kind of content management system on the fly from the rendered Web pages. They extract the content and deliver it on the fly as if it came from a CMS right to the iPad magazine.
StumbleUpon is another interesting one. They use Diffbot as their moderation queue. Whenever a new website is submitted to their index, they want to make sure it’s legitimate before it’s available for stumbling. They have to rule out people who stumble a page, then swap it out for spam. So they run Diffbot on the source page, pipe that into their moderation queue, and if it looks like a legitimate page they can monitor that and keep checking on a regular basis to see how much it changes. If it has changed much between day 1 and day 10, it might warrant human intervention.
X: Aren’t there are a lot of news reader app these days that are doing the same thing you’re doing when it comes to identifying and isolating the text of a news article? That’s what Instapaper and Pocket and Readability and Zite are all doing.
MT: We power a lot of those apps. Our audience is the developers who work at those companies, who use our API to create their experience.
JD: We make it a lot more affordable to make those kinds of forays. When you look at building your own customized extraction tools, you are talking about multiple developers over weeks or months, to build something that is more brittle than what we offer out of the gate. Our ultimate goal is to be not only better but a lot cheaper than what you could build.
X: It’s not totally clear yet, though, whether publications or apps that aggregate lots of content from elsewhere, like Editions or even Flipboard, are going to be profitable in the long term, and where publishing is going as a business. Don’t you guys feel there’s some risk in tying your fortunes to such a troubled industry?
MT: The more interesting question is how do you monetize the Semantic Web, and where is the money in building the structured information. Articles are only one page type. Another that I mentioned is products. If you could show products on a cell phone, and people could buy the product and we could make that transaction happen, that is one very tangible way of making money. I think there is a lot of value in having structured information, because you can connect people more directly to what they want. Once we have the entire Web in machine-readable format, anybody who wants to use any sort of data can use the Diffbot view of it, and I think a lot of those apps can make money. Look at Siri—it’s great but it only works with the 10 or so sources that it’s hard-coded to work with. If you were able to combine Siri with Diffbot, Siri could operate on the Web and take a query and actually do it for you.
X: What page types will you move on to next? Did you start with articles because those are easiest?
MT: I wouldn’t say they were easiest, but they are pretty prevalent on the Web. A variety of factors help us prioritize what we should do next. One signal is what is the prevalence of that type of page on the Web. If doing one page type lets us knock out 30 percent of the Web, maybe we will go for it.
X: Will there always be a need for Diffbot, or with the transition to HTML 5, will Web pages gradually get more structure on their own?
MT: If you look at the ratio of unstructured pages to structured, it’s actually going in the opposite direction. I think human beings are creative, and they design pages for other humans. No matter what, people will find a way to create documents that lie outside of the well-defined tags, whether it’s HTML 5 or Flash or PDF or Xbox. What they all have in common is that they are just vessels that we can easily train and adapt Diffbot to work with.
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy",,http://www.xconomy.com/ san-francisco/2012/07/25/diffbot-is-using- computer-vision-to-reinvent-the-semantic-web,,
"SD Times news digest: Sprint and Ericsson IoT partnership, Diffbot Knowledge Graph, and Fieldbit and InfinityAR on AR",d2018-09-04T00:00,Jenna Sargent,SD Times,"Sprint Corporation,Ericsson,Knowledge Graph,Diffbot,Augmented reality,Fieldbit,Knowledge,Partnership","Sprint and Ericsson have launched a global partnership in order to create a distributed network for IoT and an IoT operating system.
According to Sprint, the network will provide low latency and high availability. Since it is virtualized, it reduces the distance between the device generating the data and the application processing it.
The operating system will offer connectivity management, device management, data management, and managed services, such as network operations center monitoring, service resource fulfillment, and cloud orchestration management.
“We are combining our IoT strategy with Ericsson’s expertise to build a platform primed for the most demanding applications like artificial intelligence, edge computing, robotics, autonomous vehicles and more with ultra-low-latency, the highest availability and an unmatched level of security at the chip level,” said Ivo Rook, senior vice president of IoT for Sprint. “This is a network built for software and it’s ready for 5G. Our IoT platform is for those companies, large and small, that are creating the immediate economy.”
Diffbot Knowledge Graph released
AI company Diffbot has announced the launch of Diffbot Knowledge Graph, which takes in knowledge from the web to provide a single source of data, answers, insights, and truth. The database is fully autonomous and currently contains more than 1 trillion facts and 10 billion entities.
The database is continually being rebuilt from scratch, ensuring that the data is up-to-date, accurate, and comprehensive, according to the company. It will include people, companies, locations, articles, products, discussions and images.
“A Web-wide, comprehensive, and interconnected Knowledge Graph has the power to transform how enterprises do business. Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers,” said Mike Tung, founder and CEO of Diffbot. “What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
Fieldbit and InfinityAR announce partnership
Augmented reality company Fieldbit has announced a partnership with InfinityAR, another AR company. The two companies will develop an integrated solution for field service organizations, combining InfinityAR’s SLAM and Augmented Reality engine and Fieldbit’s platform for remote assistance, collaboration, and on-job knowledge capture.
This integration will allow Fieldbit to optimize its AR-based field services application with smart glasses.
“Collaborating with both Fieldbit and AR glasses vendors helps us bring an excellent solution to an immediate market need,” said Motti Kushnir, CEO of InfinityAR. “AR glasses offer a clear competitive edge over other remote assistance solutions. The integrated solution, leveraging our advanced SLAM and Augmented Reality engine with Fieldbit’s solution allows field technicians to work hands-free and interact with digital content in the most realistic way through sophisticated scene awareness.”
BrowserStack and PagerDuty add Atlassian integrations
Atlassian has expanded its integrations to BrowserStack and PagerDuty. The integration with BrowserStack includes Jira and Trello, allowing teams to instantly share issues within BrowserStack without needing to switch between tools.
PagerDuty has extended integration with Atlassian with a new integration Jira Ops, which is Atlassian’s latest product. According to PagerDuty, the enhanced integrations will enable organizations to quickly and easily get started and gain value from PagerDuty and Atlassian products.",,https://sdtimes.com/softwaredev/sd-times-news-digest-sprint-and-ericsson-iot-partnership-diffbot-knowledge-graph-and-fieldbit-and-infinityar-on-ar/,,
http://www.outlookindia.com/magazine/story/cross-the-bar/278572?utm_r=false:842 in global code Status: 200 OK X-Diffbot-Render-Hostname: choi5 Content-Type: text/html; charset=utf-8 X-Location: http://www.outlookindia.com/magazine/story/cross-the-bar/278572,,,http://www.outlookindia.com/,,,,http://www.outlookindia.com/magazine/story/cross-the-bar/278572,,
Diffbot tries to reorganise Web data for enterprise use,d2015-06-08T00:00,Techworld Staff,Techworld,"Bloomberg Beta,Andy Bechtolsheim,IBM,Google Search,Sun Microsystems,Palo Alto, California,Organization,Johnny Depp,Semantic Web,Diffbot","Diffbot say it last week,received $500,000 in funding from Bloomberg Beta, the investment arm of the Bloomberg media company to develop yhe service. Andy Bechtolsheim, a founder of Sun MIcrosystems and the first major investor in Google, is also a backer. Diffbot says it already has paying customers for the service, which is being used by Microsoft's Bing, Adobe, Salesforce.com, and eBay.
The service creates an object for each Web page it finds. An object provides structure to a set of related data so that it can be programmatically reused, along with other similar objects, by a query engine or an external application. The software has been copying all the pages it finds on the Web and reorganising them into objects.
Perhaps the most well-known example of this object-based approach is Google's Knowledge Graph, a Semantic Web project. If a search is done on a particular keyword, such as the name ""Johnny Depp,"" Google will return, along with a standard list of Web pages, a box containing basic information on the actor, such as birth date and height. That box of information is a rendering of the ""Johnny Depp"" Knowledge Graph object built by Google.
Diffbot, which is based in Palo Alto, California, and was founded in 2008, claims its own collection of objects is superior to Google's.
The 14-person company says it has created an entirely automated system for accurately creating objects. Google's approach is at least partly manual, requiring individuals to edit objects after they have been created, confirmed a Google spokesman.
Google's Knowledge Graph is larger than Diffbot's, containing roughly a billion objects, while Diffbot's global index of the Web now includes 600 million objects. But Google doesn't yet offer a Knowledge Graph API for third-party commercial use, though it is working on one.
Diffbot is based on the idea that businesses could use such a collection of organised information for their own purposes. Nike, for instance, could deploy the service to build a profile of other shoe companies and their offerings, said Mike Tung, Diffbot CEO. DiffBot offers a set of APIs (application programming interfaces) that third-party applications can use to query the massive object set.
The company has developed a set of AI algorithms that can identify the context and subject of Web pages, some of which the company is in the process of patenting. One novel AI algorithm relies computer vision, which is not a widely used technique for indexing Web pages, Tung acknowledged. The layout and design of Web pages can provide important clues to help better define objects. ""The layout is the signal that helps us determine what kind of page it is,"" Tung said. An e-commerce site has an entirely different structure than a news site, for instance.
Diffbot is one of a number of companies building such ""knowledge graphs,"" through various sets of technologies, said Dave Schubmehl, an IDC research director who covers content analytics, discovery and cognitive systems. Such technology could be of potential value to any business that relies on understanding large amounts of external data, he said via email.
Another company working in this field is IBM, Schubmehl wrote. Last year, IBM purchased two companies to install similar capabilities in its Watson cognitive computing service. One was AlchemyAPI, which builds taxonomies of data assets, and the other is Blekko, which developed software for indexing Web sites.
Some organisations use other technologies to organise and synthesise large sets of otherwise unstructured information, according to Schubmehl. Neo4J and Oracle both offer graph databases, which are well-suited for identifying the connections across large collections of data. Others rely on semantic Web standards, such as the Sesame Java Framework, which is used for converting data into the structured RDF (Rich Description Framework) format.",,https://www.techworld.com/news/apps-wearables/diffbot-tries-reorganise-web-data-for-enterprise-use-3614795/,,
Diffbot Uses Robots To Extract Data From E-commerce Sites,d2013-07-31T00:00,Chris Crum,WebProNews,,"Diffbot announced today that it is relasing a new API that uses robots to understand and extract data from e-commerce sites.
The robotics company, which uses vision, machine learning and artificial intelligence to analyze and extract data from web pages, appeared at the Bing-sponsored LAUNCH event last year, where it laid out is plans to make the entire web machine-readable. More on that here.
The new API uses computer vision to turn any e-commerce site into a product database, the company says in an email.
""Software developers can use the API to extract a variety of data from the page include product image, SKU code, price, shipping cost, discount price, MSRP, etc.,"" a spokesperson for diffbot tells WebProNews. ""The API can identify and structure information regardless of a site’s design, layout, markup or language.""
Additionally, diffbot has developed a spider technology, which can analyze an entire site, skipping non-product pages, and extracting just the data from relevant page types.
""Think about Target.com, or Wal-Mart.com, and being able to extract ALL of the product data from all of the product pages,"" the spokesperson says.
""E-commerce is one of the most popular activities on the web. With 28% of US internet users shopping on a daily basis, we figured we should teach our robot how to understand products,"" said CEO Mike Tung. ""The Product API represents our latest advances in pushing the capabilities of automated page extraction. We are one step closer to the imminent goal of making the entire web machine-readable.""
Diffbot believes the entire web can be broken down into about twenty or so page types, such as home pages, article pages, product pages, location pages, social network pages, etc., and says will continue to roll out APIs for new page types until it has tools to index the entire Internet. It already has APIs for home pages, article pages and image pages.
The company is backed by Earthlink founder Sky Dayton, who is part of the board.",0.0127,http://www.webpronews.com/diffbot-uses-robots-to-extract-data-from-e-commerce-sites-2013-07/,,
http://www.outlookindia.com/magazine/story/ek-tha-tiger/282055?utm_r=false:667 in global code Status: 200 OK X-Diffbot-Render-Hostname: morpheus Content-Type: text/html; charset=utf-8 X-Location: http://www.outlookindia.com/magazine/story/ek-tha-tiger/282055,,,http://www.outlookindia.com/,,,,http://www.outlookindia.com/magazine/story/ek-tha-tiger/282055,,
"Rivaling Google, Web-Mining Startup Diffbot Opens Knowledge Graph to All",d2018-08-30T00:00,Bernadette Tansey,Xconomy,"startup company,ESPN.com,Diffbot,Salesforce.com","(Page 2 of 2)
monthly rates for big enterprises that need specialized integrations and significant amounts of consulting time. The company plans to continue building integrations with commonly used business tools such as Salesforce and Excel, to broaden the range of people able to use the resource.
“It’ll be a little more self-service,” Tung says. He expects the Diffbot Knowledge Graph to level the playing field and cause disruption in some sectors. For example, it could make an insightful team in a garage startup competitive with established data firms. “They could have the same knowledge as the most sophisticated quantitative analysts on Wall Street,” Tung says.
Diffbot won’t track the activity of customers, Tung says. “We believe users should have total control over their own queries,” he says. The company gathers only public information, and doesn’t make private information public, he adds.
The goal Tung has set for Diffbot now is to become “an iconic public company, like an Amazon of data providing this service to everyone.”
Acquisition offers aren’t being considered at this point. “We’re not really open to that right now,” Tung says. “I don’t have confidence that any of the big tech companies could continue our mission capably and ethically.”
Photo of Diffbot team: Courtesy of Diffbot",,https://xconomy.com/san-francisco/2018/08/30/ai-web-mining-startup-diffbot-opens-its-knowledge-graph-to-all-companies/2/,"It’ll be a little more self-service.,They could have the same knowledge as the most sophisticated quantitative analysts on Wall Street.,We’re not really open to that right now.,I don’t have confidence that any of the big tech companies could continue our mission capably and ethically.",
Could a Little Startup Called Diffbot Be the Next Google?,d2014-01-08T00:00,Wade Roush,Xconomy,"startup company,Google,Google Search,Palo Alto,Diffbot,Dan Steinberg,World Wide Web,Emmanuel Charon,Technology Review,web crawler","In tech journalism, it’s inadvisable to call any company “the next Google.” It’s almost always breathless hype or marked naïveté.
After all, people have been predicting the search giant’s demise for nearly as long as the company has existed. I wrote a Technology Review cover story called “Search Beyond Google” nearly 10 years ago. But with unlimited brainpower and money at its disposal, the company has managed to stay at the forefront in search, while also getting very good at other things, like mobile hardware.
So when I tell you that a seven-employee company called Diffbot really could be the next Google, I need to be very specific about what I mean.
I don’t mean that the tiny Palo Alto, CA-based startup is going to put Google out of business. In fact, Diffbot may already be partnering with Google. And there’s a good chance Google will just acqui-hire the startup at some point, thereby preempting the very interesting branch of the timeline where Diffbot gets big on its own.
And I don’t mean that Diffbot is going to redefine the search business. Not the search business as we’ve known it, anyway.
What I do mean is that Diffbot is poised to help the consumer and business worlds make sense of today’s more diverse Internet—one that takes many more forms, and is being put to many more uses, than the Web as it looked back in the 1990s, when Google was born.
Diffbot’s business is to use a combination of crawling software, computer vision, and machine learning to classify documents on the Web and break down each page type into its component parts. (The startup thinks there are about 20 of these types.) This allows people or programs to ask very specific questions about those parts—questions that can’t be answered very well using traditional search technology.
In other words, Diffbot is to today’s Internet as Google was to the Web of 1998. It’s a tool that can impose structure and meaning on resources that are currently disorganized and inaccessible, for a price that many businesses are willing to pay. And so far, that’s a game that Google itself doesn’t seem to want to play.
After writing my first story about Diffbot back in July 2012, I wanted to know about the latest progress at the Stanford-born startup, so I paid a visit to Diffbot’s new headquarters—a quiet backyard bungalow that feels insulated from all the nearby traffic on El Camino and Embarcadero Road. There, the Diffbot crew put aside their laptops for an hour to update me about the company’s ambitious vision. It hasn’t changed much since 2012, but it’s been fleshed out in key respects.
Diffbot founder and CEO Mike Tung started the company in 2009 to fix a problem: there was no easy, automated way for computers to understand the structure of a Web page. A human looking at a product page on an e-commerce site, or at the front page of a newspaper site, knows right away which part is the headline or the product name, which part is the body text, which parts are comments or reviews, and so forth.
But a Web-crawler program looking at the same page doesn’t know any of those things, since these elements aren’t described as such in the actual HTML code. Making human-readable Web pages more accessible to software would require, as a first step, a consistent labeling system. But the only such system to be seriously proposed, Tim Berners-Lee’s Semantic Web, has long floundered for lack of manpower and industry cooperation. It would take a lot of people to do all the needed markup, and developers around the world would have to adhere to the Resource Description Framework prescribed by the World Wide Web Consortium.
Tung’s big conceptual leap was to dispense with all that and attack the labeling problem using computer vision and machine learning algorithms—techniques originally developed to help computers make sense of edges, shapes, colors, and spatial relationships in the real world. Diffbot runs virtual browsers in the cloud that can go to a given URL; suck in the page’s HTML, scripts, and style sheets; and render it just as it would be shown on … Next Page »
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
Trending on Xconomy
(Page 2 of 3)
a desktop monitor or a smartphone screen. Then edge-detection algorithms and computer-vision routines go to work, outlining and measuring each element on the page.
Using machine-learning techniques, this geometric data can then be compared to frameworks or “ontologies”—patterns distilled from training data, usually by humans who have spent time drawing rectangles on Web pages, painstakingly teaching the software what a headline looks like, what an image looks like, what a price looks like, and so on. The end result is a marked-up summary of a page’s important parts, built without recourse to any Semantic Web standards.
The irony here, of course, is that much of the information destined for publication on the Web starts out quite structured. The WordPress content-management system behind Xconomy’s site, for example, is built around a database that knows exactly which parts of this article should be presented as the headline, which parts should look like body text, and (crucially, to me) which part is my byline. But these elements get slotted into a layout designed for human readability—not for parsing by machines. Given that every content management system is different and that every site has its own distinctive tags and styles, it’s hard for software to reconstruct content types consistently based on the HTML alone.
Hence the computer-vision approach. “What we’re trying to do is reverse-engineer the Web presentation and turn it back into structured relations,” Diffbot chief scientist Scott Waterman explains.
The first type of page that Diffbot mastered was the news story. By the time I first met Tung and Diffbot vice president John Davi in 2012, they’d already gotten very good at parsing articles on the Web, and today the Diffbot “Article API,” or application programming interface, is used by hundreds of companies to extract text and reformat it for presentation in Web or mobile news readers. Digg, Instapaper, Onswipe, and Reverb are among Diffbot’s customers. “They’re saying ‘Holy crap, how am I going to get clean text out of this sea of [Web pages], and in that case, we’re a developer’s best friend,” Davi says. “We turn an insurmountable problem into one you can solve with an API integration.”
But articles were just the first page type that Tung’s crew wanted to make Diffbot understand. Today the company offers four APIs—for articles, images, products, and home pages—as well as a classifier that can automatically determine the page type for any URL, and a “Crawlbot” that can comb through entire sites, rather than just specific URLs.
Davi says the startup rushed to finish the images API after it studied a few days’ worth of Twitter posts from mid-2012 and realized that images comprised a whopping 36 percent of the material being shared on the microblogging network. That was a tipoff that understanding image pages would allow the company to parse a huge chunk of the human-readable Web.
But finishing the product API was a much more strategic and potentially lucrative move. The reason is simple: anyone who sells or promotes anything on the Web wants to be able to show the price, and wants to know how competitors are pricing the same wares. “All of the various product-discovery startups—pinning, bookmarking, search, e-commerce companies—want pricing information,” Tung says.
Pinterest is a client, for example. Tung and Davi say Diffbot analyzes the entire “firehose” of data that Pinterest users are putting on their pinboards, including the pages that pins link to, mainly in order to figure out which pins represent products on e-commerce sites.
“The ability to turn on user-facing features based on product data is a potential future revenue stream for these bookmarking sites,” Davi explains. “Say 15 percent of pins are products. They can say, ‘Let’s find out the pricing and availability, then let’s tell the user that this product they just pinned is available at Amazon for $5 less,’ or that it’s just gone on sale somewhere.” If the tip leads to a transaction, the pinning or bookmarking site is then in line for an affiliate commission.
Another user of the product API builds Facebook ads from pages on e-commerce sites. “They end up using our Crawlbot, combined with the product API, to extract data from entire retail sites like Target or QVC, drop the product data into their backend, and generate ads on the fly,” Davi says.
There’s a real business here. Customers who tap the Diffbot APIs up to 250,000 times per month are expected to pay a $300 monthly fee. If your calls are closer to the 5-million-a-month mark, you’ll pay $5,000, and at higher volumes, “custom” pricing goes into effect. One of the major search engines (the startup isn’t allowed to say which one) is paying Diffbot “to improve the richness of their search interface,” Tung says. Almost all of the company’s deals result from inbound inquiries, he says, which means he hasn’t yet needed to hire a sales director.
And there are many page types left to tackle. There will eventually be APIs for things like comment pages, discussion forums, product reviews, social-media status updates, and pages with embedded audio and video (though the startup doesn’t plan to analyze the actual content of media files). Add in the less common kinds of pages such as … Next Page »
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
Trending on Xconomy
(Page 3 of 3)
documents, charts, FAQs, locations, event listings, personal profiles, recipes, games, and error messages, and there are about 20 important page types altogether, Tung says. “Once we have all 20, we will essentially be able to cover the gamut, and convert most of the Web into a database structure,” he says.
Okay—why is that important, and how could it lead to a Google-scale opportunity?
As I’ve been trying to hint, the Web is a far richer place today than it was in 1998, when most pages were limited to text and images. Moreover, Web data is being tapped in new ways—and the majority of the entities using it aren’t even human.
That’s both alarming and intriguing. A study released last month by Silicon Valley Web security firm Incapsula showed that only 38.5 percent of all website traffic comes from real people. Another 29.5 percent comes from malicious bots, including scrapers, spammers, and impersonators—which is, of course, a serious problem. But on the up side, the final 31 percent of traffic comes from search engines and “good bots.”
This includes all of the services, from Instapaper to Flipboard to Pinterest, that extract data for presentation in other forms, leading, ultimately, to more page views for the original publisher. And it includes the growing category of specialized search engines and virtual personal assistants, from Wolfram Alpha to Siri and Google Now, that scour the Web to perform specific tasks for their human masters.
These bots are doing complicated things, which means they thrive on structure. And for them, Diffbot makes the Web a more welcoming place. For one thing, it gives them the ability to launch far more detailed searches against the raw data. “If we have Nike’s entire catalog as a database, you can select queries like ‘Show me x from Nike.com where the price is less than $70,’ and the things you get back aren’t Web pages optimized for viewing on a screen, but the actual record,” Tung says.
For that kind of search—which is more akin to a database query in SQL, the Structured Query Language, than to a keyword-based search—Google just won’t cut it. “Text search gets you only so far,” Waterman says. “When you start to understand the meaning of aspects of the page and glue them together, then you can do all kinds of other things.”
The first company that figures out how to map today’s more complex Web, and open it fully to automated traffic, stands to occupy a central place in tomorrow’s Internet economy. For as soon as data is readable by machines, Tung points out, Tim Berners-Lee’s vision of the Semantic Web will finally begin to take concrete shape. “New knowledge can be created with old knowledge,” he says. “Apps become like mini-AIs that take information, do some value-add with it, and produce other information.”
In September, Diffbot announced that it had brought on Matt Wells, the creator of an open-source search engine called Gigablast. Alongside Google, Bing, and Blekko, Gigablast is one of the only U.S.-based search engines to maintain its own index of the Web; at one time, its index of 12 billion pages was second only to Google’s.
“I believe in Mike’s vision, I see what he’s trying to do, and I thought it would be good to team up with a lot of smart people,” Wells told me. The hire is a sign that Diffbot’s ambitions extend beyond selling access its APIs to something potentially much bigger: constructing a new kind of search engine, built around new types of queries and new ways of formulating intent. And to do that, Diffbot will obviously need its own global index. “We want to convert the entire Web into a structured database,” Tung says. “Matt is one person who has done that Web-scale crawling before. Most of his competitors were teams of thousands of people with millions of dollars.”
So, in the end, Diffbot is a small group of super-talented engineers and machine-learning experts who want to analyze and structure the Web on a huge scale. Yet the Googleplex is just five miles away—and what would be a life-altering amount of money for any of Diffbot’s team members would be pocket change for Google.
So it’s probably silly to imagine a future where Diffbot grows to 10,000 employees and becomes the substrate for a community of AIs, working to make us all happier, more comfortable, and more informed; that is to say, where our online existence isn’t ruled solely by Google and the NSA. But it’s nice to think that it’s possible.
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy",,https://www.xconomy.com/san-francisco/2014/01/08/could-a-little-startup-called-diffbot-be-the-next-google/,,
Could a Little Startup Called Diffbot Be the Next Google?,d2014-01-08T00:00,Wade Roush,Xconomy,"Diffbot,Startup company,Machine learning,World Wide Web,Google,Computer vision","In tech journalism, it’s inadvisable to call any company “the next Google.” It’s almost always breathless hype or marked naïveté.
After all, people have been predicting the search giant’s demise for nearly as long as the company has existed. I wrote a Technology Review cover story called “Search Beyond Google” nearly 10 years ago. But with unlimited brainpower and money at its disposal, the company has managed to stay at the forefront in search, while also getting very good at other things, like mobile hardware.
So when I tell you that a seven-employee company called Diffbot really could be the next Google, I need to be very specific about what I mean.
I don’t mean that the tiny Palo Alto, CA-based startup is going to put Google out of business. In fact, Diffbot may already be partnering with Google. And there’s a good chance Google will just acqui-hire the startup at some point, thereby preempting the very interesting branch of the timeline where Diffbot gets big on its own.
And I don’t mean that Diffbot is going to redefine the search business. Not the search business as we’ve known it, anyway.
What I do mean is that Diffbot is poised to help the consumer and business worlds make sense of today’s more diverse Internet—one that takes many more forms, and is being put to many more uses, than the Web as it looked back in the 1990s, when Google was born.
Diffbot’s business is to use a combination of crawling software, computer vision, and machine learning to classify documents on the Web and break down each page type into its component parts. (The startup thinks there are about 20 of these types.) This allows people or programs to ask very specific questions about those parts—questions that can’t be answered very well using traditional search technology.
In other words, Diffbot is to today’s Internet as Google was to the Web of 1998. It’s a tool that can impose structure and meaning on resources that are currently disorganized and inaccessible, for a price that many businesses are willing to pay. And so far, that’s a game that Google itself doesn’t seem to want to play.
After writing my first story about Diffbot back in July 2012, I wanted to know about the latest progress at the Stanford-born startup, so I paid a visit to Diffbot’s new headquarters—a quiet backyard bungalow that feels insulated from all the nearby traffic on El Camino and Embarcadero Road. There, the Diffbot crew put aside their laptops for an hour to update me about the company’s ambitious vision. It hasn’t changed much since 2012, but it’s been fleshed out in key respects.
Diffbot founder and CEO Mike Tung started the company in 2009 to fix a problem: there was no easy, automated way for computers to understand the structure of a Web page. A human looking at a product page on an e-commerce site, or at the front page of a newspaper site, knows right away which part is the headline or the product name, which part is the body text, which parts are comments or reviews, and so forth.
But a Web-crawler program looking at the same page doesn’t know any of those things, since these elements aren’t described as such in the actual HTML code. Making human-readable Web pages more accessible to software would require, as a first step, a consistent labeling system. But the only such system to be seriously proposed, Tim Berners-Lee’s Semantic Web, has long floundered for lack of manpower and industry cooperation. It would take a lot of people to do all the needed markup, and developers around the world would have to adhere to the Resource Description Framework prescribed by the World Wide Web Consortium.
Tung’s big conceptual leap was to dispense with all that and attack the labeling problem using computer vision and machine learning algorithms—techniques originally developed to help computers make sense of edges, shapes, colors, and spatial relationships in the real world. Diffbot runs virtual browsers in the cloud that can go to a given URL; suck in the page’s HTML, scripts, and style sheets; and render it just as it would be shown on … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy
(Page 2 of 3)
a desktop monitor or a smartphone screen. Then edge-detection algorithms and computer-vision routines go to work, outlining and measuring each element on the page.
Using machine-learning techniques, this geometric data can then be compared to frameworks or “ontologies”—patterns distilled from training data, usually by humans who have spent time drawing rectangles on Web pages, painstakingly teaching the software what a headline looks like, what an image looks like, what a price looks like, and so on. The end result is a marked-up summary of a page’s important parts, built without recourse to any Semantic Web standards.
The irony here, of course, is that much of the information destined for publication on the Web starts out quite structured. The WordPress content-management system behind Xconomy’s site, for example, is built around a database that knows exactly which parts of this article should be presented as the headline, which parts should look like body text, and (crucially, to me) which part is my byline. But these elements get slotted into a layout designed for human readability—not for parsing by machines. Given that every content management system is different and that every site has its own distinctive tags and styles, it’s hard for software to reconstruct content types consistently based on the HTML alone.
Hence the computer-vision approach. “What we’re trying to do is reverse-engineer the Web presentation and turn it back into structured relations,” Diffbot chief scientist Scott Waterman explains.
The first type of page that Diffbot mastered was the news story. By the time I first met Tung and Diffbot vice president John Davi in 2012, they’d already gotten very good at parsing articles on the Web, and today the Diffbot “Article API,” or application programming interface, is used by hundreds of companies to extract text and reformat it for presentation in Web or mobile news readers. Digg, Instapaper, Onswipe, and Reverb are among Diffbot’s customers. “They’re saying ‘Holy crap, how am I going to get clean text out of this sea of [Web pages], and in that case, we’re a developer’s best friend,” Davi says. “We turn an insurmountable problem into one you can solve with an API integration.”
But articles were just the first page type that Tung’s crew wanted to make Diffbot understand. Today the company offers four APIs—for articles, images, products, and home pages—as well as a classifier that can automatically determine the page type for any URL, and a “Crawlbot” that can comb through entire sites, rather than just specific URLs.
Davi says the startup rushed to finish the images API after it studied a few days’ worth of Twitter posts from mid-2012 and realized that images comprised a whopping 36 percent of the material being shared on the microblogging network. That was a tipoff that understanding image pages would allow the company to parse a huge chunk of the human-readable Web.
But finishing the product API was a much more strategic and potentially lucrative move. The reason is simple: anyone who sells or promotes anything on the Web wants to be able to show the price, and wants to know how competitors are pricing the same wares. “All of the various product-discovery startups—pinning, bookmarking, search, e-commerce companies—want pricing information,” Tung says.
Pinterest is a client, for example. Tung and Davi say Diffbot analyzes the entire “firehose” of data that Pinterest users are putting on their pinboards, including the pages that pins link to, mainly in order to figure out which pins represent products on e-commerce sites.
“The ability to turn on user-facing features based on product data is a potential future revenue stream for these bookmarking sites,” Davi explains. “Say 15 percent of pins are products. They can say, ‘Let’s find out the pricing and availability, then let’s tell the user that this product they just pinned is available at Amazon for $5 less,’ or that it’s just gone on sale somewhere.” If the tip leads to a transaction, the pinning or bookmarking site is then in line for an affiliate commission.
Another user of the product API builds Facebook ads from pages on e-commerce sites. “They end up using our Crawlbot, combined with the product API, to extract data from entire retail sites like Target or QVC, drop the product data into their backend, and generate ads on the fly,” Davi says.
There’s a real business here. Customers who tap the Diffbot APIs up to 250,000 times per month are expected to pay a $300 monthly fee. If your calls are closer to the 5-million-a-month mark, you’ll pay $5,000, and at higher volumes, “custom” pricing goes into effect. One of the major search engines (the startup isn’t allowed to say which one) is paying Diffbot “to improve the richness of their search interface,” Tung says. Almost all of the company’s deals result from inbound inquiries, he says, which means he hasn’t yet needed to hire a sales director.
And there are many page types left to tackle. There will eventually be APIs for things like comment pages, discussion forums, product reviews, social-media status updates, and pages with embedded audio and video (though the startup doesn’t plan to analyze the actual content of media files). Add in the less common kinds of pages such as … Next Page »
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy
(Page 3 of 3)
documents, charts, FAQs, locations, event listings, personal profiles, recipes, games, and error messages, and there are about 20 important page types altogether, Tung says. “Once we have all 20, we will essentially be able to cover the gamut, and convert most of the Web into a database structure,” he says.
Okay—why is that important, and how could it lead to a Google-scale opportunity?
As I’ve been trying to hint, the Web is a far richer place today than it was in 1998, when most pages were limited to text and images. Moreover, Web data is being tapped in new ways—and the majority of the entities using it aren’t even human.
That’s both alarming and intriguing. A study released last month by Silicon Valley Web security firm Incapsula showed that only 38.5 percent of all website traffic comes from real people. Another 29.5 percent comes from malicious bots, including scrapers, spammers, and impersonators—which is, of course, a serious problem. But on the up side, the final 31 percent of traffic comes from search engines and “good bots.”
This includes all of the services, from Instapaper to Flipboard to Pinterest, that extract data for presentation in other forms, leading, ultimately, to more page views for the original publisher. And it includes the growing category of specialized search engines and virtual personal assistants, from Wolfram Alpha to Siri and Google Now, that scour the Web to perform specific tasks for their human masters.
These bots are doing complicated things, which means they thrive on structure. And for them, Diffbot makes the Web a more welcoming place. For one thing, it gives them the ability to launch far more detailed searches against the raw data. “If we have Nike’s entire catalog as a database, you can select queries like ‘Show me x from Nike.com where the price is less than $70,’ and the things you get back aren’t Web pages optimized for viewing on a screen, but the actual record,” Tung says.
For that kind of search—which is more akin to a database query in SQL, the Structured Query Language, than to a keyword-based search—Google just won’t cut it. “Text search gets you only so far,” Waterman says. “When you start to understand the meaning of aspects of the page and glue them together, then you can do all kinds of other things.”
The first company that figures out how to map today’s more complex Web, and open it fully to automated traffic, stands to occupy a central place in tomorrow’s Internet economy. For as soon as data is readable by machines, Tung points out, Tim Berners-Lee’s vision of the Semantic Web will finally begin to take concrete shape. “New knowledge can be created with old knowledge,” he says. “Apps become like mini-AIs that take information, do some value-add with it, and produce other information.”
In September, Diffbot announced that it had brought on Matt Wells, the creator of an open-source search engine called Gigablast. Alongside Google, Bing, and Blekko, Gigablast is one of the only U.S.-based search engines to maintain its own index of the Web; at one time, its index of 12 billion pages was second only to Google’s.
“I believe in Mike’s vision, I see what he’s trying to do, and I thought it would be good to team up with a lot of smart people,” Wells told me. The hire is a sign that Diffbot’s ambitions extend beyond selling access its APIs to something potentially much bigger: constructing a new kind of search engine, built around new types of queries and new ways of formulating intent. And to do that, Diffbot will obviously need its own global index. “We want to convert the entire Web into a structured database,” Tung says. “Matt is one person who has done that Web-scale crawling before. Most of his competitors were teams of thousands of people with millions of dollars.”
So, in the end, Diffbot is a small group of super-talented engineers and machine-learning experts who want to analyze and structure the Web on a huge scale. Yet the Googleplex is just five miles away—and what would be a life-altering amount of money for any of Diffbot’s team members would be pocket change for Google.
So it’s probably silly to imagine a future where Diffbot grows to 10,000 employees and becomes the substrate for a community of AIs, working to make us all happier, more comfortable, and more informed; that is to say, where our online existence isn’t ruled solely by Google and the NSA. But it’s nice to think that it’s possible.
Wade Roush is the producer and host of the podcast Soonish and a contributing editor at Xconomy. Follow @soonishpodcast
Trending on Xconomy",,http://www.xconomy.com/san-francisco/2014/01/08/could-a-little-startup-called-diffbot-be-the-next-google/,,
Diffbot Is Using Computer Vision to Reinvent the Semantic Web,d2012-07-25T00:00,Wade Roush,Xconomy,"Google Hangouts,Diffbot,AOL,Semantic Web,Stanford University,computer vision,World Wide Web,history of videotelephony,John Davi,HuffPost","You know how the Picturephone, a half-billion-dollar project at AT&T back in the 1960s and 1970s, turned out to be a huge commercial flop, but two-way video communication eventually came back with a vengeance in the form of Skype and FaceTime and Google Hangouts? Well, something similar is going on with the Semantic Web.
That’s the proposal, dating back almost to the invention of the Web in the 1990s, that the various parts of Web pages should be tagged so that machines, as well as people, can make inferences based on the information they contain. The idea has never gotten very far, mainly because the burden of tagging all that content would fall to humans, which makes it expensive and tedious. But now it looks like the original goal of making digital content more comprehensible to computers might be achievable at far lower cost, thanks to better software.
Diffbot is building that software. This unusual startup—the first ever to emerge from the Stanford-based accelerator StartX, back in 2009—is using computer vision technology similar to that used for robotics applications such as self-driving cars to classify the parts of Web pages so that they can be reassembled in other forms. AOL is one of the startup’s first big customers and its landlord. It’s using Diffbot’s technology to assemble Editions by AOL, the personalized, iPad-based magazine comprised of content culled from AOL properties like the Huffington Post, TechCrunch, and Engadget.
I went down to AOL’s Palo Alto campus last month to meet the company’s founder and CEO Mike Tung and its vice president of products John Davi. They didn’t deliberately set out to solve the Semantic Web problem, any more than the founders of Skype set out to build an affordable Picturephone. But their venture, which has attracted about $2 million in backing from Andy Bechtolsheim and a raft of other angel investing stars, is already on its way to creating one of the world’s largest structured indexes of unstructured Web content.
Without relying on HTML tags (which can actually be used to trick traditional Web crawling software), Diffbot can look at a news page and tell what’s a headline, what’s a byline, where the article text begins and ends, what’s an advertisement, and so forth. What practical use can companies make of that, and where’s the profit in it for Diffbot? Well, aside from AOL, the startup’s software is already being used in some interesting places: reading app maker Pocket (formerly Read It Later) uses it to extract article text from websites, and content discovery service StumbleUpon employs it to screen out spam.
In fact, companies pay Diffbot to analyze more than 100 million unique URLs per month. And that’s just the beginning. Building outward from its early focus on news articles, the startup is creating new algorithms that could make sense of many kinds of sites, such as e-commerce catalogs. The individual elements of those sites could then be served up in almost any context. Imagine a Siri for shopping, to take just one example. “We’re building a series of wedges that will add up to a complete view of the Web,” says Davi. “We are excited about having them all under our belt, so there can be a fully indexed, reverse-engineered Semantic Web.”
What follows is a highly compressed version of my conversation with Tung and Davi.
Xconomy: Where did you guys meet, and how did you end up working on Diffbot?
Mike Tung: I worked at Microsoft on Windows Vista right out of high school, then went to college at Cal and studied electrical engineering for two years, then went to Stanford to start a PhD in computer science, specializing in AI. When I first moved to Silicon Valley, I also worked at a bunch of startups. I was engineer number four at TheFind, which was a product search company that built the world’s largest product index. I worked on search at Yahoo and eBay, and also did a bunch of contract work. I took the patent bar and worked as a patent lawyer for a couple of years, writing 3G and 4G patents for Panasonic and Matsushita. I first met John when we were working at a startup called ClickTV, which was a video-player-search-engine thing. It was pretty advanced for its time.
Diffbot began when I was in grad school at Stanford [in 2005]. There was this one quarter where I was taking a lot of classes, so I made this tool for myself to keep track of all of them. I would put in the URL for the class website, and whenever a professor would upload new slides or content, Diffbot would find that and download it to my phone. I always felt like I knew what was going on in my classes without having to attend every single one.
It was useful, and my friends started asking me whether they could use it. So I turned it into a Web service and … Next Page »
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
Trending on Xconomy
(Page 2 of 4)
started running it out of a dorm at Stanford. And people started adding a bunch of different kinds of URLs to Diffbot outside of classes, like they might add Craigslist if they were searching for a job or a product, or Facebook if they wanted to see if their ex’s profile had changed.
X: So I assume the name “Diffbot” related to comparing the old and new versions of a website and detecting the differences?
MT: Yes, but just doing deltas on Web pages doesn’t work too well. It turns out that on the modern Web, every page refresh changes the ads and the counters. You have to be a little more intelligent.
That’s where understanding the page comes into play. I was studying machine learning at Stanford, and in particular one project I had worked on was the vision system for the self-driving car [Stanford’s entry in the 2007 DARPA Urban Challenge]. This was the stereo camera system that would compute the depth of a scene and say, ‘This is a cactus, this is drivable dirt, this is not drivable dirt, this is a cliff, this is a very narrow passageway.’ I realized that one way of making Diffbot generalizable was to apply computer vision to Web pages. Not to say, ‘This is a cactus and this is a pedestrian,’ but to say, ‘This is an advertisement and this is a footer and this is a product.’
A human being can look at Web page and very easily tell what type of page it is without even looking at the text, and that is what we are teaching Diffbot to do. The goal is to build a machine-readable version of the entire Web.
X: Isn’t that what Tim Berners-Lee has been talking about for years—building a Semantic Web that’s machine-readable?
MT: It seems that every three years or so a new Semantic Web technology gets hyped up again. There was RSS, RDF, OWL, and now it’s Open Graph and the Knowledge Graph. The central problem—why none of these have really gone mainstream—is that you are requiring humans to tag the content twice, once for the machine’s benefit and once for the actual humans. Because you are placing so much onus on the content creators, you are never going to have all of the content in any given system. So it will be fragmented into different Semantic Web file formats, and because of that you will never have an app that allows you to search and evaluate all that information.
But what if you analyze the page itself? That is where we have an opportunity, by applying computer vision to eliminate the problem of manual tagging. And we have reached a certain point in the technology continuum where it is actually possible—where the CPUs are fast enough and the machine learning technology is good enough that we have a good shot of doing it with high accuracy.
X: Why are you so convinced that a human-tagged Semantic Web would never work?
MT: The number one point is that people are lazy. The second is that people lie. Google used to read the meta tags and keywords at the top of a Web page, and so people would start stuffing those areas with everything. It didn’t correspond to what actual humans saw. The same thing holds for Semantic Web formats. Whenever you have things indexed separately, you start to see spam. By using a robot to look at the page, you are keeping it above that.
X: Talk about the computer vision aspect of Diffbot. How literal is the comparison to the cameras and radar on robot cars?
MT: We use the very same techniques used in computer vision, for example object detection and edge detection. If you are a customer, you give us a URL to analyze. We render the page using a virtual Webkit browser in the cloud. It will render the page, run the Javascript, and lay everything out with the CSS rules and everything. Then we have these hooks into Webkit that … Next Page »
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
Trending on Xconomy
(Page 3 of 4)
allow us to get all of the visual and geometric information out of the page. For every rectangle, we pull out things like the x and y coordinates, the heights and widths, the positioning relative to everything else, the font sizes, the colors, and other visual cues. In much the same way, when I was working on the self-driving car, we would look at a patch and do edge detection to determine the shape of a thing or find the horizon.
X: Once you identify those shapes and other elements, how do you say, “This is a headline, this is an article,” et cetera?
MT: We have an ontology. Other people have done good work defining what those ontologies should be—there are many of them at schema.org, which reflects what the search engines have proposed as ontologies. We also have human beings who draw rectangles on the pages and teach Diffbot “this is what an author field looks like, this is what a product looks like, this is what a price looks like,” and from those rectangles we can generalize. It’s a machine learning system, so it lives and breathes on the training data that is fed into it.
X: Do you actually do all the training work yourselves, or do you crowdsource it out somehow?
John Davi: We have done a combination of things. We always have a cold-start problem firing up new type of pages—products versus articles, or a new algorithm for press releases, for example. We leverage both grunt work internally—just grinding out our own examples, which has the side benefit of keeping us informed about the real world—but yeah, also crowdsourcing, which gives us a much broader variety of input and opinion. We have used everything, including off-the-shelf crowdsourcing tools like Mechanical Turk and Crowdflower, and we have build up our own group of quasi-contract crowdsourcers.
Our basic effort is to cold-start it ourselves, then get an alpha-level product into the hands of our customer, which will then drastically increase the amount of training data we have. Sometimes we look at the stream of content and eyeball it and manually tweak and correct. In a lot of cases our customer gets involved. If they have an interest in helping to train the algorithm—it not only makes it better for them, but if they are first out of the gate they can tailor the algorithm to their very particular needs.
X: How much can your algorithms tell about a Web page just from the way it looks? Are you also analyzing the actual text?
MT: First we take a URL and determine what type of page it is. We’ve identified roughly 20 types of pages that all the Web can fall into. Article pages, people pages, product pages, photos, videos, and so on. So one of the fields we return will be what is the type of this thing. Then, depending on the type, there are other fields. For the article API [application programming interface], which is one we have out publicly, we can tell you the title, the author, the images, the videos, and the text that go with that article. And we not only identify where the text is, but we can tell you the topics. We do some natural language processing on the text and we can tell you “This is about Apple,” and we can tell it’s about Apple Computer and not the fruit.
JD: Another opportunity we are excited about his how Diffbot can help augment what is natively on the page. Just by dint of following so many pages through our system, we can augment [the existing formatting] and increase the value for whoever is reading. In the case of an article, the fact that we see so many articles means it’s relatively easy for us to generate tags for any given text.
X: How do you turn this all into a business?
MT: We are actually selling something. We are trying to build the Semantic Web, but in a profitable way. We analyze the pages that people pay us to analyze. That’s currently over 100 million URLs per month, which is a good slice of the Web. Other startups have taken the approach of starting by crawling and indexing the Web, and that is very capital-intensive. By doing it this way, another benefit is that people only send us the best parts of … Next Page »
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy
(Page 4 of 4)
the Web. Most of the stuff a typical Web crawler goes through never appears in any search results. Most of the Web is crap.
X: Are people finding uses for the technology that you may not have thought of?
MT: We had a hackathon last year where a guy came in and built an app for his father, who is blind. It runs Diffbot on a page and makes it into a radio station. For someone who is blind, browsing a news site is usually a really poor experience. The usual screen readers will read the entire page, including the nav bars and the ads and the text. The screen readers have no context about what is important on the page. Using Diffbot to be his father’s eyes, this guy could parse the page and read it in a way that is much more natural.
JD: AOL’s Editions app is one of the more interesting use cases that I’ve seen. It’s an iPad app that features both their own content as well as snippets from across the Web, in a daily issue. I spent five years running engineering for the media solutions group at Cisco, selling a Web platform for media companies, and the biggest problem we faced was dealing with the excess of content management systems that all media companies have. In the case of Editions, AOL has myriad properties that they want to merge into this single app. But rather than consolidate TechCrunch and Engadget and the Huffington Post and a half dozen other sites, they use Diffbot to build a kind of content management system on the fly from the rendered Web pages. They extract the content and deliver it on the fly as if it came from a CMS right to the iPad magazine.
StumbleUpon is another interesting one. They use Diffbot as their moderation queue. Whenever a new website is submitted to their index, they want to make sure it’s legitimate before it’s available for stumbling. They have to rule out people who stumble a page, then swap it out for spam. So they run Diffbot on the source page, pipe that into their moderation queue, and if it looks like a legitimate page they can monitor that and keep checking on a regular basis to see how much it changes. If it has changed much between day 1 and day 10, it might warrant human intervention.
X: Aren’t there are a lot of news reader app these days that are doing the same thing you’re doing when it comes to identifying and isolating the text of a news article? That’s what Instapaper and Pocket and Readability and Zite are all doing.
MT: We power a lot of those apps. Our audience is the developers who work at those companies, who use our API to create their experience.
JD: We make it a lot more affordable to make those kinds of forays. When you look at building your own customized extraction tools, you are talking about multiple developers over weeks or months, to build something that is more brittle than what we offer out of the gate. Our ultimate goal is to be not only better but a lot cheaper than what you could build.
X: It’s not totally clear yet, though, whether publications or apps that aggregate lots of content from elsewhere, like Editions or even Flipboard, are going to be profitable in the long term, and where publishing is going as a business. Don’t you guys feel there’s some risk in tying your fortunes to such a troubled industry?
MT: The more interesting question is how do you monetize the Semantic Web, and where is the money in building the structured information. Articles are only one page type. Another that I mentioned is products. If you could show products on a cell phone, and people could buy the product and we could make that transaction happen, that is one very tangible way of making money. I think there is a lot of value in having structured information, because you can connect people more directly to what they want. Once we have the entire Web in machine-readable format, anybody who wants to use any sort of data can use the Diffbot view of it, and I think a lot of those apps can make money. Look at Siri—it’s great but it only works with the 10 or so sources that it’s hard-coded to work with. If you were able to combine Siri with Diffbot, Siri could operate on the Web and take a query and actually do it for you.
X: What page types will you move on to next? Did you start with articles because those are easiest?
MT: I wouldn’t say they were easiest, but they are pretty prevalent on the Web. A variety of factors help us prioritize what we should do next. One signal is what is the prevalence of that type of page on the Web. If doing one page type lets us knock out 30 percent of the Web, maybe we will go for it.
X: Will there always be a need for Diffbot, or with the transition to HTML 5, will Web pages gradually get more structure on their own?
MT: If you look at the ratio of unstructured pages to structured, it’s actually going in the opposite direction. I think human beings are creative, and they design pages for other humans. No matter what, people will find a way to create documents that lie outside of the well-defined tags, whether it’s HTML 5 or Flash or PDF or Xbox. What they all have in common is that they are just vessels that we can easily train and adapt Diffbot to work with.
Wade Roush is a freelance science and technology journalist and the producer and host of the podcast Soonish. Follow @soonishpodcast
More from Xconomy
Trending on Xconomy",,http://www.xconomy.com/san-francisco/2012/07/25/diffbot-is-using-computer-vision-to-reinvent-the-semantic-web/,"We’re building a series of wedges that will add up to a complete view of the Web.,We are excited about having them all under our belt, so there can be a fully indexed, reverse-engineered Semantic Web.",
http://www.firstpost.com/shareworthy-news/10/03/17/05.html:52 in global code Status: 200 OK X-Diffbot-Render-Hostname: smith01 Content-Type: text/html; charset=utf-8 X-Location: http://www.moneycontrol.com/news/business/arms-merger-npa-resolution-key-challenges-sbi's-bhattacharya_8627581.html,d1970-01-01T05:30,,Moneycontrol.com,"Diffbot,Hostname,Character encoding,UTF-8","Budget: 2011, 2012, 2013, 2014, 2015, 2016 | Budget 2017",0.05879,http://www.moneycontrol.com/news/business/arms-merger-npa-resolution-key-challenges-sbi's-bhattacharya_8627581.html,,
"Rivaling Google, Web-Mining Diffbot Opens Its Knowledge Graph to All",,,Xconomy,XiltriX,"From :
XiltriX
XiltriX
Xconomy offers a number of ways to subscribe for free!",,https://xconomy.com/san-francisco/2018/08/30/ai-web-mining-startup-diffbot-opens-its-knowledge-graph-to-all-companies/attachment/diffbotteamaug18/,,
Diffbot organizing Web data for enterprise use,d2015-06-05T00:00,,cio.de,"Diffbot,Knowledge Graph,Data model,Application software,Database,World Wide Web,Bloomberg Beta,Semantic Web,Andy Bechtolsheim,Software","The service ""converts the existing Web into a structured database-like representation that can essentially be used for all sorts of intelligent applications,"" said Mike Tung, Diffbot CEO.
On Thursday, Diffbot said it had received $500,000 in funding from Bloomberg Beta, the investment arm of the Bloomberg media company. Andy Bechtolsheim, a founder of Sun MIcrosystems and the first major investor in Google, is also a backer. Diffbot says it already has paying customers for the service, which is being used by Microsoft's Bing, Adobe, Salesforce.com, and eBay.
The service creates an object for each Web page it finds. An object provides structure to a set of related data so that it can be programmatically reused, along with other similar objects, by a query engine or an external application. The software has been copying all the pages it finds on the Web and reorganizing them into objects.
Perhaps the most well-known example of this object-based approach is Google's Knowledge Graph, a Semantic Web project. If a search is done on a particular keyword, such as the name ""Johnny Depp,"" Google will return, along with a standard list of Web pages, a box containing basic information on the actor, such as birth date and height. That box of information is a rendering of the ""Johnny Depp"" Knowledge Graph object built by Google.
Diffbot, which is based in Palo Alto, California, and was founded in 2008, claims its own collection of objects is superior to Google's.
The 14-person company says it has created an entirely automated system for accurately creating objects. Google's approach is at least partly manual, requiring individuals to edit objects after they have been created, confirmed a Google spokesman.
Google's Knowledge Graph is larger than Diffbot's, containing roughly a billion objects, while Diffbot's global index of the Web now includes 600 million objects. But Google doesn't yet offer a Knowledge Graph API for third-party commercial use, though it is working on one.
Diffbot is based on the idea that businesses could use such a collection of organized information for their own purposes. Nike, for instance, could deploy the service to build a profile of other shoe companies and their offerings, Tung suggested. DiffBot offers a set of APIs (application programming interfaces) that third-party applications can use to query the massive object set.
The company has developed a set of AI algorithms that can identify the context and subject of Web pages, some of which the company is in the process of patenting. One novel AI algorithm relies computer vision, which is not a widely used technique for indexing Web pages, Tung acknowledged. The layout and design of Web pages can provide important clues to help better define objects. ""The layout is the signal that helps us determine what kind of page it is,"" Tung said. An e-commerce site has an entirely different structure than a news site, for instance.
Diffbot is one of a number of companies building such ""knowledge graphs,"" through various sets of technologies, said Dave Schubmehl, an IDC research director who covers content analytics, discovery and cognitive systems. Such technology could be of potential value to any business that relies on understanding large amounts of external data, he said via email.
Another company working in this field is IBM, Schubmehl wrote. Last year, IBM purchased two companies to install similar capabilities in its Watson cognitive computing service. One was AlchemyAPI, which builds taxonomies of data assets, and the other is Blekko, which developed software for indexing Web sites.
Some organizations use other technologies to organize and synthesize large sets of otherwise unstructured information, according to Schubmehl. Neo4J and Oracle both offer graph databases, which are well-suited for identifying the connections across large collections of data. Others rely on semantic Web standards, such as the Sesame Java Framework, which is used for converting data into the structured RDF (Rich Description Framework) format.
Joab Jackson covers enterprise software and general technology breaking news for The IDG News Service. Follow Joab on Twitter at @Joab_Jackson. Joab's e-mail address is Joab_Jackson@idg.com
Joab Jackson",-0.1018,"http://www.cio.de/a/diffbot-organizing-web-data-for-enterprise-use,3230286",,
New Diffbot API Client Libraries Released,d2014-02-07T00:00,Janet Wagner,ProgrammableWeb,"natural language processing,ProgrammableWeb,Diffbot,Web API,computer vision,machine learning,E-commerce","Diffbot has just announced the release of brand new client libraries for 35+ different programming languages. The company now provides developers client libraries for the Diffbot API in the most-used programming languages as well as languages that are not as common.
Diffbot uses computer vision, machine learning, natural language processing (NLP) and other technologies to create APIs that are capable of understanding and extracting data from web pages such as text, images, links, HTML attributes, e-commerce product page information and other web page elements. Diffbot currently has three core products: Automatic APIs, Custom API Toolkit and Crawlbot.
Last August, ProgrammableWeb reported that Diffbot had launched the Product API, which can be used by developers to extract product information (product title, description, sale price, regular price, UPC, etc.) from e-commerce web pages. The extracted product information can then be integrated into third-party applications. The Product API is included in the suite of Diffbot Automatic APIs.
Releasing officially supported and maintained client libraries solved several problems and provided key benefits:
It prevents users of third-party contributed libraries from running into buggy, non-maintained code.
It allows Diffbot to control the release of new updates and bug fixes.
It allows Diffbot to provide client libraries with clean code and adequate documentation.
Diffbot API client libraries are now available in nearly every programming language.
The API client libraries work with the programmatic Crawlbot and Bulk-submission interfaces (for premium users).
There is an additional benefit, says Diffbot CEO Mike Tung: ""Diffbot is a REST API and that means it can be called from nearly any kind of software environment or programming language. This makes handling all the potential support questions tricky to say the least,"" he says. ""Now with official supported clients in every programming language, we can point integration questions to example working code.""
Although developers are not required to use a Diffbot API client library, using a library can save developers some time when it comes to writing code. Each library contains an already-written Diffbot API call, and there are now client libraries for 35+ different programming languages. Available programming languages include C#, CoffeeScript, Java, JavaScript, Objective-C, PHP, Python and Ruby.
""A developer generally writes an application in a required programming language / environment; if he is writing an Android app, then he's using Java. If he's writing an iOS app, he's using Objective-C, if he's writing a plugin for Excel, he's probably using VBA,"" Tung explains. ""They don't have to use a library, but it makes it easier because the call is already done in their language. An analogy might be to think of these libraries like templates for Word.""
Diffbot was able to complete this project in a time-saving and cost-effective way by using the oDesk API to post jobs for each individual programming language. The company posted the jobs on oDesk which as a group received thousands of responses from developers worldwide. Using developers found on oDesk resulted in the completion of 35+ new client libraries for the Diffbot API, only 18 hours of Diffbot's own time spent on the project, 56,042 new lines of code written, and an increase in the Diffbot developer network to approximately 10,000 developers.
Diffbot has come a long way since the company was founded in 2009. The news story was the first type of web page that Diffbot was able to parse. Today in addition to the News Article API, there is the Frontpage API, Product API, Image API, Page Classifier API and other web page data extraction APIs and tools.
In a recent article published on Xconomy, writer Wade Roush asks ""Could a Little Startup Called Diffbot Be the Next Google?""
It may not be long before that question is definitively answered.
By Janet Wagner. Janet is a data journalist and full stack developer based in Toledo, Ohio. Her focus revolves around APIs, open data, data visualization and data-driven journalism. Follow her on Twitter, Google+ and LinkedIn.",0.218,https://www.programmableweb.com/news/new-diffbot-api-client-libraries-released/brief/2014/02/07,Diffbot is a REST API and that means it can be called from nearly any kind of software environment or programming language. This makes handling all the potential support questions tricky to say the least.,
Sky Dayton Backed Diffbot Gets More Funding,d2015-06-05T00:00,,SoCalTech,"Bloomberg Beta,Diffbot,Sky Dayton,Funding","Diffbot, the developer of artificial intelligence, visual recognition, and natural language processing software for interpreting web pages which is backed by Sky Dayton, has scored $500,000 more in funding, the company said this week. The funding--a continuation of the firm's seed funding--came from Bloomberg Beta. Diffbot's AI software allows developers to tap into an application programming interface (API), to better parse and understand the content on a web page as structured objects, rather than as just a collection of text. Dayton and other angels, such asa Any Bechtolsheim, Joi Ito, Brad Garlinghouse, Elad Gil, Maynard Webb, and others made the first investment in Diffbot back in 2012; the company says it has now raised a total of $3M. Diffbot is based in Palo Alto. More information »",,https://www.socaltech.com/sky_dayton_backed_diffbot_gets_more_funding/s-0060516.html,,
"Diffbot Raises $2M to Power Its Learning, Web Browsing Robot",d2012-05-31T00:00,Darrell Etherington,BetaKit,"Diffbot,Joi Ito,Startup company,Sky Dayton,Investor,Natural language processing,World Wide Web,Brad Garlinghouse,EarthLink,Web page","The first startup out of Stanford’s StartX student accelerator program, website form and content recognition engine Diffbot, is today announcing a $2 million round of funding from a select group of investors including Sky Dayton, founder of Earthlink and Boingo, Brad Garlinghouse, Any Bechtolsheim, Joi Ito and other web-tech all-stars. Diffbot founder and CEO Michael Tung told BetaKit that the round was actually oversubscribed, and the startup was careful to pick investors who were good at one thing in particular: scaling web-based companies in response to demand.
That’s because the Diffbot API is seeing over 100 million calls per month already, and that’s with only a fraction of Diffbot’s planned functionality out in the wild available for public use. What Diffbot offers is a combination of visual robotics and natural language processing (NLP) to help apps quickly identify on-demand what kind of content is represented by any given web page, and where the most relevant parts of that content are stored for any given application.
“Essentially we have this software that can analyze a web page and determine what are the objects on the page, like ‘this is an article, this is a product, this is a review, this an event, this is a location, etc.’,” Tung explained. “The idea is that by making this web page that was designed for humans machine-readable, we can then treat the web just like a big database of information.”
So far, Diffbot is able to identify two different content types, including front pages and article pages. Tung said that rolling out functionality slowly is part of the company’s plan to help it scale effectively, and also to help it make sure the tech is really ready for each intended purpose before it goes out. Overall, the Diffbot team has identified around 20 different distinct content types that basically cover the range of content available on the web, according to Tung.
“If you were to pull up an Israeli magazine and you don’t know Hebrew, or if you were to pull up a Japanese blog and you don’t know Japanese, you can still kind of tell how that page works, both editorially and in terms of navigation,” he said, describing how Diffbot can apply basically universally in terms of its optical recognition engine. “You can still tell ‘this is the headline, these are the comments and this is the picture that goes with it.’ That’s the level we’ve taught a computer how to do automatically.” Its content interpretation applies to different languages, too; Diffbot currently can handle about 250 different languages with the NLP side of its equation.
Because of its current focus on article content, Diffbot’s clients tend to be media companies. Some noteworthy examples include AOL Editions, which uses the tech to scan and prep the content for formatting in its Flipboard-style iPad reading application. Tung said that much of what it provides for AOL is a way to get all of its own content from various acquired properties, including TechCrunch, Engadget and the Huffington Post, into a single format that’s easy to work with, despite the fact that they all may come from different backend content management systems.
Diffbot represents a new way of approaching content, one that can be taught to handle variation and is equipped to evolve with the web. Tung and his team are already seeing strong uptake, and they’ve only just begun; once they expand to other types of content, Diffbot is a product that should appeal to almost anyone working on the web, from student researchers to multinational corporations. This new round of funding, and the advisory talent in brings with it, should definitely help the startup begin to better realize that potential.",,http://betakit.com/diffbot-raises-2m-to-power-its-learning-web-browsing-robot/,,
Web-reading robot Diffbot raises $2M from tech titans,d2012-05-30T12:00,,VatorNews,"Sky Dayton,Matrix Partners,Andy Bechtolsheim,Diffbot,web page,Teen Titans,Brad Garlinghouse,Sun Microsystems,United States dollar,robot","The visual intelligence technology reads Web pages the same way humans do
All I want is a robot that can read my emails to me while I drive. Is that so much to ask? Actually, I’m sure there’s already an app for that and I’m exposing myself for the lazy technology journalist I really am.
Visual artificial intelligence startup Diffbot announced Thursday that it has raised $2 million in a round of funding led by Earthlink founder Sky Dayton, Sun Microsystems co-founder Andy Bechtolsheim, MIT Media Lab director Joi ito, YouSendIt CEO Brad Garlinghouse, a number of executives from Facebook, Twitter, and Yahoo, as well as Matrix Partners.
The company, which launched out of beta last August, has developed a visual intelligence robot that can essentially read Web pages the way humans do. The technology can look at Web pages written in any language and identify whatever an app developer wants it to.
For example, one app developer at Hackathon used the technology to create an app that could read online content aloud for blind users. Existing apps for the blind simply read from top to bottom and don’t differentiate between the content and ads, copyright credits, and so on. But Diffbot’s technology was used to create a content reading app that can identify the relevant content (i.e. headline, author, body of the text, etc.).
And all this time I’ve been reading my own online content like a chump.
That’s just one of the many uses to which Diffbot can be applied. The technology is the brainchild of Stanford Ph.D. student Mike Tung, who developed the robot to monitor his classes’ Web pages. When a Web page would be updated with a new assignment, for example, Mike’s cell phone would buzz to alert him. When he opened up the technology to his friends, they used it to monitor other websites, like job sites.
Explaining how the robot differentiates between relevant content and the junk content, Mike Tung told me that most Web pages follow similar styles in terms of layout. “Diffbot looks at the height and width of the page. Ads usually have a common format and are usually positioned in certain ways across the page,” he said. Diffbot is designed to identify these visual features and discriminate between relevant content and all else.
When Diffbot launched in August, it could identify two types of Web pages: article pages and front pages. But Tung says that the entire Web can be effectively broken down into 18 types of Web pages, and the new funding from this round will be used to scale the technology out to identify those pages—such as video pages, people pages, reviews, products, photo galleries, and more.
The company also announced that it's now processing 100 million API calls per month.
The San Francisco-based company currently has five full-time employees.
“We chose investors who were experienced with scaling out big Internet-sized companies,” said Tung.",,http://vator.tv/news/2012-05-31-web-reading-robot-diffbot-raises-2m-from-tech-titans,Diffbot looks at the height and width of the page. Ads usually have a common format and are usually positioned in certain ways across the page.,
Diffbot Frontpage API,,,ProgrammableWeb,"Microsoft FrontPage,Diffbot,American Petroleum Institute",Diffbot is a tool that identifies and extracts the main content and sections from any web page. Diffbot Frontpage allows users to enter in the main URL for a web page that is multi-faceted and Diffbot Frontpage returns the main elements of the page. The Diffbot Frontpage API allows developers to access and integrate the Frontpage functionality with other applications and to create new applications. The main API method is retrieving web page contents.,0.402,https://www.programmableweb.com/api/diffbot-frontpage,,
Diffbot Python SDK by Andrey Popp,d2017-05-31T00:00,,ProgrammableWeb,,,,https://www.programmableweb.com/sdk/diffbot-python-sdk-andrey-popp,,
Diffbot Global Index API,,,ProgrammableWeb,,"Search the web like a database in realtime with the Diffbot Global Index API. The Automatic APIs scour the web regularly, indexing articles, comments, images, products and more—available across any structured field.",0.626,https://www.programmableweb.com/api/diffbot-global-index,,
Diffbot Ruby SDK by Nicolas Sanguinetti,d2017-05-31T00:00,,ProgrammableWeb,,,,https://www.programmableweb.com/sdk/diffbot-ruby-sdk-nicolas-sanguinetti,,
Diffbot Adds Page Classifier API to Help Developers Categorize the Web,d2012-08-16T00:00,Darrell Etherington,BetaKit,"machine learning,Google Chrome extension,classifier,Twitter,Uniform Resource Locator,World Wide Web,categorization,Diffbot,media type,application programming interface","Diffbot, the visual robotic software tool that uses machine learning to crawl the web and identify types of content, today released a new beta API for developers. Dubbed the “Page Classifier API,” this new hook for developers to plug into providers an analysis engine that’s designed to recognize and categorize the entire web into just 20 different basic page types. Back when BetaKit covered Diffbot’s $2 million seed round in May, its engine could only spot two different types of content: front pages and articles. Now, the startup is betting it has got the bulk of the web covered, though it’s also open to suggestions.
“We’d released commercial APIs for only articles and front pages, so we had a lot of existing customers pass stuff into our article API that weren’t articles, because they were just piping things in from their users,” Diffbot founder and CEO Michael Tung said in an interview. “If you pass an image into the article API, you’re not going to get a good experience. So one immediate use is for existing customers to redirect the flow of URLs, to only send the article URLs to the article API, and to send the other ones elsewhere.”
That’s how the new API will benefit existing customers, but in general, the product is aimed at making sense of a massive amount of incoming URL information that’s not organized by content type or category to begin with. Diffbot is providing a great example of how that might work in practice with its own Chrome extension, which is also being released today. That Chrome extension allows users to see exactly what type of links are being shared on Twitter.com without having to click through to a page. They can just click the link tag, and tweets will expand to show detailed information from the source, including article text if it’s an article, or a photo or video if it’s classified as either of those types of pages (Diffbot has an infographic of information they’ve found using the extension about what we share on Twitter available here).
Unlike some other solutions, the Diffbot engine can parse a page with multiple elements and decide what the primary focus of that is. So if you’re looking at a video with an accompanying short article, it’ll recognize the video is the intended showpiece of the page and serve that up via the API. The Twitter use case is a particularly strong one, since it shows what Twitter client apps could also accomplish using the Diffbot API. Diffbot VP of Product John Davi explained that aside from being able to provide an instant reading view for articles, and viewing windows for image and video pages, another possible application is creating ecommerce windows that recognize product pages at online stores and include a ‘buy-it-now’ link, which could lead to additional monetization opportunities for people aggregating and serving a lot of links from users and other sources.
While this is a big step in Diffbot’s goal of categorizing the entire web, there’s still plenty left to map. The startup says that the next step is drilling down to additional individual content types it’s identified, and then creating more sophisticated tools for gathering even more specific information. The next page type Diffbot will tackle with an individual API will be photos, Tung said, since it represents a massive percentage of what’s being shared on the web. It’s easy to imagine the applications of being able to identify and relate information about what type of picture is on a given page, especially for tools that use family filters.
Wednesday, Hopper announced a significant round to help it tackle the task of categorizing just one subset of the web’s pages, specifically in the travel vertical. Diffbot is after a much bigger fish, so it’ll be interesting to see how it fares with a broadly focused approach compared to those taking on much smaller chunks of the mass of information that makes up the web.",,https://betakit.com/diffbot-adds-page-classifier-api-to-help-developers-categorize-the-web/,,
"Daily API RoundUp: Gracenote, Diffbot Video Extraction, ADP, Fotolia",d2015-10-30T00:00,Joy Culbertson,ProgrammableWeb,"ProgrammableWeb,Panopto,Diffbot,Web API,Apis","Every day, the ProgrammableWeb team is busy, updating its three primary directories for APIs, clients (language-specific libraries or SDKs for consuming or providing APIs), and source code samples. If you have new APIs, clients, or source code examples to add to ProgrammableWeb’s directories, we offer forms (APIs, Clients, Source Code) for submitting them to our API research team. If there’s a listing in one of our directories that you’d like to claim as the owner, please contact us at editor@programmableweb.com.
We've added 15 APIs to the ProgrammableWeb directory under video, OCR, Human Resources, and other categories. Take a look at what's new in this summary.
APIs
SemaMedia Data provides video analysis services. Two APIs for integration with the service are available. The SemaMediaData Lecture Video Analysis API allows users to detect lecture slide transition in video presentations, recognize text (OCR), extract keywords, and analyze video browsing. And the SemaMediaData Video Segmentation API allows users to implement visual detection features into applications. It works by separating a video stream into scenes, recognizing camera transitions. Responses are available in JSON format.
We've categorized the SemaMediaData APIs under the OCR category. See ProgrammableWeb's complete list of OCR APIs.
Diffbot provides APIs and algorithms for extracting data from websites. With the Diffbot Video Extraction API, developers can extract information from a video, including images, URL, and embed code, author, date, duration, viewCount, and humanLanguage information.
Panopto is a video platform for education and businesses to share knowledge. The Panopto Video Platform API allows integration of video features including capturing and streaming lectures. Documentation includes server integration, web services, security, and more.
We've categorized the Diffbot Video Extraction and well as the Panopto APIs under the Video category. See ProgrammableWeb's complete list of Video APIs.
Gracenote (formerly CDDB) maintains and licenses data about audio CDs, digital file identifications, and song lyrics. Three APIs have been added to the directory for use with Gracenote's database.
The Gracenote Data Delivery API is a read only application to access TV and movie schedules and episode information.
Gracenote Rhythm API lets developers integrate radio and music recommendation features into web services. It supports regulation provided by the Digital Millenium Copyright Act.
The Gracenote Online Video API allows integration of video content and a social media API component, which aims to engage users through their preference for movies and TV shows. The Gracenote APIs are listed under the Entertainment category. See ProgrammableWeb's complete list of Entertainment APIs.
Fotolia is an image repository from Adobe, which offers over 46 million royalty-free professional and crowdsourced images for subscription or pay-as-you-go rates. The Fotolia Print on Demand API allows users to integrate image library features into ready to print applications. Developers of printing applications, web design solutions, and print on-demand services can find the API useful. The Fotolia Intranet API is also available to access data for searching, filtering, organizing, tracking, and downloading Fotolia images.
Fotolia APIs can help users query, track, and download images from the Fotolia service / image: Fotolia
The Fotolia APIs are listed under the Photos category. See ProgrammableWeb's complete list of Photos APIs.
LIKE WHAT YOU SEE? GET THESE UPDATES AND OTHER API NEWS DELIVERED DAILY TO YOUR INBOX. REGISTER FOR THE PROGRAMMABLEWEB TODAY NEWSLETTER.
The PKPass API from Open Merchant Account offers an Apple Wallet coupon generation system for integration. The coupons are ready for use, and users can avoid navigating through Apple's security certificates. This API is categorized under Coupons. See ProgrammableWeb's complete list of Coupons APIs.
WorldWide Parcel Services is a global tracking and delivery company and is based in the U.K. The WorldWide Parcel Services API offers a shipping system to integrate with existing applications. The API allows access to shipment booking and tracking data. We've listed this API under the Shipping category. See ProgrammableWeb's complete list of Shipping APIs.
ADP provides payroll services to businesses in the U.S. A number of new APIs from the company have been added to the directory and are listed below.
ADP Payroll API provides developer tools for payroll management including distribution, tax statements, ATM locators, Paycard Funding Initiation, and more.
ADP Benefits API brings developers tools for Health and Welfare Administration Services. This includes developer tools for benefit elections and enrollment services, flexible spending account management, and retirement plans.
ADP Core API brings developers tools for core services which cross domains. This includes Calendars, Notifications, Data Management, News, and more.
ADP Marketplace API provides a structure to interact with the other ADP APIs.
All the ADP APIs are available only in the US, but will be opened up globally soon. They are listed under the Human Resources category. See ProgrammableWeb's complete list of Human Resources APIs.",0.596,https://www.programmableweb.com/news/daily-api-roundup-gracenote-diffbot-video-extraction-adp-fotolia/brief/2015/10/30,,
Today in APIs: Diffbot Product Pages API and 4 New APIs,d2013-07-31T00:00,Kevin Sundstrom,ProgrammableWeb,"Diffbot,Apigee,E-commerce","Diffbot announced the release of a Product Pages API that hunts down product pages and curates useful information. Plus: Apigee Raises $35M, Chirpify raises $6M, and 4 new APIs.
Diffbot Releases Product Pages API
Diffbot, a web content analysis company, recently announced the release of a comprehensive API that helps developers identify information from article pages on the web. Today the company has announced the addition of a Product Pages API that will help with the curation of e-commerce information in the same fashion.
API News You Shouldn't Miss
4 New APIs
Today we had 4 new APIs added to our API directory including a bulk sms service provider, an sms messaging and voice broadcasting service and a bitcoin wallet service. Below are more details on each of these new APIs.
5star SMS API: 5star SMS is a Nigeria-based bulk SMS service provider delivering single or bulk SMS to over 200 countries. The 5star SMS API provides a simple developer interface for sending bulk SMS and checking account balances.
Call Loop API: Call Loop provides web-based voice broadcasting, SMS messaging, and appointment reminder software. Call Loop enables users to send voice and text messages to large audiences, and offers an API allowing developers to integrate Call Loop’s SMS and voice capabilities into their applications.
The API allows applications to add or remove contacts from groups and to send SMS texts or voice broadcasts. Potential uses include automated notifications, customer service, order status updates, and more.
Inputs.io API: Inputs.io is a secure payment service that allows users to store use Bitcoins for payment. The service uses two factor authentication for security and is designed to enable users to get a wallet and start utilizing it with speed and ease. The Inputs.io API uses REST calls, returns JSON, and allows users to get balances, get last transactions, send transactions, get transaction details, sign and verify messages, and generate/redeem vouchers. An account is required with service.
Runscope API: Built to support the modern application development lifecycle, Runscope is a suite of developer tools allowing users to debug, test and share HTTP or REST API calls. The Runscope API provides developers with programmatic access to data in their Runscope accounts.
The API accepts requests sent over HTTPS, returns JSON formatted data, and supports OAuth2 authentication.",0.726,https://www.programmableweb.com/news/today-apis-diffbot-product-pages-api-and-4-new-apis/2013/07/31,,
"Daily API RoundUp: SignWise, Diffbot, Rice University, Fuzzy.ai, Social Bicycles",d2017-01-11T00:00,Joy Culbertson,ProgrammableWeb,"ProgrammableWeb,Diffbot,Source Code,Web API,Jump Bikes,Apis","Every day, the ProgrammableWeb team is busy, updating its three primary directories for APIs, clients (language-specific libraries or SDKs for consuming or providing APIs), and source code samples. If you have new APIs, clients, or source code examples to add to ProgrammableWeb’s directories, we offer forms (APIs, Clients, Source Code) for submitting them to our API research team. If there’s a listing in one of our directories that you’d like to claim as the owner, please contact us at editor@programmableweb.com.
Twenty four APIs have been added to the ProgrammableWeb directory in categories such as Cycling, Machine Learning, and Music. Highlights today include twelve APIs for use with the Polysync platform for building applications for driverless vehicles. Here's a rundown of the latest additions.
APIs
Social Bicyclesis a bike ride-sharing platform for cyclists in small towns or metropolitan cities, corporate workers, or university students. The platform is similar to Zipcar, in which riders reserve a bike online or at a bike hub, and then park the ride at any hub one they have reached their destination. The SocialBicycles API integrates mapped rides and statistics, CO2 reduced, calories burned, and money saved versus driving, all components of the Web data available in REST architecture. The API is listed under the Cycling category. See ProgrammableWeb's complete list of Cycling APIs.
Social Bicycles uses wireless tech to enhance cyclist's mobility. Image Credit: Social Bicycles
SignWise Services API provides the necessary infrastructure for applications to allow users to authenticate and electronically sign legal documents. This API is listed under the Electronic Signature category. See ProgrammableWeb's complete list of Electronic Signature APIs.
LIKE WHAT YOU SEE? GET THESE UPDATES AND OTHER API NEWS DELIVERED DAILY TO YOUR INBOX. REGISTER FOR THE PROGRAMMABLEWEB TODAY NEWSLETTER.
With Fuzzy.ai, developers can kickstart the machine learning process by starting with rules that learn and improve automatically over time. Fuzzy.ai API allows any developer to easily add machine learning to their working code. The API is useful for applications concerning product recommendations, content recommendations, dynamic pricing, matching in marketplaces and customer scoring into Web, mobile and conversational apps. This API is listed under the Machine Learning category. See ProgrammableWeb's complete list of Machine Learning APIs.
A Fuzzy agent is a virtual intelligent machine added to code for complex decision-making. Image Credit: Fuzzy.ai
Truckers MP is a multiplayer mod for American Truck Simulator and Euro Truck Simulator 2. The TruckersMP Web API provides access to game stats and data about rules, a player, game time and more. This API is listed under the Games category. See ProgrammableWeb's complete list of Games APIs.
Robin is an office meeting and conference room booking service. The Robin API is available to programmatically access the service and integrate it into third-party applications. We've listed this API in the Meetings category. See ProgrammableWeb's complete list of Meetings APIs.
EZ RentOut is an equipment rental management platform. The platform allows for equipment tracking, invoicing, inventory and order mangement, point of sale and webstore services, and customer management services. Business such as DVD and movie, sports equipment, party and events or any business that rents out equipment will find the platform useful. With the EZRentOut API, developers can rent assets and sell inventory through orders. The API is listed under the Rentals category. See ProgrammableWeb's complete list of Rentals APIs.
Diffbot's Global Index API allows users to search the Web as if it were a database. The API allows users to query the Web for news articles, authors, region, language, also reader comments, images and video. This API is listed under the Search category. See ProgrammableWeb's complete list of Search APIs.
Rice Apps provide developers a way to improve student life through technology at Rice University. Developers can utilize the Rice University Library API to allow library members to access data about the University and information about the Fondren Library. Rice University Courses API allows developers to receive data such as course titles and course dates. Rice University People API provides names and IDs of Rice University students and faculty. Theses API are listed under the Education category. See ProgrammableWeb's complete list of Education APIs.
Trafiklab is an online community that provides APIs for public transport in Sweden. The Trafiklab ResRobot Travel Planner REST API provides travel planning data for all of Swedish public transport. That includes time tables for all trains, busses, subways and trams in Sweden. Data about bus stops and stations is also available. The API is listed under the Transportation category. See ProgrammableWeb's complete list of Transportation APIs.
Musemantik's MusicFlow is an online music production service that enhances media content such as videos, games, and animations. The platform provides music that adapts to the mood and feel of every moment of media content. The MusicFlow Web service API allows applications such as video editors or other content creation tools to incorporate and customize the functionality of MusicFlow's music content and service. This API is listed under the Music category. See ProgrammableWeb's complete list of Music APIs.
Polysync is a technology company that aims to ""simplify and accelerate the development of autonomous cars."" We've added a dozen APIs to the directory in the Auto category for use with the Polysync self-driving vehicle applications platform.
They are:
Polysync Core REST API allows developers to access and integrate the functionality of Polysync with other applications and to create new applications. The API provides access to the most commonly used functions of Polysync.
Polysync CAN API can be used to connect to and interface with devices that use controller area network protocols.
Polysync Messaging REST API can pass information using predefined messages.
Polysync Shared Memory API provides an interface for reading and writing data from a shared memory queue.
Polysync Host API provides access to runtime information about the local machine.
Polysync Record and Replay API enables the coordination between nodes participating in log sessions.
Polysync Socket API provides a wrapper around a network socket.
Polysync Video API method provides an interface for images, video, and decoder devices.
Polysync Transform API allows for performing linear transformations on coordinate frames.
Polysync Serial API's main API method is wrapping around a serial port.
Polysync Node API and Polysync Logfile API are also available.",0.696,https://www.programmableweb.com/news/daily-api-roundup-signwise-diffbot-rice-university-fuzzyai-social-bicycles/brief/2017/01/11,,
Diffbot PHP SDK by Bruno Škvorc,,Bruno Škvorc,ProgrammableWeb,"PHP,Diffbot,Software development kit,Application programming interface","The Diffbot PHP SDK by Bruno Škvorc allows developers to integrate Diffbot APIs in web content applications using the PHP language. It allows developers to make API calls using PSR-7 and PHP-HTTP friendly client implementations. The SDK currently supports the Diffbot Article, the Diffbot Analyze, the Diffbot Product, the Diffbot Crawl, the Diffbot Search, and the Diffbot Image APIs. Its deployment requires the minimum of PHP 5.6, although PHP 7 is the most recommended version.",,https://www.programmableweb.com/sdk/diffbot-php-sdk-bruno-%C5%A1kvorc,,
Diffbot Adds Page Classifier API to Help Developers Categorize the Web | BetaKit,d2012-08-16T15:00:01,,BetaKit,BetaKit,"As a technology publication, BetaKit attends a lot of hackathons. Some big, some small, but all sharing a few commonalities: tired eyes, lots of…",0.188,http://www.betakit.com/diffbot-adds-page-classifier-api-to-help-developers-categorize-the-web/,,
"Daily API RoundUp: Mozilla WebVR, Yammer, CloudBoost, Diffbot Clients",d2015-06-24T00:00,Joy Culbertson,ProgrammableWeb,"Mozilla Corporation,ProgrammableWeb,Diffbot,Web API,Yammer,Ruby,Mozilla,WebVR,Apis,augmented reality","Every day, the ProgrammableWeb team is busy, updating its three primary directories for APIs, clients (language-specific libraries or SDKs for consuming or providing APIs), and source code samples. If you have new APIs, clients, or source code examples to add to ProgrammableWeb’s directories, we offer forms (APIs, Clients, Source Code) for submitting them to our API research team. If there’s a listing in one of our directories that you’d like to claim as the owner, please contact us at editor@programmableweb.com.
Six APIs have been added to the ProgrammableWeb directory today in Augmented Reality, Collaboration, and API Management categories, among others. Also added were several client libraries provided by Diffbot for use with Diffbot APIs.
APIs
The Mozilla WebVR project offers a way to provide high performance virtual reality experiences online. The Mozilla WebVR API allows developers to access and integrate the functionality of Mozilla WebVR with other applications and devices. Some example API methods include integrating virtual reality devices with WebVR functionality, managing movements and tracking of the devices, and setting parameters for the VR experience.
We’ve primarily categorized the Mozilla WebVR API under the Augmented Reality category. See ProgrammableWeb’s complete list of Augmented Reality APIs.
LIKE WHAT YOU SEE? GET THESE UPDATES AND OTHER API NEWS DELIVERED DAILY TO YOUR INBOX. REGISTER FOR THE PROGRAMMABLEWEB TODAY NEWSLETTER.
Yammer provides social and collaboration software for businesses and enterprises of all sizes. The Yammer Data Export API allows developers [verified Admin only] to package and export all messages, notes, files, topics, users, and groups. This API also allows for performing a one-time export by specifying the starting and ending dates for the export data. This API is categorized under the Collaboration category. See ProgrammableWeb’s complete list of Collaboration APIs.
Cloud Elements is an API management and integration platform that helps developers design, manage, and integrate their APIs. The Cloud Elements REST API allows developers to access and integrate the functionality of Cloud Elements with other applications. Some example API methods include retrieving instances, managing organization information, and managing user account information. The Cloud Elements Hub REST API allows developers to manage Customer Relationship Management (CRM) platforms, manage files, and manage eSignatures. We’ve primarily categorized the Cloud Elements APIs under the API Management category. See ProgrammableWeb’s complete list of API Management APIs.
Use CloudBoost to add data-storage, search, cache graphs, and real-time database service to your apps. The CloudBoost API provides database services for building apps. CloudBoost provides extensive API documentation for detailed instructions on setting up an app database. The REST API uses JSON over HTTP with HTTP Basic Auth for authentication.
The CloudBoost API is listed under the Database category. See ProgrammableWeb’s complete list of Database APIs.
EdX provides an open source online course platform to educators and higher education institutions. The EdX Profile Images API allows developers to enable their users to upload and remove profile images on their applications. The EdX Platform APIs use REST design principles, support the JSON data-interchange format, and use edX OAuth 2.0 for authentication. We’ve primarily categorized the EdX Profile Images API under the Education category. See ProgrammableWeb’s complete list of Education APIs.
Clients
Diffbot provides developers with tools that can identify, analyze, and extract the main content and sections from any web page. They provide several APIs for developers to extract information, analyze data, and gain insight. The company provides several clients for use with the Diffbot APIs. We’ve added the following clients provided by Diffbot to the client area of the ProgrammableWeb directory. They are as follows:
Diffbot C Client is a C language library that supports Diffbot Article, Frontpage, Product, Image, and Classifier (Analyze) APIs. This client requires lib curl and libjson-c to use.
Diffbot Perl Client provides a Perl language library for the Diffbot REST APIs. This client requires a Diffbot user token to implement.
Diffbot JavaScript Client buses JSONP protocol to support cross-domain communication and supports JavaScript in Diffbot Analyze and Article APIs.
Diffbot Ruby Client is the official Ruby library for use with Diffbot APIs and Crawlbot. This library uses Faraday as an HTTP middleware library.
Diffbot Go Client implements a Go language library for Diffbot APIs.
Diffbot Objective C Client allows general calls to be made to Diffbot Analyze and Article APIs using the Objective-C programming language.
Diffbot Scala Client uses Spray middleware and ActorSystem is required for use.
Diffbot R Language Client provides a simple function to be used to retrieve the JSON response of URLs provided with parameters to this function. Diffbot.R file and Curl and RJSONIO R-packages are required to use this library.
Diffbot RapidMiner Java Client provides RapidMiner 6.1 or above to analyze web pages.",0.592,https://www.programmableweb.com/news/daily-api-roundup-mozilla-webvr-yammer-cloudboost-diffbot-clients/brief/2015/06/24,,
Diffbot's Discussions API Provides Comment Section Searchability,d2015-03-31T00:00,Patricio Robles,ProgrammableWeb,"Livefyre,natural language processing,startup company,JavaScript,Diffbot,computer vision,machine learning","Diffbot, a computer vision, machine learning and natural language processing startup, today unveiled a new Discussions API that provides developers with access to conversations taking place on the web every day through comments, forums and reviews.
According to the company, its new Discussions API ""allows developers, branding executives, and media monitoring companies to monitor every conversation on the Web the same way they monitor Twitter."" A host of social media monitoring platforms have firehose access to popular social networks like Twitter, and have built solutions that enable companies to track and analyze the chatter on the those services.
But Diffbot, which says its mission is ""to bring structure to the unstructured Web,"" believes that the millions of conversations taking place across the web in comments sections and forums every day are often just as valuable. So it built the Discussions API to give developers the opportunity to build applications to more easily search and obtain insights from this content – content they currently have little to no access to.
Diffbot indexes data from a variety of comment platforms, including Disqus, Livefyre, Wordpress, Blogger, Intense Debate and Kinja, no small feat given that some of them are JavaScript-based, making them more difficult to crawl. It also picks up content from popular forums like Reddit, as well as reviews from Amazon and other retailers. Diffbot's technology automatically identifies the structure in the data it crawls, including author, date and discussion content.
Documentation for the Discussion API is now available in the Diffbot developer portal, and developers can use Diffbot's testdrive console to test the Discussions API against a URL of their choice.
Picking up Google's crumbs?
Despite the undeniable success of popular social networks like Facebook and Twitter, some still question the value of user generated content. The signal-to-noise ratio isn't very good, they argue. That might be one of the reasons why the world's largest search engine, Google, shuttered its own discussion search product.
At the time, a small but vocal group of users complained, but Diffbot CEO Mike Tung doesn't believe his company is trying to realize an opportunity Google missed. Google's product ""was a consumer functionality,"" Tung told me. Google was ""never in the business of providing web data to developers/business users.""
Specifically, Google's offering was also far more limited than Diffbot's. According to Tung, Google didn't look at comment platforms, and its coverage of forums was far narrower. The search giant's offering was also far less sophisticated in terms of structured data. With Diffbot's Discussions API, users can build smart queries, like ""give me images from forum posts made in January that mention apple watch,"" making it possible to identify interesting and potentially valuable content.
Diffbot's Discussions API also helps developers separate the wheat from the chaff. ""Developers can point the Discussions API at whatever sites and use whatever search terms they think will provide them with the best results,"" Tung explained. ""The Discussion API visual extracts the number of 'votes' of a comment (or likes/agrees). This provides a bit of social signal, similar to how a human would determine the best comments in a big list. We also extract the positive/negative sentiment of each comment, which can be used for sorting/filtering.""
Diffbot already offers a number of APIs and Tung tells me he's very excited to see how its newest addition to the Diffbot API suite is used. ""We've always been constantly surprised by what developers do with our tools – they are more creative than we are,"" he said. As 2016 nears, Tung is particularly interested in seeing how users employ his company's API for trend and sentiment analysis related to the upcoming presidential election cycle.",0.392,https://www.programmableweb.com/news/diffbots-discussions-api-provides-comment-section-searchability/2015/03/31,Was a consumer functionality.,
"Today in APIs: Diffbot’s New API is a Decoder Ring for the Web, Telesocial Releases Social Calling API and 16 New APIs",d2012-08-16T00:00,Wendell Santos,ProgrammableWeb,"San Francisco,Social Calling,Telesocial,statistical classification,Diffbot,Decoder Ring,hyperlink,bookmark","Diffbot's API uses visual learning robot to instantly identify page content that lies behind any URL. Telesocial releases an extension to their API. Plus: Facebook announces their upcoming World HACK tour, details on the TwilioCon 2012 Workshop Day and 16 new APIs.
Diffbot's ""Page Classifier"" API Identifies Page Content Behind any URL
Today, Diffbot announced the launch of their newest API, a tool that uses visual learning to instantly identify page content that lies behind any URL. The API looks to take on the growing problem of decoding hyperlinks coming from mobile browsing, social sharing sites and link shortening services. In their release Diffbot explains how it works:
Building atop the more than 100 million URLs analyzed monthly by its developers, Diffbot has trained its visual robot to categorize the entire web into 20 different page types. The new Page Classifier tool lets developer applications identify and classify the type of page and language behind any URL, understanding instantly whether a given link leads to a product, map, social networking profile or any of the other identified page types.
The Page Classifier API is already being used by social bookmarking application Springpad and is the latest addition to the suite of Diffbot APIs.
Telesocial Releases Social Calling API for LinkedIn Developer Platform
Telesocial, the San Francisco-based telephony platform that enables mobile voice calling within social networks, announced the extension to their API. The update allows developers to use the Telesocial API and LinkedIn platform to integrate mobile voice features into their applications for free. As redOrbit describes:
The Telesocial API is unique because it gives developers a versatile interface, providing for a wide range of voice-powered use cases and integrations. Unlike existing VoIP solutions, a user does not need to be logged into the social platform or application to receive a call or a voice message, as call functionalities are delivered via the existing mobile-phone infrastructure.
Another addition to the API lets users click-to-call directly from within a mobile app without knowing a receivers' phone number. This allows the use of a LinkedIn profile as a phone book while enhancing privacy.
API News You Shouldn’t Miss:
16 New APIs
Today we had 16 new APIs added to our API directory including a customer relationship management service, mediates between widgets and backplane server, authenticates calls to server, universal data standards repository, lesson plan creation and delivery platform, forex trading platform, uk vacation search and booking service, online payment solution service, textual criticism tool, slideshow creation platform, notification alerts and mobile data delivery, two-way global sms, systembolaget alcohol database access, smart tv and media platform, financial data feed service, writing content marketplace and publishing platform. Below are more details on each of these new APIs.
amoCRM API: amoCRM is an online sales and relationship management service designed to help users manage and pursue sales leads. The amoCRM API allows users to retrieve, search, edit and add contacts, deals, notes, and tasks. The service is available over HTTPS GET and POST calls, and returns XML for all responses.
Backplane JavaScript API: The Backplane JavaScript Library runs in an end user’s browser and mediates communication between Backplane-enabled Widgets on the page and the Backplane Server. The Backplane JavaScript Library provides an API for developers that assures it will be the first library to load on the page to make it possible for other scripts to use its subscription functionality.
Backplane Server API: The Backplane Server is an independent orchestrator of the message interchange between Backplane Clients and may serve multiple independent buses. Once the Backplane client has been recognized as authenticated and the buses specified then the server will continue through the authorization grant. The API uses OAuth2 and HTTP basic authentication and returns messages in JSON format.
CommonDataHub API: CommonDataHub (CDH) is the global repository for data standards such as ISO codes and industry code sets. CDH aggregates and consolidates data sets from multiple sources, provides additional attributes as needed and maps related code sets giving users a full picture of a subject area. The CDH API gives users the capability to retrieve data and use it in their own applications. The API uses SOAP protocol and responses are formatted in XML.
Educreations API: Educreations is learning tools platform. It allows users to create and share lesson plans with their mobile devices. Educreations was built for iPad but users can also draw lessons plans on web browser platform. Users can set their lesson plans to be available for public consumption on Educreations Showcase page. Otherwise they may embed them on personal sites or provide private links for their students. The Educreations API is available to their web and mobile clients.
Gain Capital AutoEx Trading API: GAIN Capital is a leading provider of online forex trading. The GAIN Trading platform gives members an environment where they can conduct trading in an anonymous fasion with direct access and trade execution capabilities using streaming prices that provide a transparent view of both price and order book depth. The platform functionality is available via a SOAP API for integration into third party systems.
Hoseasons API: Hoseasons is a leading UK vacation company. Hoseasons offers access to over 40,000 properties across the UK and Europe through various brands. Visitors can search for, compare prices and book their vacations. The Hoseasons API offers much of the functionality of the site, making it available to affiliates. The data is returned in real-time so that affiliates can offer customers the latest deals. Nearly 30 methods are available via a SOAP API with responses formatted in XML.
Paymill API: Paymill is a credit card payment processing API. It allows businesses to develop simple and cost-effective credit card payment solution with their existing technology. Users create an account that gives them a means of processing credit cards and a dealer cockpit that keeps a record of payment transactions. The API is RESTful and returns JSON-encoded data. The website is originally in German.
Performant Juxta API: Juxta is an open-source tool that lets scholars and researchers examine the history of a text from manuscript to print versions. Users can compare and collate multiple witnesses to a single textual work. The desktop version of the software lets users complete textual criticism operations on digital texts. Users can also annotate Juxta-revealed comparisons and save the results. The web service offers a limited set of functionality, uses HTTP calls and responses are formatted in JSON.
Picovico API: Picovico is a slideshow creation tool. Its platform allows users to upload photo sets and arrange them in desired order. Users can spiffy up their presentations with Picovico’s themes and styles. Furthermore, users can upload their own audio to add a soundtrack their slideshow. The final product has its own URL for users to share. The Picovico API exposes the entire slideshow creation functionalities for developers to build new applications and servers on top. It is a RESTful API that returns JSON-encoded data.
Push IO API: Push IO is a provider of real-time push notification alerts and mobile data delivery. The API offers methods for sending category-based target broadcasts and test device push notifications to all platform users with a single call. The Push IO API is a simple RESTful API with JSON responses and POST calls.
Quiubas SMS API: The Quiubas SMS API allows application developers to send text messages to over 200 countries. With this, developers also receive Two-Way SMS functionality in a variety of different languages all for a small fee. The API is available in PHP, C++, JAVA and HTTP.
Systembolaget System API: Systembolaget, the Swedish alcohol monopoly mandated by the Swedish state to help control the medical and social harm caused by alcohol, has released a simple API to retrieve product information. The idea of the API is to fill the void that the monopoly creates as they only provide big XML / XLS files that users have to download and then entertain themselves. This API provides a quick and easy retrieval of Systembolaget's current sales records.
TVSync API: TVSync is a media management API platform. It allows developers to build apps for web, mobile, TV, and more. It can identify media content from TV, music, and movies to authenticate and describe a stream’s assets for consumers. It also allows developers to collocate different for unique content delivery. Developers must request access to the closed beta API. The API exposes the essential content identification and delivery functionalities.
WorldDataSource API: WorldDataSource is an information provider that gives users access to financial information from markets around the world. The WorldDataSource API is a Java API that provides users with direct access to data feeds from the New York Stock Exchange, American Stock Exchange, Nasdaq Stock Exchange, Chicago Board of Trade, Chicago Mercantile Exchange, New York Mercantile Exchange, New York Boards of Trade, and other financial exchanges.
WriterAccess API: WriterAccess is a marketplace for writers and publishers to connect. As a content marketing platform, it allows businesses the opportunity to employee expert writers efficiently and remotely. Writers have profiles describing their expertise that businesses can browse. Content editors can also be hired. The WriterAccess API exposes integration functionality for developers to build one-click publishing features. It also exposes account management and communication functionality.",0.411,https://www.programmableweb.com/news/today-apis-diffbot%E2%80%99s-new-api-decoder-ring-web-telesocial-releases-social-calling-api-and-16-new-apis/2012/08/16,,
Diffbot:????? ?web???????,d2011-08-26T15:56,,QQ.com,"???,??,???,??????","?????(Kathy)????8?26???,???????,Diffbot?????????“????”, ??????:????????????????????,???????Web?????Diffbot??????Mike Tung?:“???????????30??????,Diffbot????????????”????,Diffbot?????????????????????????????????????
??,Diffbot????????API(??????),??????????????????????????????????????????,????????,????????????????
????API
???????Diffbot??????API???????????:????????????;????????????????????;?????RSS??????RSS????;????????????,?????????????????
??????????Diffbot??????API,??????????????????,???????????????,?????????????????????Diffbot?????????,?????API?????????????
???????? API?:
On-Demand API:??API???????“??” (Frontpage)API?“??” (Article) API??????????????????(??????????????????????????????),“??”API?????“???”???????????
Follow API:???????????????????Diffbot???????????????????,?????????????????,???????????????
??????????Diffbot?API?,???????????Nuance??,????(AOL)??,?????????SocMetrics???
AOL??Diffbot?API?????iPad ??????????????????,????????? Nuance????????????????????????;??????????????????????SocMetrics??bit.ly?????Diffbot,????????????,?????????????????????????
???????????Diffbot?????,???????????????????????Hacker News Radio(??????)?????????«????»??????,FeedBeate?????????????????RSS?????????Diffbot??Twitter?: ???????????????????(???RSS),??Twitter ???????
“????”??
??Diffbot???????????,?????????API 5???“???” ?? 500??,???????API 10??,??????? 0.002???????????????????????
Diffbot????????????Mike Tung?Leith Abdulla??????Tung????????????????????????????Diffbot??????????(????SSE Labs,??StartX)??????????????????>>",,http://tech.qq.com/a/20110826/000428.htm,,
Ready to Go Shopping? Diffbot’s Product API Parses Product Data,d2013-08-01T00:00,Amy Castor,ProgrammableWeb,"startup company,ProgrammableWeb,data mining,AlchemyAPI,Diffbot,Web API,E-commerce,representational state transfer","Diffbot, a startup known for its data mining services, released a new API that will scan an e-commerce shopping page and parse information on a particular product. Developers can use the API in their web and mobile apps to find the best price on different sites, track merchandise availability or even migrate shopping sites to new platforms without having to deal with backend integration.
The Palo Alto company says it spent two years developing its new API, which is based on the company’s core technology used for extracting structured data from Internet web pages.
If you’re not sure of the difference, structured data is machine readable data that is organized in fixed fields such as columns and rows. In contrast, unstructured data is generally human communication in the form of letters, emails, documents and social media.
According to Diffbot, its Product API can parse from an e-commerce product page information such as price, discount or savings, shipping cost, product description, images, SKU and manufacturer's product number.
Diffbot suggests using the Product API in conjunction with its site-spidering tool Crawlbot, which extracts an entire e-commerce site and then pinpoints the pages containing product information.
The Product API is a RESTful API that returns JSON. Currently, the API only returns extracted data from a single product. The company says that in the future, the API will return information from multiple products, if multiple items are available on the same page.
ProgrammableWeb recently reported on two other companies aiming to bring data mining tools to the masses, AlchemyAPI and Textalytics, although these firms focus on unstructured data.",0.183,https://www.programmableweb.com/news/ready-to-go-shopping-diffbot%E2%80%99s-product-api-parses-product-data/2013/08/01,,
"API Spotlight: Diffbot, Call Loop, and NFL Data.com APIs",d2013-08-03T00:00,Matthew Scott,ProgrammableWeb,"StatusPage,Google,web search engine,Vidyard,Mailchimp,Call Loop","Of the many APIs we published this week, thirteen were highlighted on the blog by our team of writers. In this post, we’ll shine a spotlight on those thirtenn, which include the StatusPage API. The StatusPage API provides developers with a solution to customer communication when their site or application is experiencing downtime. Specifically, it provides notification tools (email and SMS), downtime monitoring, insight into performance metrics, and customer customization. To learn more about the StatusPage API visit the StatusPage site as well as the StatusPage API blog post.
Tracking the effectiveness of a video campaign, how viewership is translated into sales, is inherently difficult. That is why Vidyard and the Vidyard API break down videos and provide developers with the tools they need to gain insight into their videos. These tools consist of video player customization, thumbnail customization, and the ability to see what parts of the video are being watched, skipped, or even watched again. To learn more about the Vidyard API visit the Vidyard site as well as the Vidyard API blog post.
The KarmaCRM API is a client relationship management system that bridges the gap between simplicity and functionality. It’s meant for anyone in sales and provides various tools to stay in contact with future and present clients. These tools consist of email notifications, filtering functionlaity, customizing contact lists, reports, chats, and etc. It also allows users to import data from Google calenders, MailChimp, and others. To learn more about the KarmaCRM API visit the KarmaCRM site as well as the KarmaCRM API blog post.
OpenSearchServer is, if you couldn’t guess it by the name, a search engine website. Their OpenSearchServer Screenshot API allows users to take a screenshot of a website and save it in a PNG format. Said pictures can be taken by specifying the screen size, single areas of the screen to be captured, and even a delay function to capture images that will appear on the screen shortly. To learn more about the OpenSearchServer Screenshot API visit the OpenSearchServer site as well as the OpenSearchServer Screenshot API blog post.
The revision of the Opendatacommunities API brings forth a few new tools and features to access UK Government data sets. Of these new features are the quickened deployments of Government-collected data, including finance, fire and rescue, and housing and planning data, all of which comprise of the 83 data sets currently available. Currently there is a focus on wellbeing indicators but look for that focus to expand as the API is built further. To learn more about the Opendatacommunities API visit the Opendatacommunities site as well as the Opendatacommunities API blog post.
The Sensetonic WoTkit (Web of Things kit) API gives developers the ability to connect objects to the Internet. As crazy as that sounds, connecting real-world objects to the internet, the company actually intends for end users to become sensors, or participants, that will provide developers valuable insight on their customers. To learn more about the Interent of things (IoT) visit the Sensetonic WoTkit website as well as the Sensetonic WoTkit API blog post.
Imagine you are online shopping for a gift for someone. You could have it shipped to your residence and add a card yourself, pay for a cheap typed out card from the retailer, or send a Sociagram video message along with it. If you chose the latter then the Sociagram API, allowing developers to integrate Sociagram functionality to sites, is something you might want. The API isn’t publicly available but developers can gain access by emailing the partnership department. To learn more about the Sociagram API visit the Sociagram site as well as the Sociagram API blog post.
The Oakland Athletics were on of the first teams to use analytics to determine the best team composition, use of money, and recruitment of future prospects in the MLB. Times have come far from that as shown by the release of the NFL Data.com API. The API opens NFL Data.com’s data sets to be callable via the API with a focus for those who play fantasy football. The data includes basic stats, rankings, injury reports, news, roster updates and etc. To learn more about the NFL Data.com API visit the NFL Data.com site as well as the NFL Data.com API blog post.
Evvnt is an online event marketing service designed to help event planners reach their target audience by broadcasting and publishing the event across the web. The Evvnt API allows developers to integrate the functionality of the Evvnt site into their applications and websites. This functionality consists of event publishing and broadcasting across relevant sites, targeting specific users, natural searches, and event publishing technology for specific publishers. To learn more about the specifics of the functionality as well as the Evvnt API, visit the Evvnt site and stop by the Evvnt API blog post.
Call Loop, a web based voice and SMS provider, has announced the release of their Call Loop API. The API gives developers the ability to integrate Call Loop’s broadcast communication functionality with their sites or applications. Developers can manage their current customer base by adding or deleting users then contact them with a simple function. To learn more about the Call Loop API visit the Call Loop site as well as the Call Loop API blog post.
App Annie tracks application metrics and provides business with app store data so they can make the best business decisions moving forward. The App Annie API provides current developers with a way to receive this information programmatically through their applications. As of this moment, the API is only available to existing App Annie users but the company plans to release additional API’s that will provide third party developers with the same functionality as in house developers. To learn more about the specific of the App annie API visit the App Annie site as well as the App Annie API blog post.
AppDirect is a cloud service marketplace and management platform designed to help marketplace developers with all the heavy back-end functionality. The company recently released five separate but connected APIs. The five APIs, Marketplace Listing, MyApps, App Profile Page, Accounts, and Analytics API, allow developers to integrate all the features of AppDirect. The most notable features are back-end billing as a service and more than 150 cloud applications that developers can leverage. To learn more about the AppDirect API visit the AppDirect site as well as the AppDirect API blog post.
Diffbot, specializing in extracting structured data from Internet web pages, has released the Diffbot API, which puts this functionality in the hands of developers. To be more specific, the Diffbot API parses product data from multiple websites providing developers insight into the best prices, merchandise availability, and tracking. Futerhmore, the API runs in conjunction with Diffbot’s tool Crawlbot. To learn more about Crawlbot and the Diffbot API visit the Diffbot site as well as the Diffbot API blog post.",0.355,https://www.programmableweb.com/news/api-spotlight-diffbot-call-loop-and-nfl-data.com-apis/2013/08/03,,
Diffbot Analyze API Enables Automatic Data Extraction,d2014-10-30T00:00,Candice McMillan,ProgrammableWeb,"Automatic Data Processing,Palo Alto,Diffbot,machine learning","Imagine the possibilities when apps and programs can see the web the way humans do? Well, this is what Palo Alto-based startup, Diffbot, set out to achieve. Using a combination of crawling software, computer vision and machine learning, the company provides something that understands pages on the web and is able to classify them and break each one down into its basic parts. Diffbot's Analyze API makes this functionality available to developers.
So how does it all work exactly? In an article on Xconomy, the author sums it up quite nicely, saying,
""Diffbot runs virtual browsers in the cloud that can go to a given URL; suck in the page’s HTML, scripts, and style sheets; and render it just as it would be shown on a desktop monitor or a smartphone screen. Then edge-detection algorithms and computer-vision routines go to work, outlining and measuring each element on the page. Using machine-learning techniques, this geometric data can then be compared to frameworks or 'ontologies'...""
Diffbot makes it possible for users to automatically retrieve the data they need from specific web pages. Users can access content from articles, products, images and other page types using Diffbot's selection of automatic APIs. The Analyze API does just what its name suggests; it analyzes a web page visually and accessing the page's URL, can determine what type of page it is. The Analyze API will then determine which of the extraction APIs would be appropriate; the Article API, Image API, Product API or Discussion API. The Article API extracts clean article text and related data from news articles and blog posts, the Image API identifies primary images on a page and returns detailed information about those images, the Product API extracts detailed data from shopping or e-commerce product pages, and the Discussion API (currently in beta) extracts detailed information regarding discussion pages.
Further information and API documentation is available on the Diffbot website.",0.49,https://www.programmableweb.com/news/diffbot-analyze-api-enables-automatic-data-extraction/brief/2014/10/30,,
Diffbot Article API,,,ProgrammableWeb,"Diffbot,American Petroleum Institute","The Diffbot Article API automatically extracts clean article text and other article data (author, date, images, etc.) from news article web pages and blog posts. The Article API works in any language; automatically concatenates multiple-page articles; extracts comments where available using functionality integrated from the Diffbot Discussion API; and also optionally performs sentiment analysis and entity-extraction/tag-generation on the extracted text.",0.3,https://www.programmableweb.com/api/diffbot-article,,
Diffbot Receives Equity Investment from Bloomberg Beta,d2015-06-04T00:00,"USA 
Published",FinSMEs,"Web page,Robot,Palo Alto, California,Company,Extract,Bloomberg Beta,Diffbot,Investment,Technology,Application software","Diffbot, a Palo Alto, CA-based devceloper of an engine to find, extract and understand the objects from any Web page for use in their applications, received an equity investment from Bloomberg Beta.
The amount of the deal was not disclosed.
The company intends to use the funds to expand its visual robot technology.
Led by Mike Tung, Founding CEO, Diffbot combines computer vision, machine learning, and artificial intelligence to allow businesses to find, extract and understand the objects from any Web page for use in applications of companies including Adobe, CBS Interactive, Cisco, eBay, Instapaper, Salesforce, Samsung, StumbleUpon.
FinSMEs
04/06/2015",,http://www.finsmes.com/2015/06/diffbot-receives-equity-investment-from-bloomberg-beta.html,,
"Web Scraping Software Market 2020 COVID-19 Impact Analysis, Revenue Study, Best Players – DataForSEO,Diffbot,HelpSystems,Import.io,justLikeAPI",d2020-11-14T01:17,Premium Market Insights,openPR.com,"pricing,Software Market,Web mining,HelpSystems,Diffbot,DataForSEO,justLikeAPI,COVID-19,software,Mozenda","Web scraping software is data scraping utilized for extracting data from websites. Web scraping a web page comprises fetching and extracting data from it. Web scraping is used for contact scraping, web mining and data mining, online price change monitoring, and price comparison. Web scraping is also known as web harvesting or web data extraction.
Key Players:
– DataForSEO
– Diffbot
– HelpSystems
– Import.io
– justLikeAPI
– Mozenda, Inc.
– Octopus Data Inc.
– Scrapinghub
– SerpApi, LLC
– Webhose.io
Request Sample Copy of Web Scraping Software Market: https://bit.ly/35rtccf
The growth of the web scraping software market is driven by key factors such as manufacturing activity in accordance with the current market situation and demand, risks of the market, assessment of the new technologies, acquisitions, new trends, and their implementation. Moreover, an increase in research and development activities in various industries is anticipated to boost the growth of the web scraping software market.
The global web scraping software market is segmented on the basis of deployment type, organization size. On the basis of deployment type, the market is segmented as on-premise, cloud. On the basis of organization size, the market is segmented as large enterprises, SMEs.
The “Global Web Scraping Software Market Analysis to 2027”? is a specialized and in-depth study of the web scraping software market with a special focus on the global market trend analysis. The report aims to provide an overview of web scraping software market with detailed market segmentation by deployment type, organization size. The global web scraping software market is expected to witness high growth during the forecast period. The report provides key statistics on the market status of the leading web scraping software market players and offers key trends and opportunities in the web scraping software market.
The report analyzes factors affecting web scraping software market from both demand and supply side and further evaluates market dynamics effecting the market during the forecast period i.e., drivers, restraints, opportunities, and future trend. The report also provides exhaustive PEST analysis for all five regions namely; North America, Europe, APAC, MEA and South America after evaluating political, economic, social and technological factors effecting the web scraping software market in these regions.
Avail Discount on this Report@: https://bit.ly/3put7fZ
Critical Success Factors (CSFs)
The competitive landscape of the market has been examined on the basis of market share analysis of key players. Detailed market data about these factors is estimated to help vendors take strategic decisions that can strengthen their positions in the market and lead to more effective and larger stake in the global Web Scraping Software Market. Pricing and cost teardown analysis for products and service offerings of key players has also been undertaken for the study.
Table of Contents:
1 Executive Summary
2 Preface
3 Web Scraping Software Market Overview
4 Market Trend Analysis
5 Global Web Scraping Software Market Segmentation
6 Market Effect Factors Analysis
7 Market Competition by Manufacturers
8 Key Developments
9 Company Profiling
Sameer Joshi
Call: US: +1-646-491-9876, Apac: +912067274191
Email: sales@premiummarketinsights.com
About Premium market insights:
Premiummarketinsights.com is a one stop shop of market research reports and solutions to various companies across the globe. We help our clients in their decision support system by helping them choose most relevant and cost effective research reports and solutions from various publishers. We provide best in class customer service and our customer support team is always available to help you on your research queries.
This release was published on openPR.",0.779,https://www.openpr.com/news/2187693/web-scraping-software-market-2020-covid-19-impact-analysis,,
Diffbot Secures $10M in Series A Funding,d2016-02-11T00:00,FinSMEs,FinSMEs,"Series A round,Diffbot,Palo Alto, California,Application software,Tencent,Natural language processing,Web page,Software developer,Computer vision,Andy Bechtolsheim","Diffbot, a Palo Alto, CA-based platform that structures the world’s knowledge, secured $10m in Series A funding.
The round was led by Tencent and Felicis Ventures, with participation from Andy Bechtolsheim, Amplify Ventures, Valor Capital, Bill Lee, and Georges Harik.
The company intends to use the funds to build the platform.
Led by Mike Tung, Founding CEO, Diffbot is a robot that examines the Web using computer vision and natural language processing, and provides developers with tools to find, extract and understand the objects from any Web page for use in their applications. The technology visually recognizes, reads, understands, and monitors Web pages and components including product pages, news articles, discussions/comments/forums, videos, pictures, and more. Each element of the Web page is extracted, organized, tagged, cross-referenced, and stored as an “object” in the company’s database, called Global Index. To date, the Global Index contains more than 1.2B objects, and is adding roughly 10 million objects per day.
Thousands of developers and businesses use Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, the mobile web and hundreds of other consumer applications.
Customers include Adobe, Amazon, CBS Interactive, Cisco, eBay, Instapaper, Microsoft, Salesforce, Samsung, StumbleUpon.
11/02/2016",,http://www.finsmes.com/2016/02/diffbot-secures-10m-in-series-a-funding.html,,
Launching the Largest Database of Human Knowledge: Diffbot Knowledge Graph,d2018-08-31T00:00,A.R. Guess,DATAVERSITY,"Knowledge Graph,Database,Diffbot,Knowledge","A new press release reports, “Diffbot today announced the launch of Diffbot Knowledge Graph (DKG): all of the knowledge on the Web, collected and connected into a single, structured source of data, answers, insights, and truth. Using a sophisticated combination of machine learning, computer vision, and natural language processing, the DKG is a fully autonomous, AI-curated database of more than 1 trillion facts and 10 billion entities. This represents a repository of knowledge that is nearly 500 times larger than the Google Knowledge Graph, and growing every day. Diffbot is the first company to turn broad-application Artificial Intelligence into a profitable business, powering applications for customers including Salesforce, Cisco, eBay, Yandex, and more. Far from a theoretical research project in search of a business application, Artificial Intelligence is the backbone of Diffbot and the company uses state of the art AI methods to deploy profitable products at scale while also furthering the field by funding extensive research.”
The release goes on, “In contrast to other solutions marketed as Knowledge Graphs, the DKG is: (1) Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labor. (2) Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models. (3) Web-wide, regardless of originating language. Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as English. (4) Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.”
Photo credit: Diffbot",,http://www.dataversity.net/launching-largest-database-human-knowledge-diffbot-knowledge-graph/,,
Diffbot Raises $2M,d2012-05-31T00:00,FinSMEs,FinSMEs,"Diffbot,Palo Alto, California,Joi Ito,Sky Dayton,Brad Garlinghouse,Web page,EarthLink,Matrix Partners,Andy Bechtolsheim,Application programming interface","Diffbot, a Palo Alto, CA-based maker of visual learning robot technology that allows developers to analyze, extract, and enhance web content, has raised $2m in funding.
Backers include:
– Matrix Partners;
– Sky Dayton, founder of EarthLink;
– Andy Bechtolsheim, co-founder of Sun Microsystems;
– Joi Ito, Director of the MIT Media Lab;
– Brad Garlinghouse, CEO of YouSendIt, and
– other executives and founders from Facebook, Twitter and Yahoo.
The company intends to use the capital for new hires.
Led by Founder and CEO Michael Tung, Diffbot is a new form of visual-based technology that identifies and extracts the important objects on web pages using artificial intelligence, computer vision, machine learning and natural language processing. The company’s APIs allow application developers to use data from any Web page in their own applications. Customers are using Diffbot for web site mobilization, content management system migration, tag generation, article grouping/clustering, etc. .
31/05/2012",,http://www.finsmes.com/2012/05/diffbot-raises-2m.html,,
The Largest Database of Human Knowledge? Diffbot Knowledge Graph,,,IDM.net.au,"data,natural language processing,artificial intelligence,database,Diffbot,Salesforce.com,World Wide Web,Knowledge Graph,knowledge,ontology","AI startup Diffbot has announced the launch of Diffbot Knowledge Graph (DKG): all of the knowledge on the Web, collected and connected into a single, structured source of data, answers, insights, and truth.
Using a sophisticated combination of machine learning, computer vision, and natural language processing, the DKG is a fully autonomous, AI-curated database of more than 1 trillion facts and 10 billion entities. This represents a repository of knowledge that is nearly 500 times larger than the Google Knowledge Graph, and growing every day.
Diffbot claims to be the first company to turn broad-application Artificial Intelligence into a profitable business, powering applications for customers including Salesforce, Cisco, eBay, Yandex, and more.
Far from a theoretical research project in search of a business application, Artificial Intelligence is the backbone of Diffbot and the company uses state of the art AI methods to deploy profitable products at scale while also furthering the field by funding extensive research.
The company says in contrast to other solutions marketed as Knowledge Graphs, the DKG is:
Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labour.
Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models.Web-wide, regardless of originating language.
Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as English.
Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.
Starting today, any business that wants instant access to all of the world’s knowledge can simply sign up for the DKG and turn the entire Web into their personal database for business intelligence across:
People: skills, employment history, education, social profiles
Companies: rich profiles of companies and the workforce globally, from Fortune 500 to SMB’s
Locations: mapping data, addresses, business types, zoning information
Articles: Every news article, dateline, byline from anywhere on the Web, in any language
Products: pricing , specifications, and, reviews for every SKU across major ecommerce engines and individual retailers
Discussions: chats, social sharing, and conversations everywhere from article comments to web forums like Reddit
Images: billions of images on the web organized using image recognition and meta data collection
“A Web-wide, comprehensive, and interconnected Knowledge Graph has the power to transform how enterprises do business. Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers,” said Mike Tung, founder and CEO of Diffbot. “What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
DKG data can be integrated via API into any internal business process or application, from business intelligence and analytics to marketing campaigns and CRM systems. Users can also create custom queries using Diffbot’s DQL syntax. Users simply enter a query and the DKG instantly generates a comprehensive set of results with every single item on the internet that relates to it, with links to all existing connections between those results. Results can be viewed in a list, map or table layout, with the ability to easily expand or refine results based on connections captured by the Knowledge Graph.
“Simply put, Diffbot is using the power of AI on a scale we’ve never seen before,” said Aydin Senkut, founder and managing director of Felicis Ventures, one of Diffbot’s investors. “It’s the first profitable AI company on record, they are the ‘secret ingredient’ powering applications from many of the largest companies in tech, and the launch of the Knowledge Graph is going to further elevate Diffbot’s status as a clear leader in the space.”
Companies interested in accessing the Diffbot Knowledge Graph can contact Diffbot at sales@diffbot.com or visit www.diffbot.com/knowledge-graph",,https://idm.net.au/blog/0012170-largest-database-human-knowledge-diffbot-knowledge-graph,"A Web-wide, comprehensive, and interconnected Knowledge Graph has the power to transform how enterprises do business. Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers.,What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.,Simply put, Diffbot is using the power of AI on a scale we’ve never seen before.,It’s the first profitable AI company on record, they are the ‘secret ingredient’ powering applications from many of the largest companies in tech, and the launch of the Knowledge Graph is going to further elevate Diffbot’s status as a clear leader in the space.",
Realtime Text Content Analysis and Categorization with Diffbot,d2017-01-30T00:00,PubNub Staff,PubNub,"content analysis,JavaScript,plain text,analysis,Diffbot,categorization,content,real-time computing,application programming interface,user interface","Diffbot is a powerful API that extracts web data and content from articles, products, discussions, images, and more. Using AI, computer vision, and natural language processing, the API understands objects from any webpage and retrieves clean, structured data.
A perfect fit for the PubNub BLOCKS Catalog, our new Diffbot block for analyzing and extracting web data allows you to process incoming realtime messages with attached URLs, and amend website contextual information to that stream. For example, if you were to submit a URL to a New York Times article, Diffbot and PubNub would output a message that includes article type, language, operating system, and titles.
So, what exactly is this content analysis that Diffbot provides? In this case, content analysis refers to taking a piece of text and trying to extract features such as author and language, assign tags with numeric scores (indicating confidence), and even provide semantic categories and relationships. Content analysis has a number of challenges, including incomplete context, ambiguity, sarcasm, slang, international languages and issues with domain-specific text analysis.
Content Analysis Tutorial
In this tutorial, we’ll dive into a simple example of how to enable textual content analysis in a realtime AngularJS web application using 25 lines of the PubNub JavaScript BLOCK and 74 lines of HTML and JavaScript. In the end, you’ll have an app with this basic functionality:
As we prepare to explore our sample web application with content analysis features, let’s check out the underlying Diffbot Analysis API.
Diffbot Analysis API
Automated text and content analysis services are quite challenging to build and train on your own; they require substantial effort and engineering resources to maintain across a diverse array of application domains and user languages (not to mention immense compute resources and training sets!). In the meantime, the
On the other hand, the Diffbot Analysis APIs make it easy to enable your applications with straightforward text content analysis.
Looking closer at the APIs, text content analysis is just the beginning. There are a lot of API methods available for things like image and video processing and categorization, discussion analysis and more. It really is a powerful tool for distilling meaning from text, images and video. In this article though, we’ll keep it simple and just implement a basic text content analysis for user-provided URLs.
Since you’re reading this at PubNub, we’ll presume you have a realtime application use case in mind. In the sections below, we’ll dive into the content analysis use case, saving other web service use cases for the future.
Obtaining your PubNub Developer Keys
To get started, you’ll need a PubNub account, which includes your unique publish and subscribe keys. Once you do that, the publish and subscribe keys look like UUIDs and start with “pub-c-” and “sub-c-” prefixes respectively. Keep those handy – you’ll need to plug them in when initializing the PubNub object in your HTML5 app below.
PubNub JavaScript SDK
PubNub plays together really well with JavaScript because the PubNub JavaScript SDK is extremely robust and has been battle-tested over the years across a huge number of mobile and backend installations. The SDK is currently on its 4th major release, which features a number of improvements such as isomorphic JavaScript, new network components, unified message/presence/status notifiers, and much more.
NOTE: for compatibility with the PubNub AngularJS SDK, our UI code will use the PubNub JavaScript v3 API syntax. We expect the AngularJS API to be v4-compatible soon. In the meantime, please stay alert when jumping between different versions of JS code!
Getting Started with Diffbot Analysis API
Next you’ll need a Diffbot account. Head over to the Diffbot signup form and sign up for a free trial, and make note of the API credentials (client token) sent to the registration email address.
Setting up the BLOCK
Next is getting started with PubMub BLOCKS.
Step 1: go to the application instance on the PubNub Admin Dashboard.
Step 2: create a new BLOCK.
Step 3: paste in the BLOCK code from the next section and update the credentials with the Diffbot credentials from the previous steps above.
Step 4: Start the BLOCK, and test it using the “publish message” button and payload on the left-hand side of the screen.
That’s all it takes to create your serverless code running in the cloud with BLOCKS!
Diving into the Code – the BLOCK
You’ll want to grab the 25 lines of BLOCK JavaScript and save them to a file called pubnub_diffbot_block.js. It’s available as a Gist on GitHub for your convenience.
First up, we declare our dependency on xhr and query
let xhr = require('xhr');
let query = require('codec/query_string');
Next, we set up variables for accessing the service (the client token from previous steps and API url).
Next, we set up the HTTP params for the analysis API request. We use a GET request to submit the data (by default). We use the client token to authenticate our request to the API. We pass the URL attribute from the message.
Next, we create the URL from the given parameters.
Finally, we call the analysis endpoint with the given data, decorate the message with a diffbotResponse value containing the parsed JSON analysis data, and catch any errors and log to the BLOCKS console.
All in all, it doesn’t take a lot of code to add text content analysis to our application.
OK, let’s move on to the UI!
Diving into the Code – the User Interface
You’ll want to grab these 74 lines of HTML & JavaScript and save them to a file called pubnub_diffbot_ui.html.
The first thing you should do after saving the code is to replace two values in the JavaScript:
YOUR_PUB_KEY: with the PubNub publish key mentioned above.
YOUR_SUB_KEY: with the PubNub subscribe key mentioned above.
If you don’t, the UI will not be able to communicate with anything and probably clutter your console log with entirely too many errors.
For your convenience, this code is also available as a Gist on GitHub, and a Codepen as well.
Dependencies
First up, we have the JavaScript code & CSS dependencies of our application.
For folks who have done front-end implementation with AngularJS before, these should be the usual suspects:
PubNub JavaScript client: to connect to our data stream integration channel.
AngularJS: were you expecting a niftier front-end framework? Impossible!
PubNub Angular JavaScript client: provides PubNub services in AngularJS quite nicely indeed.
Underscore.js: we could avoid using Underscore.JS, but then our code would be less awesome.
In addition, we bring in 2 CSS features:
Bootstrap: in this app, we use it just for vanilla UI presentation.
Font-Awesome: we love Font Awesome because it lets us use truetype font characters instead of image-based icons. Pretty sweet!
Overall, we were pretty pleased that we could build a nifty UI with so few dependencies. And with that… on to the UI!
The User Interface
Here’s what we intend the UI to look like:
The UI is pretty straightforward – everything is inside a div tag that is managed by a single controller that we’ll set up in the AngularJS code.
We provide a simple text input for a URL to send to the PubNub channel as well as a button to perform the publish() action.
Our UI consists of a simple list of messages. We iterate over the messages in the controller scope using a trusty ng-repeat. Each message includes the original URL as well as the text analysis including tags, content type, language, and title. For simplicity, we just display the first object detected (hence objects[0]).
And that’s it – a functioning realtime UI in just a handful of code (thanks, AngularJS)!
The AngularJS Code
Now we’re ready to dive into the AngularJS code. It’s not a ton of JavaScript, so this should hopefully be pretty straightforward.
The first lines we encounter set up our application (with a necessary dependency on the PubNub AngularJS service) and a single controller (which we dub MyTextCtrl). Both of these values correspond to the ng-app and ng-controller attributes from the preceding UI code.
Next up, we initialize a bunch of values. First is an array of message objects which starts out empty. After that, we set up the channel as the channel name where we will send and receive realtime structured data messages.
NOTE: make sure this matches the channel specified by your BLOCK configuration and the BLOCK itself!
We initialize the Pubnub object with our PubNub publish and subscribe keys mentioned above, and set a scope variable to make sure the initialization only occurs once.
NOTE: this uses the v3 API syntax.
The next thing we’ll need is a realtime message callback called msgCallback; it takes care of all the realtime messages we need to handle from PubNub. In our case, we have only one scenario – an incoming message containing text fragments with sentiment analysis. The concat() operation should be in a $scope.$apply() call so that AngularJS gets the idea that a change came in asynchronously.
The publish() function takes the contents of the text input, publishes it as a structured data object to the PubNub channel, and resets the text box to empty.
Finally, in the main body of the controller, we subscribe() to the message channel (using the JavaScript v3 API syntax) and bind the events to the callback function we just created.
We mustn’t forget close out the HTML tags accordingly.
Not too shabby for about 74 lines of HTML & JavaScript!
Additional Features
There are a couple other endpoints worth mentioning in the Diffbot API.
You can find detailed API documentation here.
Analyze: analysis of web pages.
Article: detailed analysis of web articles.
Discussion : detailed analysis of forums and discussion pages.
Image: detailed analysis of web-based images.
Product: detailed analysis of web-based e-commerce product pages.
Video (Beta): analysis of web-based video files.
All in all, we found it pretty easy to get started with content analysis using the API, and we look forward to using more of the deeper analysis features!
Conclusion
Thank you so much for joining us in the Content Analysis article of our BLOCKS and web services series! Hopefully it’s been a useful experience learning about content-enabled technologies. In future articles, we’ll dive deeper into additional web service APIs and use cases for other nifty services in realtime web applications.
Stay tuned, and please reach out anytime if you feel especially inspired or need any help!",,https://www.pubnub.com/blog/2017-01-30-realtime-text-content-analysis-and-categorization-with-diffbot/,,
"Tencent, Felicis Ventures lead $10m Series A investment in AI startup Diffbot",d2016-02-12T00:00,,DealStreetAsia,"Artificial intelligence,Series A round,Diffbot,Startup company,Investment,Tencent,Venture capital,Cloud computing,Andy Bechtolsheim,Silicon Valley","Chinese Internet major Tencent has partnered with Silicon Valley venture capital (VC) firm Felicis Ventures to lead a $10-million Series A round into artificial intelligence (AI) technology venture Diffbot. Other participating investors were Amplify Ventures, Valor Capital Group and a host of individual investors.
For Tencent, it is the third publically reported deal outside of China as of February 2016 and reflects a general trend of investing abroad. Incidentally, it synchronises with the theme of Tencent investing in cloud computing, as well as investments aimed at establishing international market leadership in the international mobile games sector.
This deal could be characterised as strategic given that it is complementary to many investments that Tencent made in 2015, such as Practo, Futu5 and MedLinker. It also reflects a larger trend of AI investment by Internet firms with a global footprint, synchronising with larger patterns of VC investment into AI.
Some of the technology investors participating in the investment syndicate included: Andy Bechtolsheim, the co-founder of Sun Microsystems and the first investor in Google; Bill Lee, an early investor in SpaceX and Tesla; and Georges Harik, one of Google’s first 10 employees and an AI expert.
Diffbot, which has raised $12.5 million in equity financing to date, was founded by Michael Tung in 2010, Based in Silicon Valley, it is a venture that has positioned itself as a data company, offering a data product with AI capabilities that can conduct data mining on the web. It offers turnkey APIs, automatic web-crawling, and bulk data capabilities. According to the firm, it services clients such as Cisco, Adobe, Microsoft, eBay, and Yandex.
Its website claims: “Using AI, computer vision, machine learning and natural language processing, Diffbot provides software developers with tools to extract and understand objects from any web page.”
Also Read: Nine tech trends shaping our lives and work
About Diffbot
Using its AI technology, the company claims to analyse and synthesise unstructured data for clients, in order to collect and collate more data about people, places and other phenomena and artefacts online. It offers automatic application programming interfaces (APIs) that automate content extraction from articles, products, discussions, images and the online content, retrieving “clean, structured data” without the need for explicit programming.
In relation to the investment, Aydin Senkut, founder and managing director of Felicis Ventures, observed: “Structured data will be to the AI revolution and intelligent applications what oil was to the second industrial revolution and the combustion engine. To understand Diffbot’s value in that context: they’ve invented a drill and pump while everyone else is digging with spoons and straws.”
Senkut added, “The breadth, depth, and accuracy of the data they’re accumulating may become the single most valuable resource in tech. That said, what truly makes them an amazing investment is that, unlike many ‘unicorns,’ Diffbot has identified a high-margin, high-leverage business model in AI that is only limited in its data collection capabilities by the size of an easily expanded data center.”
Another product, Crawlbot, claims to enable claims to enable the structuring and consolidation of a website’s content into a single, searchable index. In essence, it develops machine learning and computer vision algorithms and public APIs for web scraping.
Commenting on the investment, Tung explained: “Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon. We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight.”
Tung added, “We’re fortunate to have attracted investors that have high intellectual fit with our vision, understand the long-term value of developing this technology, and bring their track record of identifying the most transformative technologies. Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon.”
He continued: “We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight.”
Also Read: Former BMW exec Carsten Breitfeld is new CEO of Hon Hai, Tencent electric car venture
Cloud computing, machine learning & AI
A December 2015 article in CloudTech observed: “Cloud computing solved the two biggest hurdles for AI: abundant, low cost computing and a way to leverage massive volumes of data. However, a number of challenges remain. Chief among those challenges is the one affecting the whole industry: skills. While open source libraries make it easy to get started, for genuinely powerful AI you need actual data scientists.”
It added, “People with strong programming backgrounds, a deep understanding of mathematics and statistics, as well as business domain knowledge. Needless to say, those people are rare. The other challenges will mainly be around the data. Most modern data is inherently unstructured – it’s geographic data, sensor data, and social data.”
Diffbot’s challenge will be maintaining a small, focused, cloud-based algorithm as its AI product, while establishing market leadership in its niche of crawling and indexing online content. With major firms like IBM developing and offering the IBM Watson as a service and Google deepening its involvement in AI development,
Artificial intelligence is intimately linked with machine learning, which sees programmers invested time in developing software that can learn about the world. Google has been investing deeply in AI ad machine learning, amid intense competition with Apple and Microsoft.
Amongst the largest corporate sponsors of AI, Google is increasingly relying on machine learning and AI for searching and indexing online content such as videos, speech, translation and, recently, search. One instance of this, according to a Bloomberg report, is an AI system termed RankBrain.
Using AI to embed vast amounts of written language into mathematical entities termed vectors which computers can understand and process, it enables high-level processing of search queries. This investment by Tencent in Diffbot comes at a time as AI is poised to potentially disrupt financial markets and services as well.
Also Read: Following Alibaba, Tencent’s footsteps China’s Baidu, CITIC in JV for Internet bank
Apple buys artificial intelligence startup Emotient that reads emotions by analysing facial expressions
Tesla CEO Musk, other tech titans back non-profit artificial intelligence startup, OpenAI, with $1b funding
Google picks minority stake in China’s artificial intelligence start-up Mobvoi",,http://www.dealstreetasia.com/stories/30543-30543/,,
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729; Diffbot/0.1; +http://www.diffbot.com),,,parable.com,,,,http://www.parable.com/i.Fierce-Convictions-The-Extraordinary-Life-of-Hannah-More-Poe.9781400206254,,
Diffbot primeste 2 milioane$ sa agregheze continut online,d2012-06-01T13:30,,PLAYTECH.ro,,"Diffbot este o platforma care citeste orice pagina de internet si extrage informatii utile, clare si structurate din ea. Desi ar putea parea relativ simplu, Sun, Twitter, Facebook, AOL, Earthlink – cu totii sunt incantati de potentialul companiei, motiv pentru care au investit in aceasta saptamana nu mai putin de 2 milioane dolari in Diffbot.
In practica, pe langa faptul ca extrage continut din diverse pagini, Diffbot agregheaza respectivul volum de date intr-un API. Acesta din urma, in secunda urmatoare poate fi integrat in diverse aplicatii. AOL, de exemplu, asa cum mentionam mai sus, a apelat la Diffbot, fiind si cel mai mare cleint al lor. AOL are mai multe site-uri de informare, pe diverse domenii, site-uri folosesc platforme diferite de management de continut, motiv pentru care ar fi ineficient sa creezi de la 0 platforma care sa gregheze tot volumul respectiv de informatii intr-un stream utilizabil ulterior. Diffbot este folosit pentru pentru agregarea din toate site-urile a continutului pe care sa-l foloseasca in revista digitala Editions, pentru iPad.
Desi sistemul inca nu este unul dintre cele mai performante, intelege doar pagini principale cu articole, este imbunatatit in permanenta. Ultima inovatie este intelesul de retete pentru mancare. Astfel, daca vrei de exemplu sa-ti creezi o aplicatie pentru Android sau iOS, este suficient sa-ti alegi pagini de internet cu retete, iar continutul acestora este agregat intr-un API care ajunge direct in aplicatie, impartit cum se cuvine, in ingrediente si instructiuni.
Momentan, compania are doua servicii mari si late: poate scana URL-uri trimise de un utilizator si poate monitoriza o pagina si sa alerteze utilizatorul in momentul in care au aparut modificari pe respectiva pagina. Al doilea serviciu este o unealta perfecta de monitorizare a concurentei pentru unele companii.
In ceea ce priveste sistemul de plata, primele 10.000 de apelari de la un utilizator sau companie sunt gratuite, urmand ca restul sa fie taxate proportional.
Proaspata finantare intrata in vistieria companiei, aceasta va fi folosita pentru upgrade-uri de servere si pentru angajarea de noi experti care sa invete sistemele sa desluseasca cat mai bine continutul.",,https://playtech.ro/2012/diffbot-primeste-2-milioane-sa-agregheze-continut-online/,,
The Largest Database of Human Knowledge? Diffbot Knowledge Graph,,,IDM.net.au,"Diffbot,Knowledge","AI startup Diffbot has announced the launch of Diffbot Knowledge Graph (DKG): all of the knowledge on the Web, collected and connected into a single, structured source of data, answers, insights, and truth.
Using a sophisticated combination of machine learning, computer vision, and natural language processing, the DKG is a fully autonomous, AI-curated database of more than 1 trillion facts and 10 billion entities. This represents a repository of knowledge that is nearly 500 times larger than the Google Knowledge Graph, and growing every day.
Diffbot claims to be the first company to turn broad-application Artificial Intelligence into a profitable business, powering applications for customers including Salesforce, Cisco, eBay, Yandex, and more.
Far from a theoretical research project in search of a business application, Artificial Intelligence is the backbone of Diffbot and the company uses state of the art AI methods to deploy profitable products at scale while also furthering the field by funding extensive research.
The company says in contrast to other solutions marketed as Knowledge Graphs, the DKG is:
Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labour.
Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models.Web-wide, regardless of originating language.
Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as English.
Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.
Starting today, any business that wants instant access to all of the world’s knowledge can simply sign up for the DKG and turn the entire Web into their personal database for business intelligence across:
People: skills, employment history, education, social profiles
Companies: rich profiles of companies and the workforce globally, from Fortune 500 to SMB’s
Locations: mapping data, addresses, business types, zoning information
Articles: Every news article, dateline, byline from anywhere on the Web, in any language
Products: pricing , specifications, and, reviews for every SKU across major ecommerce engines and individual retailers
Discussions: chats, social sharing, and conversations everywhere from article comments to web forums like Reddit
Images: billions of images on the web organized using image recognition and meta data collection
“A Web-wide, comprehensive, and interconnected Knowledge Graph has the power to transform how enterprises do business. Google’s ‘Knowledge Graph’ is little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between them and built solely to serve advertisers,” said Mike Tung, founder and CEO of Diffbot. “What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
DKG data can be integrated via API into any internal business process or application, from business intelligence and analytics to marketing campaigns and CRM systems. Users can also create custom queries using Diffbot’s DQL syntax. Users simply enter a query and the DKG instantly generates a comprehensive set of results with every single item on the internet that relates to it, with links to all existing connections between those results. Results can be viewed in a list, map or table layout, with the ability to easily expand or refine results based on connections captured by the Knowledge Graph.
“Simply put, Diffbot is using the power of AI on a scale we’ve never seen before,” said Aydin Senkut, founder and managing director of Felicis Ventures, one of Diffbot’s investors. “It’s the first profitable AI company on record, they are the ‘secret ingredient’ powering applications from many of the largest companies in tech, and the launch of the Knowledge Graph is going to further elevate Diffbot’s status as a clear leader in the space.”
Companies interested in accessing the Diffbot Knowledge Graph can contact Diffbot at sales@diffbot.com or visit www.diffbot.com/knowledge-graph",,http://www.idm.net.au/blog/0012170-largest-database-human-knowledge-diffbot-knowledge-graph,,
"Tencent, Felicis Ventures lead $10m Series A investment in AI startup Diffbot",d2016-02-12T00:00,,DealStreetAsia,,"Chinese Internet major Tencent has partnered with Silicon Valley venture capital (VC) firm Felicis Ventures to lead a $10-million Series A round into artificial intelligence (AI) technology venture Diffbot. Other participating investors were Amplify Ventures, Valor Capital Group and a host of individual investors.
For Tencent, it is the third publically reported deal outside of China as of February 2016 and reflects a general trend of investing abroad. Incidentally, it synchronises with the theme of Tencent investing in cloud computing, as well as investments aimed at establishing international market leadership in the international mobile games sector.
This deal could be characterised as strategic given that it is complementary to many investments that Tencent made in 2015, such as Practo, Futu5 and MedLinker. It also reflects a larger trend of AI investment by Internet firms with a global footprint, synchronising with larger patterns of VC investment into AI.
Some of the technology investors participating in the investment syndicate included: Andy Bechtolsheim, the co-founder of Sun Microsystems and the first investor in Google; Bill Lee, an early investor in SpaceX and Tesla; and Georges Harik, one of Google’s first 10 employees and an AI expert.
Diffbot, which has raised $12.5 million in equity financing to date, was founded by Michael Tung in 2010, Based in Silicon Valley, it is a venture that has positioned itself as a data company, offering a data product with AI capabilities that can conduct data mining on the web. It offers turnkey APIs, automatic web-crawling, and bulk data capabilities. According to the firm, it services clients such as Cisco, Adobe, Microsoft, eBay, and Yandex.
Its website claims: “Using AI, computer vision, machine learning and natural language processing, Diffbot provides software developers with tools to extract and understand objects from any web page.”
Also Read: Nine tech trends shaping our lives and work
About Diffbot
Using its AI technology, the company claims to analyse and synthesise unstructured data for clients, in order to collect and collate more data about people, places and other phenomena and artefacts online. It offers automatic application programming interfaces (APIs) that automate content extraction from articles, products, discussions, images and the online content, retrieving “clean, structured data” without the need for explicit programming.
In relation to the investment, Aydin Senkut, founder and managing director of Felicis Ventures, observed: “Structured data will be to the AI revolution and intelligent applications what oil was to the second industrial revolution and the combustion engine. To understand Diffbot’s value in that context: they’ve invented a drill and pump while everyone else is digging with spoons and straws.”
Senkut added, “The breadth, depth, and accuracy of the data they’re accumulating may become the single most valuable resource in tech. That said, what truly makes them an amazing investment is that, unlike many ‘unicorns,’ Diffbot has identified a high-margin, high-leverage business model in AI that is only limited in its data collection capabilities by the size of an easily expanded data center.”
Another product, Crawlbot, claims to enable claims to enable the structuring and consolidation of a website’s content into a single, searchable index. In essence, it develops machine learning and computer vision algorithms and public APIs for web scraping.
Commenting on the investment, Tung explained: “Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon. We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight.”
Tung added, “We’re fortunate to have attracted investors that have high intellectual fit with our vision, understand the long-term value of developing this technology, and bring their track record of identifying the most transformative technologies. Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon.”
He continued: “We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight.”
Also Read: Former BMW exec Carsten Breitfeld is new CEO of Hon Hai, Tencent electric car venture
Cloud computing, machine learning & AI
A December 2015 article in CloudTech observed: “Cloud computing solved the two biggest hurdles for AI: abundant, low cost computing and a way to leverage massive volumes of data. However, a number of challenges remain. Chief among those challenges is the one affecting the whole industry: skills. While open source libraries make it easy to get started, for genuinely powerful AI you need actual data scientists.”
It added, “People with strong programming backgrounds, a deep understanding of mathematics and statistics, as well as business domain knowledge. Needless to say, those people are rare. The other challenges will mainly be around the data. Most modern data is inherently unstructured – it’s geographic data, sensor data, and social data.”
Diffbot’s challenge will be maintaining a small, focused, cloud-based algorithm as its AI product, while establishing market leadership in its niche of crawling and indexing online content. With major firms like IBM developing and offering the IBM Watson as a service and Google deepening its involvement in AI development,
Artificial intelligence is intimately linked with machine learning, which sees programmers invested time in developing software that can learn about the world. Google has been investing deeply in AI ad machine learning, amid intense competition with Apple and Microsoft.
Amongst the largest corporate sponsors of AI, Google is increasingly relying on machine learning and AI for searching and indexing online content such as videos, speech, translation and, recently, search. One instance of this, according to a Bloomberg report, is an AI system termed RankBrain.
Using AI to embed vast amounts of written language into mathematical entities termed vectors which computers can understand and process, it enables high-level processing of search queries. This investment by Tencent in Diffbot comes at a time as AI is poised to potentially disrupt financial markets and services as well.
Also Read: Following Alibaba, Tencent’s footsteps China’s Baidu, CITIC in JV for Internet bank
Apple buys artificial intelligence startup Emotient that reads emotions by analysing facial expressions
Tesla CEO Musk, other tech titans back non-profit artificial intelligence startup, OpenAI, with $1b funding
Google picks minority stake in China’s artificial intelligence start-up Mobvoi",,https://www.dealstreetasia.com/stories/30543-30543/,,
????Diffbot:?Web????????,?????????,d2013-08-30T15:23,,CSDN.net,"Facebook,Instapaper,????,???,???,?????,??,??,??,??????","Diffbot???????????,???????????????????????Web??,??????????“????”????Diffbot?????API,????????????????????,?????????????,????????????
Diffbot???????“?????”?????????????(??????????),??????????????????Diffbot?????????Mike Tung??:“????????????????,????????????????????”
Diffbot?API??????????????????,????????????????????,?????????????????SKU???????????(?????CloudTimes)
Diffbot????Web?????????——????,??,??,????????Diffbot ????????,????????????????????????API???API,????API?
Diffbot?????Instapaper(???????????????),?????????????????,????????????????
????,???????????????,?????????????????????Web???????????,????????????????????,??????????“???”?,?????????Tung??,Diffbot?????????API??Web???SaaS????????????
Diffbot?????????????,??Andy Bechtolsheim(?????????Google???????,Sun???????)?Sky Dayton(EarthLink?Boingo Wireless????)?Joi Ito(MIT Media?????)? Brad Garlinghouse(????????)??Jonathan Heiliger(Facebook???????)?
??????Palo Alto?Diffbot???2008?,???????????Mike Tung?Leith Abdulla??????????(?/??,??/??)
????:Diffbot aims to convert the web into one big database, one page at a time
Cloud Edge:2013???“???”????
??
????
????
CEO/CTO
????
????/??
1.
HStreaming
2011?
Jana Uhlig
$ 1M (B)
??Hadoop??
2.
CitusData
2012?
Matt Ocko
CitusDB
3.
Backblaze
2009?
Gleb Budman
??????
4.
Kickboard
2009?
Jennifer Medberry
$2.8M(A)
Kickboard(????)
5.
Elasticsearch
2012?
Shay Banon
$24 M(B)
??????
6.
Appcore
2008?
Jeff Tegethoff
$6M (B)
??????IaaS??
7.
Pertino
2011?
Craig Elliott
$20 M(B)
??????(SDN)
8.
SwiftStack
2011?
Joe Arnold
$6.1M(A)
??????
9.
Spiral Genetics
2009?
Adina Mangubat
$3M(A)
DNA????????
10.
DNNResearch
2012?
Geoffrey Hinton
????
11.
AppNeta
2011?
Jim Melvin
$16M(C)
??????(APM)
12.
Concurrent
2008?
Chris K. Wensel
$4M(A)
Java?????
13.
AirWatch
2003?
John Marshall
$200M(A)
??????
14.
Pluribus
2012?
Robert Drost
$44M(C)
?????
15.
Bina Technology
2006?
Narges Bani Asadi
$6.5M(B)
??????
16.
Sociocast
2010?
Albert Azout
$1M(B)
??????
17.
ParElastic
2010?
Ken Rugg
$5.7M(A)
????????
18.
Optimizely
2009?
Dan Siroker
$28M(A)
A/B ????
19.
Instart Logic
2010?
Manav Mital
$17M(B)
????
20.
CloudFlare
2010?
Matthew Prince
$20M(B)
????????
21.
Ionic Security
2012?
Ted Schlein
$9.4M(A)
???
22.
MemSQL
2011?
Eric Frenkiel
$5M(A)
MemSQL
23.
Qubole
2012?
Ashish Thusoo
$7M(A)
AWS??Hive??
24.
Clustrix
2005?
Robin Purohit
$16.5M(C)
Clustrix Sierra
25.
Cloudant
2008?
Derek Schoettle
$12M(B)
DBaaS
26.
DataTorrent
2012?
Phu Hoang
$8M(B)
??????
27.
WibiData
2010?
Christophe Bisciglia
$15M(B)
Hadoop????
28.
Dataminr
2009?
Ted Bailey
$30M(C)
??????
29.
PlumGrid
2011?
Awais Nemat
$10.7M(A)
??????(SDN)
30.
Meldium
2012?
Anton Vaynshtok
$1 M(A)
??????
31.
Myrrix
2012?
Sean Owen
???,????
32.
Alpine data Labs
2010?
Joe Otto
$7.5 M(A)
???????
33.
Context Relevant
2012?
Stephen Purpura
$7 M(A)
???????
34.
Datameer
2009?
Stefan Groschupf
$9.2M(B)
???????
35.
Skytree
2012?
Alexander Gray
????
36.
Wise.io
2012?
Joshua Bloom
????
37.
Treasure Data
2012?
Hiro Yoshikawa
$10 M(?)
???????
38.
NimbusBase
2012?
Alex Volodarsky
??API
39.
Zimory
2007?
Ruediger Baumann
$20 M(B)
IaaS ??????
40.
DataStax
2010?
Billy Bosworth
$45 M(B)
???NoSQL???
41.
ZestFinance
2012?
Douglas Merrill
$20 M(C)
????+?????
42.
InfoChimps
2009?
Jim Kasksade
$5M(?)
??????????
43.
DigitalOcean
2012?
Ben Uretsky
$3.2M
IaaS??,?????
44.
SOASTA
2006?
Tom Lounibos
$30M
?????
45.
Birst
2004?
Brad Peters
$38M
?????????
46.
Hortonworks
2011?
Rob Bearden
$98M (?)
Hadoop????
47.
Parse.ly
2012?
Sachin Kamdar
$5M (A)
??????
48.
Diffbot
2008?
Mike Tung
$2M (A)
????,????
??:2013?8?30???,?????......
???CSDN????,????????,???????market#csdn.net(#??@)",,http://www.csdn.net/article/2013-08-30/2816771-Diffbot,,
Diffbot Enables Software Applications To Look At the Web With A Human Set Of Eyes,d2011-08-25T00:00,,Diffbot,"Diffbot,Eye,Senior Director,Barack Obama,Mike Tung","Production API Lets Developers Use Visual Learning Technology to Create New Applications Tying Web Content to Context, Structure, and Action
PALO ALTO, Calif.-- Diffbot Corp, developers of Diffbot visual content and layout recognition (VCLR™) technology, today released the production build of their new set of visual learning APIs. Diffbot, a learning robot that offers visual understanding of Web content, provides APIs enabling software developers to easily create applications that use computer vision algorithms to understand the layout and meaning of Web content, and look at the Web similar to the way people do. Developers use Diffbot to create rich user experiences around content simply by submitting a link to the Diffbot API.
“I first came up with the idea in my college dorm where I needed to be instantly notified when a new assignment was posted on class websites. I built Diffbot to stay ahead of my classmates and quickly realized the potential commercial applications.”
The API’s computer vision technology perceives context and visual layout similar to the way people do; understanding common page layouts (like headlines, bylines and articles), contextual keywords, and content changes buried deep within pages enables applications to follow websites, observe when changes occur, and display that content in a variety of media. Diffbot is the foundation of a new way to develop around and consume Web-based content.
“At its core, Diffbot is an enabling technology, targeted to developers that aggregate content or have their own need to offer personalization,” said Diffbot Co-Founder Mike Tung. “I first came up with the idea in my college dorm where I needed to be instantly notified when a new assignment was posted on class websites. I built Diffbot to stay ahead of my classmates and quickly realized the potential commercial applications.”
The Diffbot technology currently consists of two types of APIs:
1. On-Demand - The Diffbot On-Demand API is divided into page types: “Frontpage” and “Article.” The Frontpage API is designed for analyzing home pages and index pages using common layout markers (like headlines, bylines, images, articles, ads, and more), while the Article API is used to extract clean article text, pictures, and tags from news article webpages.
2. Follow - The Follow API is used to follow the changes or updates made to any webpage. Diffbot automatically determines the part of the page that the developer likely wants to follow, extracts metadata such as the title, image and text summary, scores, and segments the page into structurally meaningful sections.
Diffbot allows developers to build applications that can:
Extract and analyze information displayed on an article page
Understand key words and phrases in the context of the larger article and generate tags to allow developers to categorize, sort and personalize content
Analyze homepages and index pages to understand when content has been changed
Generate an RSS feed enabling an application to follow anything on the Internet
Display or use the raw components of an article page in any manner
Convert any webpage into a mobile format
Create snapshots of sites derived from embedded links
One well-known application using the Diffbot API is Editions by AOL. The recently-launched news magazine for the iPad who’s tagline reads: “the magazine that reads you,” uses Diffbot to identify and extract relevant content tags from news sources on the Web, helping enable the personalization aspect of the app. Diffbot supplies natural language processing to cross-reference against Wikipedia, determine relevance by context and deliver keyword tags. For example, Diffbot can determine that an article about Barak Obama is related to “politics” even though the word doesn’t appear in the article, or that an article about a new computer is about Apple the technology company, and not apple the fruit.
“We are impressed with the Diffbot team and look forward to collaborating with them on future releases of Editions,” said Sol Lipman, Senior Director of AOL’s Mobile First Division. “The API is easy-to-use and understands webpage structure and content better than any other technology we’ve seen.”
About Diffbot:
Diffbot is a learning robot that offers visual understanding of Web content. Their VCLR technology provides developers automated, visual understanding of Web content that is as easy to implement into applications as submitting a URL. The company offers an innovative API that enables developers to easily create applications that apply computer vision algorithms for the purpose of extracting information and understanding the visual layouts of various webpages. Simply provide a link to a webpage and Diffbot analyzes the content. Diffbot lets any application look at Web content with a human set of eyes; rather than simply seeing text, links, and pictures, Diffbot interprets layout, contextual keywords, common sections, and changes to content in a way that lets developers easily break out that content, organize it and present it to users for direct action. With hundreds of developers currently using their API, Diffbot is the foundation of a new way to develop around and consume Web-based content.
Based in Palo Alto, Diffbot is part of the Stanford University’s accelerator StartX, a non-profit organization whose mission is to provide an entrepreneurial education to startups founded by Stanford students. StartX provides peer community, mentorship, real-time and customized educational content and infrastructure resources.
For more information visit: http://www.diffbot.com
Useful Links:
Diffbot website: http://www.diffbot.com
Editions website: http://www.editions.com
StartX website: http://startx.stanford.edu",-0.30168,https://www.diffbot.com/company/news/20110825.jsp,,
The Diffbot startup Plan,,,Diffbot,Diffbot,"You will be billed $299 monthly for 250,000 API calls. Additional calls will be billed at $0.001 per call. You may cancel at any time.
Please provide the following information to generate a token:",,https://www.diffbot.com/plans/startup,,
Diffbot Global Index,,,Diffbot,"Diffbot,World Wide Web,Application programming interface","The Diffbot Global Index lets you search the web like a database. Our Automatic APIs regularly scour the web, indexing articles, comments, images, products and more-all available for realtime search across any structured field.
Search for...
News
Tens of thousands of news sources are monitored constantly for new content. Search millions of articles and filter by author, date, topic, region, sentiment, language and more.
Comments
Article and blog post comments are automatically extracted and structured alongside article content. Find comments by author, search by topic or simply query for all comments matching your specific term.
Images and Video
The Global Index indexes all images and videos featured in articles, allowing you to filter by caption, source, image tags or image similarity.
More coming soon
Discussions, products, videos and more page types are on their way.
Features",,https://www.diffbot.com/products/globalindex,,
Diffbot Developer Dashboard,,,Diffbot,,,,https://www.diffbot.com/dev,,
Semantria and Diffbot: A Partnership That Makes a Big Diff,d2014-10-08T00:00,,Lexalytics,"San Francisco,Diffbot,Semantria,A Partnership Inc,diff","Big News!
Semantria and Diffbot, a San Francisco-based start-up that specializes in intelligent web page extraction, have partnered up. The result of this partnership is pretty spectacular.
What’s the Diff?
Say your business, or maybe a competitor, has just released a new product.
Naturally, you want to know what the buzz is: What are customers saying about it? How do they feel about it? Why?
But before you can start analyzing the data, you will need to amass conversations across hundreds, if not thousands of websites. These webpages could be Twitter conversations, blog posts, tech/movie review websites, or any other number of websites brimming with text.
Doing this manually would be a tedious, if not impossible, task.
Good thing you don’t have to
Diffbot provides APIs that automatically extract important information in a structured output from any website you wish. Or, if you’re feeling ambitious, it can crawl entire domains, recognize the type of website, and then classify the content accordingly.
The native integration of Diffbot APIs in Semantria means you now have a comprehensive service for all your web-crawling needs, from extraction to analysis.
Semantria and Diffbot: Partnering to make your life easier
Diffbot adds powerful new functionality to Semantria. You can now automatically search the internet for relevant webpages, reorganize their content into a format Semantria can understand, and then analyze the content revealing the who, the what, the how, and the why…all in one service.
Being able to make your job simpler?
That makes all the Diff.",,https://www.lexalytics.com/lexablog/semantria-diffbot-partnership-makes-diff,,
"Brand Monitoring Beyond Twitter: Diffbot ""Discussions"" API Lets Companies and Developers Unearth and Track Millions of Hidden Mentions in Forums, Comments, and Reviews",d2015-03-31T00:00,,Diffbot,"Review,Conversation,Diffbot,Twitter,Debate,Internet forum,Information technology,Brand,Hacker News,Company","Diffbot uses artificial intelligence robot to visually extract the Web’s ""dark matter""—user-generated content currently hidden from search
PALO ALTO-- Diffbot, artificial intelligence startup and creators of visual learning robot technology that lets developers analyze, extract, and enhance Web content, today broke new ground in its quest to bring structure to the unstructured Web. Nearly a quarter of all Internet users participate in online communities and forums, while even more comment on articles or write online reviews. With the release of the company’s new ""Discussions API"", Diffbot is, for the first time ever, providing developers and companies with a tool to unlock the millions of comments made every day online in these formerly hidden corners of the Web. This user-generated content makes up a part of what is known as the ""deep web,"" estimated to be 400 times larger than the ""surface Web"" that is indexed and accessible through traditional search engines. The Discussion API currently supports Facebook Comments, Disqus, Livefyre, Wordpress, Blogger, Intense Debate (owned by Automattic), Kinja, Hacker News, Reddit, and more.
""This is the holy grail of brand monitoring,"" said Mike Tung, CEO of Diffbot. “Traditional media monitoring tools track Twitter and Facebook, or editorial content. However, the substantive conversation about a brand’s products and services by actual customers is happening in the more specialized forums and review sites of the web.”
This new functionality visually analyzes web pages and instantly parses complete comment data, author information, topic analysis, and more into discrete objects. That structured data, which was never before accessible via automation, provides on-demand access allowing developers and companies to build applications that can:
Monitor brands, products or other keywords—to gauge user reaction, gather feedback or monitor sentiment—in the locations where users are actually providing feedback
Make forum and other user-created content mobile friendly for easier consumption on phones, tablets or other devices
Analyze user-created posts to identify trends or sentiment within user communities and among power users
Identify links and other content shared within discussion threads or comment sections to improve product recommendations, insert affiliate linking, and perform other link analysis
Completely process and extract an entire site’s worth of content—whether for migration or other analysis—when paired with Crawlbot, Diffbot’s intelligent spider
""Our new APIs allow developers to treat forums, comment threads and review collections as virtual databases, accessing their data on-the-fly and making this massive component of the web newly usable,"" continued Tung. ""It will even help developers find those, admittedly rare, useful and constructive YouTube comments.""
This new API augments Diffbot’s efforts to structure the data of the entire Web, including its APIs for automatic article, product, image and video extraction; its Analyze API, which immediately determines the “page type” of any unknown link; and its Crawlbot crawling platform for entire-site extraction.
About Diffbot:
Diffbot is a robot that examines the Web using computer vision and natural language processing, and provides developers with robust tools to find, extract and understand the objects from any Web page for use in their applications. Thousands of developers and businesses rely on Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, the mobile web and hundreds of other consumer applications. Customers include Adobe, CBS Interactive, Cisco, eBay, Instapaper, Salesforce, Samsung, StumbleUpon. It is based in Palo Alto, CA.
To learn more visit www.diffbot.com",-0.05231,https://www.diffbot.com/company/news/20150331.jsp,,
Why Diffbot?,,,Diffbot,"Diffbot,Data","We're focused exclusively on getting you better web data. Hundreds of customers make (hundreds of) millions of calls every month. Why?
The Web's Best Content Extractor
Diffbot works without rules or training. There's no better way to extract data from web pages. See how Diffbot stacks up to other content extraction methods:
Identify Pages Automatically
Use the Analyze API to automatically find and extract all products, articles, discussions, videos or images while crawling any site.
Detailed product data
The Product API automatically returns complete product info, including all pricing data, product IDs, brand and full specifications tables.
Clean text and html
Articles, discussion threads, product descriptions and image captions are returned in pure text and sanitized HTML.
Plus...
All APIs execute Javascript so content is parsed like a regular browser.
Works on most non-English pages thanks to visual processing.
Date normalization: Datestamps are normalized to RFC 1123 (HTTP/1.1).
Multipage articles are automatically joined together in a API response.
Entity extraction: automatic tagging identifies major topics and entities within article text.
Fix any issues realtime with the API Toolkit.
Bulk API allows the extraction of hundreds to hundreds-of-thousands of pages.
Access Crawlbot and Bulk job data in full JSON or CSV formats.
Optionally crawl using a diverse array of IP addresses.
Web Extractor Feature Comparison
Diffbot
Alchemy
Boilerpipe
Embedly
Readability
Kimono
Import.io
Mozenda
Article extraction API
Product extraction API
Discussion extraction API
Video extraction API
Image extraction API
Page classifying API
Custom rule creator for custom fields/APIs
Executes Javascript for full page rendering
Automatic tag/entity extraction
Returns normalized HTML
Returns clean plaintext
Author identification
Date extraction and normalization
Works in any language
Multipage article concatenation
Multipage discussion concatenation
Sentiment analysis
Language detection
Article comment extraction
Video extraction from articles
Product review extraction
Product specification table extraction
Link extraction (return all links on a page)
Integrated web crawler
Automatic page classifying while crawling
Crawler API
Repeating crawls
Custom crawling controls/filters
Crawling anonymity / proxying
Bulk API (up to 1M URLs)
Searchable crawl and bulk API data
Fully-hosted SaaS
Open source
Relation extraction
Face detection
Taxonomies/categorization
Ready to get started? Sign up for a free 14-day trial.",,https://www.diffbot.com/benefits/,,
Sign Up for a Diffbot Plan,,,Diffbot,Diffbot,"Contact for pricing »
Try our
Features
Features
Features
Features
Trial
Startup
Plus
Professional
Enterprise
DIY Support
Email Support
Phone Support
Phone Support
Phone Support
Automatic APIs
Automatic APIs
Automatic APIs
Automatic APIs
Automatic APIs
API Toolkit
API Toolkit
API Toolkit
API Toolkit
API Toolkit
Crawlbot
Crawlbot
Crawlbot
Crawlbot
Bulk Processing
Global Index
Global Index
Global Index
Crawl/Bulk Search
Bulk Processing
Bulk Processing
Bulk Processing
14-day storage
Crawl/Bulk Search
Crawl/Bulk Search
Crawl/Bulk Search
30-day storage
30-day storage
Additional storage
Proxy Access
Raw HTML
Raw HTML
ProServ Access
Proxy Access
Proxy Access
ProServ Included
ProServ Included
SLA Options",,https://www.diffbot.com/pricing/,,
Diffbot Raises $10M Series A to Become Leading Arms Dealer in Coming AI Wars,d2015-03-31T00:00,,Diffbot,"Diffbot,Series A round,Application programming interface","Official Releases
February 11, 2016:
July 31, 2013: Diffbot's Revolutionary Product API Automatically Extracts Data from Any Product Web Page
August 16, 2012: What's in a Link? Diffbot's New API Is a Decoder Ring for the Web
May 31, 2012: Technology Veterans Invest $2 Million in Diffbot Visual Robot
August 25, 2011: Diffbot Enables Software Applications To Look At the Web With A Human Set Of Eyes
News Coverage
Tencent: ?????????????????",,https://www.diffbot.com/company/news,,
Diffbot:?????????????????,,???,yangfenzi.com,"????,Yelp,??,??,??????,??,??","Diffbot????????,?“??”??????,???????
????????,?????????,????,??????????????????????,????????????????????????????Diffbot???,????????????,?“??”???????,??????????????????????
??????????Diffbot?30???,????????????????????????API?
Diffbot?????????,????????,?“??”??????,??????,??????????????????
??????????Mike Tung 30?????????????,??Diffbot??????????????????????,????????????????????
“????????,????????Yelp??????,???????Yelp??API??,???Diffbot,????????????”Tung??
?Diffbot????,?????????????????,???????????????
??,?????????????“??????:??????????”?????,??????,????????4???????,????“??”?“??”?“??”?“????”?“??”(??????)?“??”???????,????????????????
Tung?,Diffbot????,??????????????????,??????????????
?????“??”??????,Diffbot?2009?????,?????????????,??2012??????200??????,???????????,????12??
Tung???,Diffbot???????????????,???????????????Diffbot????????????Stanford StartX?????,????????Sun????????(??)???Andreas Bechtolsheim??????
??,Diffbot??????????eBay????????????????,Diffbot???4???????:??14?????,??1??API???????299??,??25??API??,???????????0.001??,????????????“??”?“??API”?,????4999??,??500??API??,???????????0.0009??,????????????
Tung?,Diffbot???????API??????“???”??,??????????,????????????????????
“?????????????,”Tung?,“?????????????????,?????????‘???’??????????”",,https://www.yangfenzi.com/keji/50646.html,,
The Diffbot plus Plan,,,Diffbot,Diffbot,"You will be billed $899 monthly for 1,000,000 API calls. Additional calls will be billed at $0.0009 per call. You may cancel at any time.
Please provide the following information to generate a token:",,https://www.diffbot.com/plans/plus,,
Diffbot’s New API Is a Decoder Ring for the Web,d2012-08-16T00:00,,Diffbot,"San Francisco,Diffbot","""Page Classifier"" developer tool from Diffbot uses visual learning robot to instantly identify page content that lies behind any URL
SAN FRANCISCO-- Hyperlinks are core to the Web, yet with the rise of mobile browsing, social sharing sites and link shortening services, they are becoming increasingly difficult for both humans and machines to decode. Without context, no one knows if a link shared on Twitter is a picture, an article, a product, or a Trojan horse.
This problem becomes infinitely more cumbersome for developers looking to build applications around Web content, as they deal with thousands or even millions of links each day. Today Diffbot, creators of visual learning robot technology that lets developers instantly analyze, extract, and enhance Web content, has opened the doors to a new Beta API to change all of that, dubbed “Page Classifier.”
Building atop the more than 100 million URLs analyzed monthly by its developers, Diffbot has trained its visual robot to categorize the entire web into 20 different page types. The new Page Classifier tool lets developer applications identify and classify the type of page and language behind any URL, understanding instantly whether a given link leads to a product, map, social networking profile or any of the other identified page types. Page Classifier leverages Diffbot’s proprietary computer vision and natural-language processing technologies to correctly recognize more than 90% of the Web.
To showcase the new API’s capabilities, Diffbot released an infographic built from Page Classifier data titled “A Day in the Life of Twitter.” Using the new Page Classifier API, Diffbot analyzed three quarters of a million links shared on Twitter on July 11 and 12, 2012, and categorized the underlying pages shared. Some of the company’s key findings are below:
“Just as you and I can look at a given page and say, ‘hey, that’s a recipe, or a map, or a product,’ Page Classifier does the same thing automatically for any URL.” said Mike Tung, President of Diffbot, “Most of the web fits neatly into our 20 identified page types and now Page Classifier allows developer applications to automatically organize any webpage into one of these types.”
“Our eventual goal is to identify and extract the important objects from all pages across the Web, things like the reviews about a product, or the author and content of an article,” continued Mr. Tung. “To reach that goal, we first need to teach Diffbot to recognize what kind of page it’s looking at. Page Classifier is the foundation that will let us identify the important pieces of the Web no matter what kind of page we’re dealing with.”
Diffbot’s Page Classifier technology is already being used by launch partner Springpad, a social bookmarking application that automatically organizes links users have stored from around the Web and enhances that content with useful associated links and offers to save users time and money.
""Springpad's goal is to go beyond clipping a link to help our users save and access what's most useful to them—be it a product, recipe, movie, book or restaurant, said Jeff Chow, CEO of Springpad. “Diffbot's Page Classifier is a plug and play solution that's greatly helped us to improve our categorization and user experience.""
About Diffbot:
Diffbot looks at Web content with a human set of eyes. It is a robot that examines the Web using artificial intelligence, computer vision, machine learning and natural language processing, and provides software developers with tools to find, extract and understand objects from any Web page for use in their applications. Thousands of developers use Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, mobile web and hundreds of other consumer applications. Diffbot is based in Palo Alto, CA.
To learn more visit www.diffbot.com",,https://www.diffbot.com/company/news/20120816.jsp,,
Diffbot on BBC's Click,,,Diffbot,,"Diffbot was featured on the BBC's Click program(me) on June 27, 2015 after a visit to Diffbot HQ earlier that month. See the full episode.",,https://www.diffbot.com/company/news/bbc_click.jsp,,
The Diffbot trial Plan,,,Diffbot,,"10,000 free API calls included. Your trial will expire in two weeks.
Please provide the following information to generate a token:",,https://www.diffbot.com/plans/trial,,
Diffbot Raises $10M Series A to Become Leading Arms Dealer in Coming AI Wars,d2016-02-11T00:00,Diffbot via Businesswire via SPi World News,SPi World News,"Diffbot,Business Wire","Today Diffbot announced that the company secured 3610 million in series A funding to build an AI that can autonomously synthesize the worlds largest database of knowledge Known as the Global Index Diffbots AI- synthes...
More news and information about Diffbot
Published By:
Business Wire: 16:00 GMT Thursday 11th February 2016
Published: 2016-02-11T16:00:00.",,http://www.sectorpublishingintelligence.co.uk/news/1360172/diffbot+raises+3610m+series+a+to+become+leading+arms+dealer+in+coming+ai+wars,,
Technology Veterans Invest $2 Million in Diffbot Visual Robot,d2012-05-31T00:00,,Diffbot,"Andy Bechtolsheim,Sun Microsystems,Diffbot,Brad Garlinghouse,Sky Dayton,Veteran,Matrix Partners,Robot,Technology,MIT Media Lab","Funding will be used to expand team and infrastructure, with new investors and advisors onboard to build Diffbot for scale
SAN FRANCISCO-- Diffbot, creators of visual learning robot technology that lets developers analyze, extract, and enhance Web content, has secured a $2 million investment from technology veterans, including Sky Dayton, founder of EarthLink; Andy Bechtolsheim, co-founder of Sun Microsystems; Joi Ito, Director of the MIT Media Lab; Brad Garlinghouse, CEO of YouSendIt, and other top executives and founders from Facebook, Twitter and Yahoo, with participation from Matrix Partners.
“Our goal with Diffbot is to understand every corner of the Web, and make every bit of it accessible for developers trying to create new, rich applications and experiences”
Diffbot is a new form of visual-based content extraction technology that views and understands Web content the same way human beings do. The technology identifies and extracts the important objects on any Web page using artificial intelligence, computer vision, machine learning and natural language processing. Diffbot’s APIs give application developers a way to instantly utilize data from any Web page in their own applications, effectively turning the entire Web into a usable database. Diffbot is now processing 100 million API calls per month on behalf of its customers, who are using it for Web site mobilization, content management system migration, tag generation, article grouping/clustering and a host of other functions.
“Diffbot is an incredibly sophisticated tool for developers to rapidly build compelling applications around Web content,” said Sky Dayton, founder of EarthLink and Boingo, and investor in Diffbot. “The more developers use Diffbot, the more it learns about and adds structure to data on the Web. This technology is becoming the basis for a new kind of Web experience enhanced by machine interpretation of content.”
Diffbot has categorized the Web into approximately 20 different page types that can be visually analyzed using layout and contextual cues, including everything from product and review pages to social networking profiles and recipes. This visual-based processing allows Diffbot to instantly understand and extract the content on any page, in any language. To date the company has released developer APIs for two of the most commonly consumed page types, Front Pages and Articles. The Front Page API is designed for analyzing home and index pages using common layout markers (headlines, bylines, images, articles, ads and more), while the Article API is used to extract clean article text, related images and videos and generate unique cross-referenced tags from news and blog Web pages.
“Our goal with Diffbot is to understand every corner of the Web, and make every bit of it accessible for developers trying to create new, rich applications and experiences,” said Michael Tung, Diffbot Founder and CEO. “This investment from such a prestigious group of technologists lets us accelerate towards that goal with new hires and expanded resources.”
Mr. Tung continued to say: “More than that, we’re receiving a huge vote of confidence from veterans who have built massive companies and understand the fine points of building for scale, maintaining maximum uptime and delivering the absolute highest standards of service. These are some of the best advisors in the industry to help bring our disruptive technology to every developer in the world, whether they are working at a Fortune 500 company or building the next one out of their garage.”
About Diffbot:
Diffbot looks at Web content with a human set of eyes. It is a robot that examines the Web using artificial intelligence, computer vision, machine learning and natural language processing, and provides software developers with tools to find, extract and understand objects from any Web page for use in their applications. Thousands of developers use Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, mobile web and hundreds of other consumer applications. Diffbot is based in Palo Alto, CA.
To learn more visit www.diffbot.com",0.12061,https://www.diffbot.com/company/news/20120531.jsp,,
The Diffbot professional Plan,,,Diffbot,Diffbot,"You will be billed $3999 monthly for 5,000,000 API calls. Additional calls will be billed at $0.0008 per call. You may cancel at any time.
Please provide the following information to generate a token:",,https://www.diffbot.com/plans/professional,,
Diffbot Raises $10M Series A to Become Leading Arms Dealer in Coming AI Wars,d2016-02-11T00:00,,Diffbot,"AI takeover,Georges Harik,Andy Bechtolsheim,Artificial intelligence,Series A round,Felicis Ventures,Sun Microsystems,Diffbot,Amplify Partners,Aydin Senkut","All-star investors back team of 12 world-class AI engineers synthesizing the world's largest database of structured information, providing ""knowledge-as-a-service"" to tech giants
PALO ALTO—Today, Diffbot announced that the company secured $10 million in series A funding to build an AI that can autonomously synthesize the world’s largest database of knowledge. Known as the Global Index, Diffbot’s AI-synthesized database surpassed the size of Google’s own Knowledge Graph by using a combination of deep learning, computer vision, and natural language processing with no human oversight. Diffbot is already one of the only profitable Artificial Intelligence companies in the world, giving business applications human-level accuracy when accessing the mountain of unstructured data across the Web, and powering applications for some of the largest companies in the tech industry including Cisco, Adobe, Microsoft, eBay, Yandex, and hundreds more.
The company’s $10M series A was led by tech giant Tencent and Felicis Ventures, with a syndicate of legendary tech investors including: Andy Bechtolsheim (co-founder of Sun Microsystems and the first investor in Google), Amplify Partners, Valor Capital, Bill Lee (an early investor in SpaceX and Tesla), and Georges Harik (one of Google's first 10 employees and artificial intelligence expert).
""Structured data will be to the AI revolution and intelligent applications what oil was to the second industrial revolution and the combustion engine,"" said Aydin Senkut, Felicis Ventures. ""To understand Diffbot’s value in that context: they’ve invented a drill and pump while everyone else is digging with spoons and straws.""
Senkut continued, ""The breadth, depth, and accuracy of the data they’re accumulating may become the single most valuable resource in tech. That said, what truly makes them an amazing investment is that, unlike many ‘unicorns,’ Diffbot has identified a high-margin, high-leverage business model in AI that is only limited in its data collection capabilities by the size of an easily expanded data center.""
Diffbot offers businesses a host of turnkey APIs, automatic web-crawling and bulk data processing capabilities, to allow applications to leverage previously unstructured Web data. Because Diffbot is one of the few AI startups that has developed its technology to human-level accuracy, it replaces entire human teams and significantly reduces time and expenses for large-scale data operations.
Diffbot technology visually recognizes, reads, understands, and monitors Web pages and components including product pages, news articles, discussions/comments/forums, videos, pictures, and more. Each element of the Web page is extracted, organized, tagged, cross-referenced, and stored as an ""object"" in the Global Index. To date, the Global Index contains more than 1.2B objects, and is adding roughly 10 million objects per day.
""We're fortunate to have attracted investors that have high intellectual fit with our vision, understand the long-term value of developing this technology, and bring their track record of identifying the most transformative technologies. Early-stage technology companies that are attacking the technical frontier of what's possible need steady leadership and a long-term horizon. We've developed a business model for AI that works and I'm excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight,"" said Mike Tung, Founding CEO of Diffbot.
Over the past five years Diffbot has assembled a team of 12 AI engineers hand-picked from around the world, a custom data center with proprietary hardware, an AI that reads and understands the Web better than any team of humans, a database larger than Google’s, and a paying customer base that includes some of the most respected companies in the world. For more information visit www.diffbot.com.
About Diffbot:
Diffbot is a robot that examines the Web using computer vision and natural language processing, and provides developers with robust tools to find, extract and understand the objects from any Web page for use in their applications. Thousands of developers and businesses rely on Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, the mobile web and hundreds of other consumer applications. Customers include Adobe, Amazon, CBS Interactive, Cisco, eBay, Instapaper, Microsoft, Salesforce, Samsung, StumbleUpon. It is based in Palo Alto, CA.
To learn more visit www.diffbot.com",-0.12551,https://www.diffbot.com/company/news/20160211.jsp,,
Diffbot’s Revolutionary Product API Automatically Extracts Data from Any Product Web Page,d2013-07-31T00:00,,Diffbot,"The Product,Data,Product (business),Automation,Diffbot,Database,Web page,AOL,Extract","First of Its Kind API Uses Computer Vision to Turn Any E-Commerce Site Into a Product Database
PALO ALTO-- Diffbot, inventors of computer vision technology that sees web pages like humans do, today announced the release of its Product API, which automatically identifies and extracts product data from any shopping web page.
Diffbot also announced updates to its Crawlbot spidering service, which can accurately determine which pages on a shopping site are product pages. Diffbot now offers a turnkey solution for retrieving the entire catalog from any e-commerce site -- without need of a published API or any action on the part of the retailer.
Developed over the course of two years, the Product API’s pioneering algorithm is built on Diffbot’s core vision technology which has accurately extracted structured data from billions of web pages. The API advances Diffbot’s machine learning, natural-language processing and computer vision systems to identify and structure information regardless of a site’s design, layout, markup or even its (human) language.
The Product API automatically makes available data such as price, discount/savings, shipping cost, product description, images, SKU and manufacturer's product number. The technology allows developers to immediately use product data from any e-commerce site in their web or mobile applications.
The Product API will enable developers to rapidly build applications that can:
track and compare prices from any site
augment user bookmark or clipping data with product pricing and other information
track merchandise availability across multiple storefronts
migrate entire shopping sites to new platforms without the need of back-end integration
deploy entire APIs on-the-fly for partner and other integrations
""E-commerce is one of the most popular activities on the web. With 28% of US internet users shopping on a daily basis, we figured we should teach our robot how to understand products,"" said Mike Tung, CEO of Diffbot.[1] ""The Product API represents our latest advances in pushing the capabilities of automated page extraction. We are one step closer to the imminent goal of making the entire web machine-readable.""
Last year, Diffbot conducted a study which found that 8% of links shared on Twitter are for product pages -- a total of more than eight million product links per day.[2] [3] [4] Just as with news articles, intelligent automation to help sift through the vast quantities of products offered and shared online is something needed by consumers and businesses alike.
The Product API joins Diffbot's previous computer vision APIs, including the Frontpage API (for extracting content from home pages), the Article API (for extracting news article and blog post content), the Image API, and its Page Classifier API, which automatically determines the type of page of any web link.
About Diffbot:
Diffbot is a robot that examines the Web using artificial intelligence, computer vision, machine learning and natural language processing, and provides developers with robust tools to find, extract and understand the objects from any Web page for use in their applications. Thousands of developers use Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, the mobile web and hundreds of other consumer applications. Customers include AOL, Betaworks (Digg/Instapaper), CBS Interactive, Salesforce and StumbleUpon. It is based in Palo Alto, CA.
To learn more visit www.diffbot.com
[1] http://pewinternet.org/Trend-Data-(Adults)/Online-Activities-Daily.aspx
[2] http://www.diffbot.com/products/automatic/classifier/#t-infographic
[3] https://blog.twitter.com/2013/celebrating-twitter7
[4] http://techcrunch.com/2010/09/14/twitter-seeing-90-million-tweets-per-day/",,https://www.diffbot.com/company/news/20130731.jsp,,
????Diffbot:?Web????????,?????????,d2013-08-30T15:23,,CSDN.net,"??,???,Google,???,????","??:Diffbot???????????,?????????????????Web??,?????????????????????????,????Sun????????Google????????Andy Bechtolsheim?
Diffbot???????????,???????????????????????Web??,??????????“????”????Diffbot?????API,????????????????????,?????????????,????????????
Diffbot???????“?????”?????????????(??????????),??????????????????Diffbot?????????Mike Tung??:“????????????????,????????????????????”
Diffbot?API??????????????????,????????????????????,?????????????????SKU???????????(?????CloudTimes)
Diffbot????Web?????????——????,??,??,????????Diffbot ????????,????????????????????????API???API,????API?
Diffbot?????Instapaper(???????????????),?????????????????,????????????????
????,???????????????,?????????????????????Web???????????,????????????????????,??????????“???”?,?????????Tung??,Diffbot?????????API??Web???SaaS????????????
Diffbot?????????????,??Andy Bechtolsheim(?????????Google???????,Sun???????)?Sky Dayton(EarthLink?Boingo Wireless????)?Joi Ito(MIT Media?????)? Brad Garlinghouse(????????)??Jonathan Heiliger(Facebook???????)?
??????Palo Alto?Diffbot???2008?,???????????Mike Tung?Leith Abdulla??????????(?/??,??/??)
????:Diffbot aims to convert the web into one big database, one page at a time
Cloud Edge:2013???“???”????
??
????
????
CEO/CTO
????
????/??
1.
HStreaming
2011?
Jana Uhlig
$ 1M (B)
??Hadoop??
2.
CitusData
2012?
Matt Ocko
CitusDB
3.
Backblaze
2009?
Gleb Budman
??????
4.
Kickboard
2009?
Jennifer Medberry
$2.8M(A)
Kickboard(????)
5.
Elasticsearch
2012?
Shay Banon
$24 M(B)
??????
6.
Appcore
2008?
Jeff Tegethoff
$6M (B)
??????IaaS??
7.
Pertino
2011?
Craig Elliott
$20 M(B)
??????(SDN)
8.
SwiftStack
2011?
Joe Arnold
$6.1M(A)
??????
9.
Spiral Genetics
2009?
Adina Mangubat
$3M(A)
DNA????????
10.
DNNResearch
2012?
Geoffrey Hinton
????
11.
AppNeta
2011?
Jim Melvin
$16M(C)
??????(APM)
12.
Concurrent
2008?
Chris K. Wensel
$4M(A)
Java?????
13.
AirWatch
2003?
John Marshall
$200M(A)
??????
14.
Pluribus
2012?
Robert Drost
$44M(C)
?????
15.
Bina Technology
2006?
Narges Bani Asadi
$6.5M(B)
??????
16.
Sociocast
2010?
Albert Azout
$1M(B)
??????
17.
ParElastic
2010?
Ken Rugg
$5.7M(A)
????????
18.
Optimizely
2009?
Dan Siroker
$28M(A)
A/B ????
19.
Instart Logic
2010?
Manav Mital
$17M(B)
????
20.
CloudFlare
2010?
Matthew Prince
$20M(B)
????????
21.
Ionic Security
2012?
Ted Schlein
$9.4M(A)
???
22.
MemSQL
2011?
Eric Frenkiel
$5M(A)
MemSQL
23.
Qubole
2012?
Ashish Thusoo
$7M(A)
AWS??Hive??
24.
Clustrix
2005?
Robin Purohit
$16.5M(C)
Clustrix Sierra
25.
Cloudant
2008?
Derek Schoettle
$12M(B)
DBaaS
26.
DataTorrent
2012?
Phu Hoang
$8M(B)
??????
27.
WibiData
2010?
Christophe Bisciglia
$15M(B)
Hadoop????
28.
Dataminr
2009?
Ted Bailey
$30M(C)
??????
29.
PlumGrid
2011?
Awais Nemat
$10.7M(A)
??????(SDN)
30.
Meldium
2012?
Anton Vaynshtok
$1 M(A)
??????
31.
Myrrix
2012?
Sean Owen
???,????
32.
Alpine data Labs
2010?
Joe Otto
$7.5 M(A)
???????
33.
Context Relevant
2012?
Stephen Purpura
$7 M(A)
???????
34.
Datameer
2009?
Stefan Groschupf
$9.2M(B)
???????
35.
Skytree
2012?
Alexander Gray
????
36.
Wise.io
2012?
Joshua Bloom
????
37.
Treasure Data
2012?
Hiro Yoshikawa
$10 M(?)
???????
38.
NimbusBase
2012?
Alex Volodarsky
??API
39.
Zimory
2007?
Ruediger Baumann
$20 M(B)
IaaS ??????
40.
DataStax
2010?
Billy Bosworth
$45 M(B)
???NoSQL???
41.
ZestFinance
2012?
Douglas Merrill
$20 M(C)
????+?????
42.
InfoChimps
2009?
Jim Kasksade
$5M(?)
??????????
43.
DigitalOcean
2012?
Ben Uretsky
$3.2M
IaaS??,?????
44.
SOASTA
2006?
Tom Lounibos
$30M
?????
45.
Birst
2004?
Brad Peters
$38M
?????????
46.
Hortonworks
2011?
Rob Bearden
$98M (?)
Hadoop????
47.
Parse.ly
2012?
Sachin Kamdar
$5M (A)
??????
48.
Diffbot
2008?
Mike Tung
$2M (A)
????,????
??:2013?8?30???,?????......
???CSDN????,????????,???????market#csdn.net(#??@)",,https://www.csdn.net/article/2013-08-30/2816771-Diffbot,,
Diffbot Help and Documentation,,,Diffbot,"Diffbot,Documentation","API Documentation
Analyze API
Determines the page-type for any given URL (and routes it to the appropriate extraction API, where applicable)
Article API
For structuring news articles, blog posts and other text-heavy pages
Discussion API
For discussion forums and message board threads
Image API
For extracting the primary images from pages
Product API
For structuring e-commerce product pages
Video API
For extracting metadata from video pages",,https://www.diffbot.com/dev/docs/,,
http://services.brid.tv/player/build/brid.min.js:35 Status: 200 OK X-Diffbot-Render-Hostname: morpheus Content-Type: text/html; charset=utf-8 X-Location: http://sitiosargentina.com.ar/notas/historial/2009/diciembre/13.htm,,,DIARIOS Y NOTICIAS DE ARGENTINA,"Diffbot,List of HTTP status codes,Hostname,Character encoding,Media type,UTF-8,Argentina","http://services.brid.tv/player/build/brid.min.js:35 Status: 200 OK X-Diffbot-Render-Hostname: morpheus Content-Type: text/html; charset=utf-8 X-Location: http://sitiosargentina.com.ar/notas/historial/2009/diciembre/13.htm
Sitios Argentina - Notas & Noticias Destacadas Noviembre 2009
http://services.brid.tv/player/build/brid.min.js:35 Status: 200 OK X-Diffbot-Render-Hostname: choi5 Content-Type: text/html; charset=utf-8 X-Location: http://sitiosargentina.com.ar/notas/historial/2009/diciembre/2.htm
Sitios Argentina - Notas & Noticias Destacadas Noviembre 2009
Un escalofriante video muestra que el testigo clave en la causa de Forza no se habrï¿½a suicidado",,http://sitiosargentina.com.ar/notas/historial/2009/diciembre/13.htm,,
Diffbot: Extracting Structured Data from the Internet,d2016-02-14T00:00,,Nanalyze,"Bing,deep learning,Google,Google,Diffbot,Data,World Wide Web,Knowledge Graph,Athens,Structured","Google search is something we cannot live without. There isn’t a day that goes by where we don’t use Google to look up some fact or to research some new topic or concept. All the articles you read on Nanalyze come from information obtained through Google searches. If you’re a regular Google user, you would have started to notice some changes in search results. We first started noticing it when we were looking up populations for various cities and countries around the world. Try asking Google what the population is for any country or city you can think of right now. You see? It answers your question and provides a nifty graph like the one seen below:
Traditionally, search algorithms will try to answer your search by combing through millions of pages to find the one single web page that will most likely answer your question. Now, through the power of deep learning, Google can begin to answer questions directly instead of redirecting you to someone else’s web page. Now if I search for “When did the Beatles start”, Google just tells me right off:
These types of answers are referred to as “structured knowledge”. If you ask Google “how many people are in Athens” or “population of Athens”, it knows what you are looking for even though you are asking the same question in two different ways. Google calls this technology the Google Knowledge Graph, which is a large database of semantic data describing more than 1 billion people, places, and things. Pretty soon you’ll be able to speak to Google using actual human language. If you ask Google “What was the name of that long Western movie about a slave who is freed by a bounty hunter and then goes and seeks revenge”, it’ll soon answer with “Django”. Using deep learning to comb through big data and create large databases of structured data is the next big thing for search, but it also has applications outside of search. One interesting company in this space is called Diffbot.
Founded in 2010, Silicon Valley-based startup Diffbot was the first startup funded by Stanford’s SSE Ventures. The Company just closed a Series A funding round of $10 million last week bringing their total funding to $12.5 million. Their latest round was led by Tencent, one of China’s largest Internet companies.
Diffbot’s technology automatically extracts content from websites, articles, products, discussions, images and more. Incredibly, it does so with better-than-human-level accuracy across any website or language. This technology is available via software-as-a-service (SaaS) and uses advanced artificial intelligence technology to retrieve clean, structured data without the need for manual rules or site-specific training. Big companies like AOL, Adobe, Cisco, and eBay, are all using Diffbot’s technology including Microsoft who uses it to compliment Bing (that’s Microsoft’s search engine). Diffbot utilizes an “on-demand” business model with the below pricing model:
Not only does Diffbot offer their technology for others to use but they also use it themselves to demonstrate how powerful it is. Last year, Diffbot performed a study in the travel industry to analyze customer sentiment. The unique aspect of this study was that it analyzed user-generated content (UGC) such as article comments, reviews in TripAdvisor or Yelp, blog posts etc. all of which reflect what users are actually saying about their travel experiences. The study captured 230,303,990 datapoints across over ten thousand sites in a 2-week time frame. Can you imagine how long it would take you to read 230 million comments? Diffbot completed this study in just two weeks, and found out that British Airways, Hertz, and Hilton customers are the most grumpy out there. You can read more about Diffbot’s interesting findings in this article by Fortune on the topic.
Diffbot is building their own version of Google Knowledge Graph which already contains more than 1.2 billion objects and is increasing at a rate of 10 million objects per day. So how can retail investors make money here? The problem for retail investors is that Diffbot remains private at the moment and with the Series A funding round just closing a few days ago, it won’t be likely to look for an IPO anytime soon.
If you enjoyed this article, then sign up for our free newsletter - Nanalyze Weekly. About every week, we'll send you a simple summary of all our new articles. If you didn't enjoy this article, share it on Twitter and tell everyone how much you hated it.",,https://www.nanalyze.com/2016/02/diffbot-extracting-structured-data-from-the-internet/,,
Diffbot:????? ?web???????,d2011-08-29T00:00,,oschina.net,"??,???,??·D·???,??????","????8?26???,???????,Diffbot?????????“????”, ??????:????????????????????,???????Web?????Diffbot??????Mike Tung?:“???????????30??????,Diffbot????????????”????,Diffbot????????????? ????????????????????????
??,Diffbot????????API(??????),??????????????????????????????????????????,????????,????????????????
????API
???????Diffbot??????API???????????:????????????;????????????????????;?????RSS??????RSS????;????????????,?????????????????
??????????Diffbot??????API,??????????????????,???????????????,?????????????????????Diffbot?????????,?????API?????????????
???????? API?:
On-Demand API:??API???????“??” (Frontpage)API?“??” (Article) API??????????????????(??????????????????????????????),“??”API?????“???”? ??????????
Follow API:???????????????????Diffbot???????????????????,?????????????????,???????????????
??????????Diffbot?API?,???????????Nuance??,????(AOL)??,?????????SocMetrics???
AOL ??Diffbot?API?????iPad ??????????????????,????????? Nuance????????????????????????;??????????????????????SocMetrics??bit.ly? ????Diffbot,????????????,?????????????????????????
? ??????????Diffbot?????,???????????????????????Hacker News Radio(??????)?????????«????»??????,FeedBeate?????????????????RSS???????? ?Diffbot??Twitter?: ???????????????????(???RSS),??Twitter ???????
??Diffbot???????????,?????????API 5???“???” ?? 500??,???????API 10??,??????? 0.002???????????????????????
Diffbot ????????????Mike Tung?Leith Abdulla??????Tung????????????????????????????Diffbot??????????(????SSE Labs,??StartX)???????????",,https://www.oschina.net/news/20949/diffbot-developers-tools,,
"How to save a perfectly-scraped webpage into DEVONthink using IFTTT, Diffbot, Hazel, & several command line tools",d2013-11-17T00:00,Scott Granneman,chainsawonatireswing.com,"IFTTT,Web page,Hazel,Table of contents,Diffbot,Command-line interface","DEVONthink is a key piece of software for me on my Mac. In particular, I use it to store copies of webpages that I run across that I want students to read or that I want to refer back to for teaching, or for writing, or for my own use. Now, it’s very easy to get webpages into DEVONthink by using the browser extensions that come with the software. You click on the extension, & you get a small window:
This is great, as is the checkbox for Instapaper, which runs the webpage through that awesome service & gives you results with just the featured content & none of the crap. But even with Instapaper, these results are not perfect, at least for me.
Here’s my problem: I want a webpage so that I can see images & hyperlinks & other stuff that only comes with the Web. I like PDFs, but not when I can just have good ol’ HTML to deal with. But if I choose the HTML Page or Web Archive options, then I get a bunch of junk I don’t want, like ads & extraneous content. If I check the box next to Instapaper, I get less junk, but I lose a lot of control over what gets selected & what doesn’t get selected, & the original URL of the webpage, along with a lot of other important metadata, gets stripped away by Instapaper. In other words, I want this:
See? Neat & clean, with the title of the Web article at the top as an H1, & then the author, date of publication, & URL below, all H2s in the HTML, & finally the content & nothing else.
Yes, I know this is picky, but it’s what I really want. So I set out to create it over several months, & I finally got it all figured out & set up & working this summer. After testing it for months to verify that it works well, I am now ready to unveil this process to you, the readers of Chainsaw on a Tire Swing.
Before I dig in to the details, let me give you the 20,000-foot summary of the process. It might seem complicated, & I guess it kinda is, but it’s not that bad if you go through it step by step, & it does work beautifully. I’m going to mention several services in this introduction that you might not have heard of. Don’t worry; I’ll explain everything below.
Send an email to the IFTTT (If This Then That) service which contains the URL of the webpage at the top of the message.
IFTTT saves the email as a file in a specific folder in your Dropbox.
Hazel on your Mac notices the new file in the folder & runs a shell script.
The shell script grabs the URL out of the file & sends a request to the Diffbot service, which saves the result to the /tmp directory as a webpage.
The shell script converts that resulting webpage to a .webarchive file & saves it to DEVONthink’s Inbox folder, where it is automatically imported into DEVONthink.
Got that? OK, let’s set it all up!
Table of Contents
Diffbot
I love Diffbot. I really do. It’s the best service of its type I’ve seen, the price is right (free for the 1st 10,000 requests each month!), & the support I’ve received when I’ve had questions or issues has been top-notch. So what’s it do?
Simple. It’s a scraper: you send a request to Diffbot using its API, you get back the data from a webpage, shorn of all the junk. It’s like Safari’s Reader feature, but available programmatically. Here’s an example.
First, a blog post at The Atlantic’s website, as it appears in a browser:
So, here’s what you need to do: go to Diffbot’s website, create an account, find out your Diffbot Developer Token (you’ll need it for the shell script), & then come back here.
Dropbox
You don’t have to create these folders exactly where I specify, but if you change their locations, you’re going to need to edit the shell script that’s coming up.
Create a folder at root of your Dropbox named Incoming. Inside the Incoming folder, create another folder named DEVONthink. Your folder structure should therefore look like this: ~/Dropbox/Incoming/DEVONthink
IFTTT
If you don’t already have an account with If This Then That (IFTTT), go get yourself one! It’s a free service that lets you tie together online services so that when one event happens at one service, then something happens as a response. For example, every time you post a picture to Facebook, a copy is placed in a Dropbox folder, or every time a particular RSS feed is published, it’s scanned by IFTTT, & if certain words are in the title, that post is emailed to you. It’s such a great service that I’d pay for it if I had to.
To use it with my process here, create an account at IFTTT if you don’t already have one, log in to IFTTT, & activate the Dropbox & Email channels.
Now go to My Recipes & click Create A Recipe. Here’s what you’re going to fill in:
Description: App emails IFTTT a URL, which gets saved as a text file
Trigger: Send IFTTT an email from your email address with a tag of dt (for DEVONthink, get it?).
Save it, & you’re good to go.
So here’s what happens: you find a webpage that you want to capture in DEVONthink. You email the link to yourself, with the URL as the first line of the body of the email (you can have other stuff in the email, like your signature, but it will be ignored by the upcoming shell script). As for the subject, it really doesn’t matter—it can be words, it can be a URL as well, it can be nothing—as long as you have #dt in it (I always put it at the end because that’s easy).
When the email arrives at IFTTT, it is saved as a text file in the specified Dropbox folder. The subject of your email becomes the name of the file, & the body of your email becomes the contents of the file.
We now have a place in Dropbox for incoming text files containing URLs that we want to use, & a method for getting those text files into Dropbox: emailing IFTTT. But what do we do with those text files once they’re in there? Time for some shell scripting!
Needed command line software
The shell script I’m going to provide has several requirements:
gecho (the GNU version of echo)
gsed (the GNU version of sed)
dos2unix (converts text files between Windows & UNIX/Mac OS X formats)
jsonpp (prettifies JSON files)
terminal-notifier (send Mac OS X notifications)
webarchiver (create Safari .webarchive files)
All of those but one are available through Homebrew, so if you haven’t already installed that, you’ll need to do so.
Once you have Homebrew up & running, run this command (it’s not obvious, but coreutils takes care of gecho—& a whole lot more besides):
$ brew install coreutils gnu-sed dos2unix jsonpp terminal-notifier
Update from 2016-06-03: Homebrew now includes webarchiver, so just add that to the line above. You do not need to download the code & compile it using Xcode, unless you really want to.
If you use MacPorts (who uses that anymore?), you can download webarchiver pretty easily, according to the developer:
$ sudo port install webarchiver
I don’t use MacPorts, so I have no idea how effectively this is. Instead, you’re going to have download the code & compile it using Xcode.
I went to the GitHub page for the webarchiver project, got a copy of the code (don’t download the release, as that’s 0.3, which is ancient & won’t compile on newer Macs; instead, get the latest code, which is version 0.5), & double-clicked on webarchiver.xcodeproj to open the project in Xcode. Once in Xcode, I went to Product > Build, which successfully compiled the code, leaving the binary in /Users/scott/Library/Developer/Xcode/DerivedData/webarchiver-dreeepqxmdlkgieggztknlbwsula/Build/Products/Debug/webarchiver. Obviously, your path under DerivedData will be different
. I then moved the webarchiver binary to /usr/bin.
Once you’ve moved webarchiver to its new home, test it:
$ webarchiver
webarchiver 0.5
Usage: webarchiver -url URL -output FILE
Example: webarchiver -url http://www.google.com -output google.webarchive
-url  http:// or path to local file
-output File to write webarchive to

Updates can be found at https://github.com/newzealandpaul/webarchiver/
If you see that output, you’re good to go.
The shell script
Place the shell script you see below in your ~/bin directory. I named it conv_to_webarchive.sh (you can use your own, but if you change the name, you’ll need to also change the instructions for Hazel that are coming up). I’ve commented the heck out of it, so I hope that helps explain what each step is doing.
#!/bin/bash
 
#===============================================================================
#          FILE:  conv_to_webarchive.sh
#         USAGE:  Automatic with Hazel
#   DESCRIPTION:  Uses diffbot to download essential info about an article
#                 & webarchiver to convert it to a .webarchive file
#        AUTHOR:  Scott Granneman (RSG), scott@chainsawonatireswing.com
#       COMPANY:  Chainsaw On A Tire Swing
#       VERSION:  0.4
#       CREATED:  06/22/2013 13:50:23 CDT
#      REVISION:  11/17/2013 15:20:43 CDT 
#===============================================================================

#####
####
### Variables
##
# 

incoming_dir=""/Users/scott/Dropbox/Incoming/DEVONthink""

devonthink_dir=""/Users/scott/Library/Application Support/DEVONthink Pro 2/Inbox""

fail_safe_dir=""/Users/scott/Desktop""

diffbot_token=""tm3wnis0wa1irfvgl4ulmqi3iiu0sx1f""

#####
####
### Grab webpages
##
# 

# Test to see if the necessary directories exist
if [ -e ""$incoming_dir"" ] && [ -e ""$devonthink_dir"" ] ; then
  # Set IFS to split on newlines, not spaces, but first save old IFS
  # See http://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names
  SAVEIFS=$IFS
  IFS=$'\n'
  # If you can cd to the Incoming/DEVONthink directory, run everything else
  if cd $incoming_dir ; then 
    # For every file containing a URL in the Incoming/DEVONthink directory
    for i in $(ls *)
    do
      # If it’s not empty, process it;
      # if it IS empty, move it so Diffbot doesn’t keep trying forever
      if [[ -s $i ]] ; then
        # Check if it’s a Windows-formatted file; if it is, convert it to UNIX
        if [ $(grep -c $'\r$' ""$i"") \> 0 ] ; then
          terminal-notifier -message ""$1 is a Windows file, so convert it"" -title ""Windows File Found""
          /usr/local/bin/dos2unix ""$1""
        fi
        # Delete any blank lines
        # Note: will only work with UNIX line endings, hence the previous conversion by Hazel
        /usr/local/bin/gsed '/^$/d' ""$i"" > ""$i"".out
        mv ""$i"".out ""$i""
        # Read the file to get the URL
        # I use head instead of cat because the file usually comes in via email,
        # & I’m too lazy when composing to leave off my email sig
        url=$(head -n 1 $i)
        /usr/local/bin/gecho -e ""\nURL in the file is $url""
        # URL encode the, uh, URL
        encoded_url=$(python -c ""import sys, urllib as ul; print ul.quote_plus(sys.argv[1])"" $url)
        /usr/local/bin/gecho -e ""\nEncoded URL is $encoded_url""
        # Grab JSON-formatted article & data from Diffbot, 
        # clean up JSON, & write results to file
        if curl ""http://www.diffbot.com/api/article?token=$diffbot_token&url=$encoded_url&html&timeout=20000"" | /usr/local/bin/jsonpp > /tmp/results.json ; then
          # Pull out article’s name
          article_title=$(grep -m 1 '""title"":' /tmp/results.json | /usr/local/bin/gsed 's/  ""title"": ""//' | /usr/local/bin/gsed 's/"",$//' | /usr/local/bin/gsed 's:\\\/:-:g' | /usr/local/bin/gsed 's://:-:g' | /usr/local/bin/gsed 's/\\""/""/g' | /usr/local/bin/gsed -f /Users/scott/bin/conv_to_webarchive.sed)
          /usr/local/bin/gecho -e ""\nArticle Title is $article_title""
          # If $article_title is empty, move it so Diffbot doesn’t keep trying forever;
          # if it’s not empty, continue processing it
          if [[ -z $article_title ]] ; then
            # If $article_title is empty, move it!
            mv $i $fail_safe_dir
            terminal-notifier -message ""Diffbot could not parse title in $i"" -title ""Problem with Diffbot""
          else
            # If results.json can be renamed, continue processing;
            # if it can’t be renamed, move it!
            if mv /tmp/results.json /tmp/""$article_title"".json ; then
              # Pull out article’s other metadata
              article_author=$(grep -m 1 '""author"":' /tmp/""$article_title"".json | /usr/local/bin/gsed 's/  ""author"": ""//' | /usr/local/bin/gsed 's/"",$//' | /usr/local/bin/gsed -f /Users/scott/bin/conv_to_webarchive.sed)
              /usr/local/bin/gecho -e ""\nArticle Author is $article_author""
              article_date=$(grep '""date"":' /tmp/""$article_title"".json | /usr/local/bin/gsed 's/  ""date"": ""//' | /usr/local/bin/gsed 's/"",$//' | /usr/local/bin/gsed -f /Users/scott/bin/conv_to_webarchive.sed)
              /usr/local/bin/gecho -e ""\nArticle Date is $article_date""
              article_url=$(grep '""url"":' /tmp/""$article_title"".json | /usr/local/bin/gsed 's/  ""url"": ""//' | /usr/local/bin/gsed 's/"",$//' | /usr/local/bin/gsed 's/\\//g' | /usr/local/bin/gsed 's/""$//')
              /usr/local/bin/gecho -e ""\nArticle URL is $article_url""
              # Write HTML to file
              # Remove JSON stuff, fix Unicode, then remove \n, \t, & \
              grep '""html"":' /tmp/""$article_title"".json | /usr/local/bin/gsed 's/  ""html"": ""//' | /usr/local/bin/gsed 's/"",$//' | /usr/local/bin/gsed -f /Users/scott/bin/conv_to_webarchive.sed | /usr/local/bin/gsed 's/\\n//g' | /usr/local/bin/gsed 's/\\t//g' | /usr/local/bin/gsed 's/\\//g' > /tmp/""$article_title"".html
              # Prepend metadata to file
              /usr/local/bin/gsed ""1i <h1>$article_title</h1>\n<h2>$article_author</h2>\n<h2>$article_date</h2>\n<h2>$article_url</h2>\n"" /tmp/""$article_title"".html > /tmp/""$article_title""_1.html && mv /tmp/""$article_title""_1.html /tmp/""$article_title"".html
              # Prepend HTML metadata to file
              /usr/local/bin/gsed ""1i <!DOCTYPE html>\n<html>\n<head>\n<meta charset=""UTF-8"">\n<title>$article_title</title>\n</head>\n<body>\n"" /tmp/""$article_title"".html > /tmp/""$article_title""_1.html && mv /tmp/""$article_title""_1.html /tmp/""$article_title"".html
              # Append HTML metadata to file
              echo ""</body></html"" >> /tmp/""$article_title"".html
              # Using the webarchiver tool I downloaded & compiled, create a webarchive
              if webarchiver -url /tmp/""$article_title"".html -output $devonthink_dir/""$article_title"".webarchive ; then
                # If it works, then delete the file
                rm $i
              else
                # Couldn’t create a webarchive
                terminal-notifier -message ""No webarchive for $i"" -title ""Problem creating webarchive""
              fi
            else
              # If results.json can’t be renamed, move it!
              mv $i $fail_safe_dir
            fi
          fi
        else
          # If Diffbot fails, move it!
          mv $i $fail_safe_dir
          terminal-notifier -message ""Could not Diffbot $i"" -title ""Problem with Diffbot""
        fi
      else
        # If it’s empty, move it!
        terminal-notifier -message ""$i is empty!"" -title ""Problem with parsing file""
        mv $i $fail_safe_dir
      fi
    done
  else
    # Needed directory isn’t there, which is weird
    /usr/local/bin/gecho -e ""\nIncoming DevonThink directory is missing for $i!"" >> ""$fail_safe_dir/DEVONthink Problem.txt""
  fi
  # Restore IFS so it’s back to splitting on <space><tab><newline>
  IFS=$SAVEIFS
else
  # Needed directories aren’t there, which is very bad
  /usr/local/bin/gecho -e ""\nIncoming or DevonThink directories are missing for $i!"" >> ""$fail_safe_dir/DEVONthink Problem.txt""
fi

exit 0
Note the following about the script:
Make sure the variables are correct for your setup.
In particular, you’ll need to enter your Diffbot Developer Token for diffbot_token. And no, that’s not mine. I randomly generated a lookalike.
The paths that start with /Users/ are all for my Mac. You’ll need to change them for yours.
You might notice that I encode URLs in the middle; in other words, I turn http://www.chainsawonatireswing.com/ into http%3A%2F%2Fwww.chainsawonatireswing.com%2F. This is what Diffbot wants, so it is what Diffbot gets.
It’s pretty easy to test and make sure you’re getting the right results from Diffbot. Just use the line from the script: curl ""http://www.diffbot.com/api/article?token=$diffbot_token&url=$encoded_url&html&timeout=20000"" | /usr/local/bin/jsonpp, but put in your Diffbot Developer Token instead of $diffbot_token & the encoded URL you want to test instead of $encoded_url. By piping the output to jsonpp, you get readable results.
Yes, I use sed (actually gsed) a lot. I refer to a file named conv_to_webarchive.sed a few times. That file is detailed in the next section.
You don’t need the lines with gecho, but I found them very useful while I was developing & testing the script, & they don’t do any harm, so I left them. If they bother you, take ’em out.
Notice the lines that say mv $i $fail_safe_dir. All are fail-safes in case files can’t be renamed or parsed. This became critical when I did not have them in place, & one night a file got stuck trying Diffbot repeatedly, so that I racked up 15,000 queries or so in just a few hours. Fortunately, I shamefacedly explained what happened to Diffbot support, & they very kindly forgave me. And then I immediately put in place those fail-safes, as I should have from the beginning. So if you see files on your desktop, look in them, as they indicate problems that you need to fix by hand.
The sed file
In my shell script I refer to conv_to_webarchive.sed a number of times. If you don’t know what sed is, it’s basically a way to edit files programmatically from the command line. It’s also very cool & does a million things, most of which I know nothing about (although I’d love to learn!).
Here’s the contents of conv_to_webarchive.sed:
s/\\u0092/’/g
s/\\u0093/“/g
s/\\u0094/”/g
s/\\u0097/—/g
s/\\u2013/–/g
s/\\u2014/—/g
s/\\u2018/‘/g
s/\\u2019/’/g
s/\\u201C/“/g
s/\\u201c/“/g
s/\\u201D/”/g
s/\\u201d/”/g
s/\\u2020/†/g
s/\\u2021/‡/g
s/\\u2022/•/g
s/\\u2026/…/g
s/\\u2033/""/g
I have built this file up over time, as I have found errors in the results generated by Diffbot & the other programs. Basically, Diffbot stuck the encoding for a character in the results, & I want the actual character itself. So, for instance, instead of an ellipses, I saw \u2026 in the file; my sed file turns \u2026 back into … so that it’s readable. As I discover more, I’ll add to the file.
Hazel
So we have a shell script that processes files, but how do we tell the shell script to run? Enter Hazel. Basically, Hazel watches the folders you tell it to watch, & when something changes in those folders, Hazel processes the files according to the rules you specify.
In this case, we’re going to tell Hazel to watch the Incoming/DEVONthink folder, & when a file is placed inside, the shell script detailed above should run, processing the file. Lather, rinse, repeat.
Open Hazel. On the Folders tab, press the + under Folders to add a new folder. Select ~/Dropbox/Incoming/DEVONthink.
Under Rules, press the + to add a new rule. A sheet will open, named for the folder; in this case, DEVONthink.
For a Name, I chose Convert URL to DEVONthink webarchive.
Now you need to make selections so that the following instructions are mirrored in Hazel.
If all of the following conditions are met:
is
txt
Do the following:
Run shell script
Choose Other… & select ~/bin/conv_to_webarchive.sh
Press OK to close the sheet, & then close Hazel.
Test
To test your work, email a URL to trigger@ifttt.com with #dt in the subject. A few seconds later, you should see a new entry appear in your DEVONthink Inbox, stripped of extraneous formatting & info thanks to Diffbot.
In subsequent posts, I’m going to tell how to automate emailing that URL using Keyboard Maestro on the Mac & Mr. Reader & other apps on your iOS devices. I could do it here, but this post is long enough already! And even without that info, I still find this process I’ve detailed here to be incredibly useful, so much so that I use it at least 10 times a day to save webpages into DEVONthink. I hope you find it useful too!",,https://www.chainsawonatireswing.com/2013/11/17/how-to-save-a-perfectly-scraped-webpage-into-devonthink/,,
"Web Scraping Software Market 2020 COVID-19 Impact Analysis, Revenue Study, Best Players – DataForSEO,Diffbot,HelpSystems,Import.io,justLikeAPI",d2020-08-14T00:00,Sameer Joshi,Galus Australis,"pricing,Software Market,Web mining,HelpSystems,Diffbot,DataForSEO,justLikeAPI,COVID-19,software,Mozenda","Web scraping software is data scraping utilized for extracting data from websites. Web scraping a web page comprises fetching and extracting data from it. Web scraping is used for contact scraping, web mining and data mining, online price change monitoring, and price comparison. Web scraping is also known as web harvesting or web data extraction.
Key Players:
– DataForSEO
– Diffbot
– HelpSystems
– Import.io
– justLikeAPI
– Mozenda, Inc.
– Octopus Data Inc.
– Scrapinghub
– SerpApi, LLC
– Webhose.io
Request Sample Copy of Web Scraping Software Market: https://www.premiummarketinsights.com/sample/TIP00029158
The growth of the web scraping software market is driven by key factors such as manufacturing activity in accordance with the current market situation and demand, risks of the market, assessment of the new technologies, acquisitions, new trends, and their implementation. Moreover, an increase in research and development activities in various industries is anticipated to boost the growth of the web scraping software market.
The global web scraping software market is segmented on the basis of deployment type, organization size. On the basis of deployment type, the market is segmented as on-premise, cloud. On the basis of organization size, the market is segmented as large enterprises, SMEs.
The “Global Web Scraping Software Market Analysis to 2027”? is a specialized and in-depth study of the web scraping software market with a special focus on the global market trend analysis. The report aims to provide an overview of web scraping software market with detailed market segmentation by deployment type, organization size. The global web scraping software market is expected to witness high growth during the forecast period. The report provides key statistics on the market status of the leading web scraping software market players and offers key trends and opportunities in the web scraping software market.
The report analyzes factors affecting web scraping software market from both demand and supply side and further evaluates market dynamics effecting the market during the forecast period i.e., drivers, restraints, opportunities, and future trend. The report also provides exhaustive PEST analysis for all five regions namely; North America, Europe, APAC, MEA and South America after evaluating political, economic, social and technological factors effecting the web scraping software market in these regions.
Avail Discount on this Report@: https://www.premiummarketinsights.com/discount/TIP00029158
Critical Success Factors (CSFs)
The competitive landscape of the market has been examined on the basis of market share analysis of key players. Detailed market data about these factors is estimated to help vendors take strategic decisions that can strengthen their positions in the market and lead to more effective and larger stake in the global Web Scraping Software Market. Pricing and cost teardown analysis for products and service offerings of key players has also been undertaken for the study.
Table of Contents:
1 Executive Summary
2 Preface
3 Web Scraping Software Market Overview
4 Market Trend Analysis
5 Global Web Scraping Software Market Segmentation
6 Market Effect Factors Analysis
7 Market Competition by Manufacturers
8 Key Developments
9 Company Profiling
About Premium market insights:
Premiummarketinsights.com is a one stop shop of market research reports and solutions to various companies across the globe. We help our clients in their decision support system by helping them choose most relevant and cost effective research reports and solutions from various publishers. We provide best in class customer service and our customer support team is always available to help you on your research queries.
Sameer Joshi
Call: US: +1-646-491-9876, Apac: +912067274191
Email: sales@premiummarketinsights.com",0.793,https://galusaustralis.com/2020/08/844162/web-scraping-software-market-2020-covid-19-impact-analysis-revenue-study-best-players-dataforseodiffbothelpsystemsimport-iojustlikeapi/,,
Diffbot search engine,,,Diffbot,,,,http://staticpages.diffbot.com/smoketest/analyze/index_nonarticle.html,,
Diffbot API: Batch Requests,d2013-01-16T00:00,,Diffbot,"Diffbot,Hypertext Transfer Protocol,POST (HTTP),Uniform Resource Locator,Application programming interface","For those of you with heavy call volume, our Batch API lets you submit up to 50 API calls in a single request, and set a custom timeout parameter to make sure you get what you want on your own timeline.
Full documentation is in our Developer Dashboard. Here’s a quick introduction:
Package each individual API call in its own JSON object, comprised of:
Method (GET or POST)
The URL to the specific API you want to call — including all individual API parameters, even your token — minus “http://www.diffbot.com”
Here’s a sample object that calls our Article API. (Note that the URL is URL-encoded.)
{""method"": ""GET"", ""relative_url"": ""/api/article?url=http%3A%2F%2Fblogs.wsj.com%2Fventurecapital%2F2012%2F05%2F31%2Finvestors-back-diffbots-visual-learning-robot-for-web-content%2F%3Fmod%3Dgoogle_news_blog%26token=<token>""}
Post the JSON objects to http://www.diffbot.com/api/batch, along with your developer token.
Here’s an example using curl that makes two requests of our Article API:
curl -d 'token=<token>' -d 'batch=[{""method"": ""GET"", ""relative_url"": ""/api/article?url=http%3A%2F%2Fblogs.wsj.com%2Fventurecapital%2F2012%2F05%2F31%2Finvestors-back-diffbots-visual-learning-robot-for-web-content%2F%3Fmod%3Dgoogle_news_blog%26token=<token>""},{""method"": ""GET"", ""relative_url"": ""/api/article?url=http%3A%2F%2Fgigaom.com%2Fcloud%2Fsilicon-valley-royalty-pony-up-2m-to-scale-diffbots-visual-learning-robot%2F%26token=<token>""}]' http://www.diffbot.com/api/batch
Optionally, include a timeout parameter in your POST
Specify in milliseconds how long you want to wait for results. The Batch API will return whatever it’s able to retrieve within that timeframe.
If you don’t submit a “timeout” parameter, the API will return its results when all submitted URLs have received individual responses.
Happy batching!",,http://blog.diffbot.com/diffbot-api-batch-requests/amp/,,
http://support.diffbot.com/author/johndavi/,,,Diffbot,,,,http://support.diffbot.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fsupport.diffbot.com%2Fcrawlbot%2Fwhat-does-all-crawling-temporarily-paused-by-root-administrator-mean%2F&format=xml,,
Diffbot_????,,,Baidu,,,,https://m.baidu.com/sf_bk/item/Diffbot/17029283?ms=1&rid=12166750303957125621,,
Installing_100G_RAM_in_Diffbot_Server,d2013-06-17T00:00,Mike Tung,Diffblog,,,,http://blog.diffbot.com/amazon-ec2-spot-instances-autoscaling-for-machine-learning-loads/installing_100g_ram_in_diffbot_server/,,
What’s the Difference Between Web Scraping and Diffbot?,d2017-05-15T00:00,Dru Wynings,Diffblog,"Diffbot,Web scraping","Web scraping is one of the best techniques for extracting important data from websites to use in your business or applications, but not all data is created equal and not all web scraping tools can get you the data you need.
Collecting data from the web isn’t necessarily the hard part. Web scraping techniques utilize web crawlers, which are essentially just programs or automated scripts that collect various bits of data from different sources.
Any developer can build a relatively simple web scraper for their own use, and there are certainly companies out there that have their own web crawlers to gather data for them (Amazon is a big one).
But the web scraping process isn’t always straightforward, and there are many considerations that cause scapers to break or become less efficient. So while there are plenty of web crawlers out there that can get you some of the data you need, not all can produce results.
Here’s what you need to know.
There are actually plenty of ways you can get data from the web without using a web crawler. For instance, many sites have official APIs that will pull data for you. For example, Twitter has one here. If you wanted to know how many people were mentioning you on Twitter, you could use the API to gather that data without too much effort.
The problem, however, is that your options when using site-specific API are somewhat limited; you can only get information from one site at a time, and some APIs (like Twitter) are rate limited, meaning that you have to pay fees to access more information.
In order to make data useful, you need a lot of it. That’s where more generic web crawlers come in handy; they can be programmed to pull data from numerous sites (hundreds, thousands, even millions) if you know what data you’re looking for.
The key is that you have to know what data you’re looking for. Your average web crawler can pull data, but it can’t always give you structured data.
If you were looking to pull news articles or blog posts from multiple websites, for example, any web scraper could pull that content for you. But it would also pull ads, navigation, and a variety of other data you don’t want. It would then be your job to sort through that data for the content you do want.
If you want to pull the most accurate data, what you really need is a tool that can extract clean text from news articles and blog posts without extraneous data in the mix.
This is precisely why Diffbot has tools like our Article API (which does the above) as well as a variety of other specific APIs (like Product, Video, and Image and Page extraction) that can get you the right data from hundreds of thousands of websites automatically with zero configuration.
How Structure Affects Your Outcome
You also have to worry about the quality of the data you’re getting, especially if you’re trying to extract a lot of it from hundreds or thousands of sources.
Apps, programs and even analysis tools – or anything you would be feeding data to – for the most part rely on highly structured data, which means that the way your data is delivered is important.
Web crawlers can pull data from the web, but not all of them can give you structured data, or at least high-quality structured data.
Think of it like this: You could go to a website, find a table of information that’s relevant to your needs, and then copy it and paste it into an Excel file. It’s a time-consuming process, which a web scraper could handle for you en masse, and much faster than you could do it by hand.
But what it can’t do is handle websites that don’t already have that information formatted perfectly, like sites with badly formatted HTML code with little to no underlying structure, for example.
Sites with CAPTCHA codes, pay walls, or other authentication systems may be difficult to pull data from with a simple scraper. Session-based sites that track users with cookies, those that have server admins that block access to non-servers, or those that have a lack of complete item listings or poor search features can all wreak havoc when it comes to getting well-organized data.
While a simple web crawler can give you structured data, it can’t handle complexities or abnormalities that pop up when browsing thousands of sites at once. This means that no matter how powerful it is you’re still not getting all the data you could possibly get.
That’s why Diffbot works so well; we’re built for complexities.
Our APIs can be tweaked for complicated scenarios, and we have several other features, like entity tagging that can find the right data sources from poorly structured sites.
We offer proxying for difficult-to-reach sites that block traditional crawlers, as well as automatic ban detection and automatic retries, making it easier to get data from difficult sites. Our infrastructure is based on gigablast, which we’ve open sourced.
There are many other issues with your average web crawler as well, including things like maintenance and stale data.
You can design a web crawler for specific purposes, like pulling clean text from a single blog or pulling product listings from an ecommerce site. But in order to get the sheer amount of data you need, you have to run your crawler multiple times, across thousands or more sites, and you have to adjust for every complex site as needed.
This can work fine for smaller operations, like if you wanted to crawl your own ecommerce site to generate a product database, for instance.
If you wanted to do this on multiple sites, or even on a single site as large as Amazon (which boasts nearly 500 million products and rising), you would have to run your crawler every minute of every day across multiple clusters of servers in order to get any fresh, usable data.
Should your crawler break, encounter a site that it can’t handle, or simply need an update to gather new data (or maybe you’re using multiple crawlers to gather different types of data), you’re facing countless hours of upkeep and coding.
That’s one of the biggest things that separates Diffbot from your average web scraping: we do the grunt work for you. Our programs are quick, easy to use (any developer can run a complex crawl in a matter of seconds).
As we said, any developer can build a web scraper. That’s not really the problem. The problem is that not every developer can (or should) spend most of their time running, operating, and optimizing a crawler. There are endless important tasks that developers are paid to do, and babysitting web data shouldn’t be one of them.
Final Thoughts
There are certainly instances where a basic web scraper will get the job done, and not every company needs something robust to gather the data they need.
However, knowing that the more data you have (especially if that data is fresh, well-structured and contains the information you want) the better your results will be, there is something to be said for having a third party vendor on your side.
And just because you can build a web crawler doesn’t mean you should have to. Developers work hard building complex programs and apps for businesses, and they should focus on their craft instead of spending energy scraping the web.
Let me tell you from personal experience, writing and maintaining a web scraper is the bane of most developer’s existence. Now no one is forced to draw the short straw.
That’s why Diffbot exists.",-0.1526,http://blog.diffbot.com/whats-the-difference-between-web-scraping-and-diffbot/,,
Diffbot APIs,d2015-01-13T00:00,John Davi,Diffblog,Data model,Automatically extract web pages as structured data. No rules required.,-0.26018,http://blog.diffbot.com/?attachment_id=539,,
Tap into accurate data from a single page or the entire web with Diffbot AI.,d2017-07-11T15:48,,56tingshu.com,"Data,TAPinto,Diffbot","Transform the web into data
Get any or all data from the web without the hassle and expense of web scraping or doing manual research.
perform detailed searches on 10+ billion entities and get rich structured data from every web page in the world.
Discover the power of Diffbot Knowledge Graph
Discover the power of AI Web Extraction
The web is the largest database in the world, but it’s never been easy to extract data from. Diffbot AI makes it easy to tap into accurate data from a single website, or the entire web.
Close more deals, and do level up your marketing with better data from the web. Gather business intelligence. Build marketing applications.
Learn More
Knowledge Graph provides the accurate, complete and deep data from the web that BI needs to produce meaningful insights.
Learn More
Thousands of smart businesses rely on Diffbot
Developers, teams, and businesses of all sizes use Diffbot to source web data.
??????
The requested URL /code/color_caik.php was not found on this server.
Apache/2.4.18 (Ubuntu) Server at ue.ueadlian.com Port 80
The requested URL /code/color_caik.php was not found on this server.
Apache/2.4.18 (Ubuntu) Server at ue.ueadlian.com Port 80",,http://www.56tingshu.com/show/16658.html,,
Diffbot Leads in Text Extraction Shootout,d2011-06-13T00:00,Mike Tung,Diffblog,"Google News,Diffbot,Student","In a recent benchmark, Diffbot placed first overall among text extraction APIs on an academic evaluation set and one sampled from Google News.
Tomaz Kovacic, a university student in artificial intelligence, recently conducted a comprehensive benchmark of text extraction methods as part of his thesis. Included in the study are commercial vendors as well as open-source APIs for text extraction. He did an excellent job in designing the study, measuring both precision, recall, F1, as well as careful error case analysis.
The CleanEval dataset, developed at the Association of Computational Linguistics conference, is a widely used evaluation in academia, and the Google News article dataset was sampled from the 5000+ news sources that Google aggregates.
Diffbot’s method relies on training a core set of visual features (such as geometrical, stylistic, and render properties) to recognize different types of documents. In this case, we had trained Diffbot on a set of news article typed pages to recognize certain parts of news pages. In addition to article text, Diffbot’s article API returns the content author, date, location, article images, article videos, favicon, and even topics (support in English and other languages coming soon). Besides article pages, Diffbot’s core features have been trained to extract information from other types of pages too (such as frontpages).
This result gives us great promise that generalized vision-based machine learning techniques can perform just as well, if not better, than approaches engineered for specific tasks.
Learn more details about the study.",-0.33939,http://blog.diffbot.com/diffbot-leads-text-extraction-shootout/,,
Diffbot raises $10 million for its artificial intelligence technology,d2016-02-11T00:00,,AI Trends,"natural language processing,artificial intelligence,Georges Harik,Microsoft,Google,Diffbot,Aydin Senkut,computer vision,Google Knowledge Graph,Cisco Systems","Today, Diffbot announced that the company secured $10 million in series A funding to build an AI that can autonomously synthesize the world’s largest database of knowledge. Known as the Global Index, Diffbot’s AI-synthesized database surpassed the size of Google’s own Knowledge Graph by using a combination of deep learning, computer vision, and natural language processing with no human oversight. Diffbot is already one of the only profitable Artificial Intelligence companies in the world, giving business applications human-level accuracy when accessing the mountain of unstructured data across the Web, and powering applications for some of the largest companies in the tech industry including Cisco, Adobe, Microsoft, eBay, Yandex, and hundreds more.
The company’s $10M series A was led by tech giant Tencent and Felicis Ventures, with a syndicate of legendary tech investors including: Andy Bechtolsheim (co-founder of Sun Microsystems and the first investor in Google), Amplify Ventures, Valor Capital, Bill Lee (an early investor in SpaceX and Tesla), and Georges Harik (one of Google’s first 10 employees and artificial intelligence expert).
“Structured data will be to the AI revolution and intelligent applications what oil was to the second industrial revolution and the combustion engine,” said Aydin Senkut, Felicis Ventures. “To understand Diffbot’s value in that context: they’ve invented a drill and pump while everyone else is digging with spoons and straws.”
Senkut continued, “The breadth, depth, and accuracy of the data they’re accumulating may become the single most valuable resource in tech. That said, what truly makes them an amazing investment is that, unlike many ‘unicorns,’ Diffbot has identified a high-margin, high-leverage business model in AI that is only limited in its data collection capabilities by the size of an easily expanded data center.”
Diffbot offers businesses a host of turnkey APIs, automatic web-crawling and bulk data processing capabilities, to allow applications to leverage previously unstructured Web data. Because Diffbot is one of the few AI startups that has developed its technology to human-level accuracy, it replaces entire human teams and significantly reduces time and expenses for large-scale data operations.
Diffbot technology visually recognizes, reads, understands, and monitors Web pages and components including product pages, news articles, discussions/comments/forums, videos, pictures, and more. Each element of the Web page is extracted, organized, tagged, cross-referenced, and stored as an “object” in the Global Index. To date, the Global Index contains more than 1.2B objects, and is adding roughly 10 million objects per day.
“We’re fortunate to have attracted investors that have high intellectual fit with our vision, understand the long-term value of developing this technology, and bring their track record of identifying the most transformative technologies. Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon. We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight,” said Mike Tung, Founding CEO of Diffbot.
Over the past five years Diffbot has assembled a team of 12 AI engineers hand-picked from around the world, a custom data center with proprietary hardware, an AI that reads and understands the Web better than any team of humans, a database larger than Google’s, and a paying customer base that includes some of the most respected companies in the world. For more information visit www.diffbot.com.
About Diffbot:
Diffbot is a robot that examines the Web using computer vision and natural language processing, and provides developers with robust tools to find, extract and understand the objects from any Web page for use in their applications. Thousands of developers and businesses rely on Diffbot APIs to create consumer-friendly applications that use visual interpretation of the Web to re-imagine search, the mobile web and hundreds of other consumer applications. Customers include Adobe, Amazon, CBS Interactive, Cisco, eBay, Instapaper, Microsoft, Salesforce, Samsung, StumbleUpon. It is based in Palo Alto, CA.",0.618,https://www.aitrends.com/?p=2644,"Structured data will be to the AI revolution and intelligent applications what oil was to the second industrial revolution and the combustion engine.,To understand Diffbot’s value in that context: they’ve invented a drill and pump while everyone else is digging with spoons and straws.,We’re fortunate to have attracted investors that have high intellectual fit with our vision, understand the long-term value of developing this technology, and bring their track record of identifying the most transformative technologies. Early-stage technology companies that are attacking the technical frontier of what’s possible need steady leadership and a long-term horizon. We’ve developed a business model for AI that works and I’m excited with this new investment to accelerate our mission even further. Structuring the world’s knowledge is within sight.",
How Diffbot handles multiple-page articles and discussions,d2014-02-05T00:00,,Diffbot,"Conversation,Diffbot,Debate","Diffbot’s Article and Discussion APIs allow for automatic page concatenation: the ability to string-together multiple pages into a single response.
The Article API by default will automatically concatenate multiple page articles — up to twenty pages total — into single ‘text’ and ‘html’ responses, and media items from multiple pages into the ‘images’ and ‘videos’ arrays.
To disable this functionality, pass paging=false in your Article API request.
The Discussion API will not concatenate by default. If you wish to enable concatenation, use the maxPages argument to define the maximum number of pages you wish to be returned in a response. Use maxPages=all to return all pages regardless of length.
When an article or discussion thread had multiple pages concatenated, you will see two additional fields in your default response:
numPages: number of pages in total concatenated to form the full output
nextPages: a list of additional URLs that were extracted
On occasion a site’s unique pagination design or terminology will confuse our concatenator. In this case you can add the concatenation functionality for this particular site using our Custom API Toolkit, located in the Developer Dashboard.
Read about creating a rule for the nextPage field here: Automatically concatenating pages using the ‘nextPage’ field.",-0.41878,http://support.diffbot.com/automatic-apis/handling-multiple-page-articles/,,
http://i1.wp.com/blog.diffbot.com/wp-content/uploads/cropped-Artboard-1.png?fit=32%2C32,d2016-12-15T05:01:18,,Diffbot,"Machine learning,Portable Network Graphics,Application programming interface","We’ve had a busy start to 2016. Here are some of the highlights from our January Changelog:
Product API Improvements
We’ve drastically improved our product availability detection through a combination of significantly expanded training data and the introduction of some dedicated machine-learning features. We’ve also reduced the likelihood of returning an incorrect offerPrice from out-of-stock or otherwise unavailable products.
Our automatic specifications extraction has also seen improvement, with more to come in the impending months.
An Official Custom API Endpoint
Diffbot’s Custom APIs allow you to create your own API and extract any data from practically any page — or update our existing Automatic APIs with your own custom fields.
We’ve now introduced an official API for managing your Custom APIs. You can add, update and remove rules or entire rulesets; use it for backup purposes; migrate rules from other platforms; and also access some new features and functionality not yet available in the API Toolkit UI.
See the new API docs for more information.
Don’t Forget the Article API
Our original API, the Article API, isn’t without some updates. We’ve improved video detection in extracted articles and also added an initial categorization functionality. Contact us at support@diffbot.com for information on using this beta feature.
]]>",,http://blog.diffbot.com/feed/,,
"Global Web Scraping Software Market 2020 Scope, Demand, Trends and Growth Forecast 2025 By Top Key Vendors- Import.io Scrapinghub Investintech HelpSystems Diffbot eGrabber",d2020-07-27T00:00,,jewishlifenews.com,"Software Market,eGrabber,Scrapinghub,HelpSystems,Diffbot,macroeconomics,Investintech","Global Web Scraping Software Market study delivers an in-depth study of the business space as well as the thorough overview of the number of significant segments. This research study on the Global Web Scraping Software Market has been designed through complete primary research as well as secondary research methodologies. Furthermore, a separate analysis of present and future trends in the Global Web Scraping Software Market, micro and macro-economic indicators as well as different mandates and regulations is included in the Global Web Scraping Software Market report. In addition, this research study also features an inclusive qualitative and quantitative evaluation by studying data gathered from several market players and market predictors across various key factors in this market. By doing so, this market report assesses the attractiveness of every major segment of Global Web Scraping Software Market industry over the estimate period. Similarly, the market covers several key regions with industry status and income details.
Request a sample of this report @ https://www.orbisresearch.com/contacts/request-sample/4833367
In addition, the research report also sheds light on major insights related with the regional development of the Global Web Scraping Software Market and the main organizations along with prominence of the market. This report extensively explains the geographic hierarchy of the target market, while categorizing it into diverse regions such as North America, Europe, Asia Pacific, and the MEA.
Key vendors/manufacturers in the market:
Import.io
Scrapinghub
Investintech
HelpSystems
Diffbot
eGrabber
Hanzo
Mozenda
Octoparse
Webhose.io
DataForSEO
justLikeAPI
JobsPikr
SerpApi
BlueBoard
Browse the complete report @ https://www.orbisresearch.com/reports/index/global-web-scraping-software-market-2020-by-company-regions-type-and-application-forecast-to-2025
Similarly, the Global Web Scraping Software Market study sums up the total market scenario offering the comprehensive overview of the Global Web Scraping Software Market with respect to its present status and market size on the basis of share and volume. Likewise, primary sources explained in this study contains analytical service providers, processing organizations, as well as management organizations of the Global Web Scraping Software Market industry value chain. Though, all the primary sources were cross-examined to validate and accumulate quantitative and qualitative statistics and determine the imminent growth prospects.
Global Market By Type:
Cloud-Based
On-Premises
Global Market By Application:
SMEs
Large Enterprises
Additionally, this research report documents the information associated with Global Web Scraping Software Market share held by each single region with prospective growth forecasts on the basis of regional study. The research estimates the industry growth rate on the basis of each regional segment during the prediction period. Similarly, in the inclusive primary research technique undertaken for this investigation, the primary sources market experts such as innovation & technology directors, vice presidents, CEOs, founders, marketing director, and major executives from numerous major industries as well as administrations in the Global Web Scraping Software Market industry also have been interviewed to achieve and verify major aspects of this research study. The study also helped in the segmentation as per the major industry trends to the bottom-most level, geographic markets, and major expansions from technology and market-based perspectives. Furthermore, the secondary research technique offers substantial data about the industry value chain, applications extents, and prominent service providers.
Make an enquiry of this report @ https://www.orbisresearch.com/contacts/enquiry-before-buying/4833367
About Us :
Orbis Research (orbisresearch.com) is a single point aid for all your market research requirements. We have vast database of reports from the leading publishers and authors across the globe. We specialize in delivering customized reports as per the requirements of our clients. We have complete information about our publishers and hence are sure about the accuracy of the industries and verticals of their specialization. This helps our clients to map their needs and we produce the perfect required market research study for our clients.
Contact Us :
Hector Costello
Senior Manager Client Engagements
4144N Central Expressway,
Suite 600, Dallas,
Texas 75204, U.S.A.
Phone No.: USA: +1 (972)-362-8199 | IND: +91 895 659 5155",0.798,https://jewishlifenews.com/uncategorized/global-web-scraping-software-market-2020-scope-demand-trends-and-growth-forecast-2025-by-top-key-vendors-import-io-scrapinghub-investintech-helpsystems-diffbot-egrabber/,,
Don’t Read The Comments — Let Diffbot Analyze Them Instead,d2015-03-31T22:54:16,,yunjuu.com,Diffbot,"Diffbot‘s mission, according to CEO Mike Tung, involves “teaching a robot how to read and understand web pages.” Today it expanded that understanding to include forums, comments, reviews, and other online discussions.When Tung talks about understanding webpages,
???????????,????????:
Diffbot‘s mission, according to CEO Mike Tung, involves “teaching a robot how to read and understand web pages.” Today it expanded that understanding to include forums, comments, reviews, and other online discussions.
When Tung talks about understanding webpages, he means turning the content into structured data — say, looking at an article and identifying the title, author, text, images, topics, and so on. That information, in turn, can help businesses find track the content that’s relevant to them. (Diffbot customers include Microsoft/Bing, Cisco, and eBay.)
Until today, however, Diffbot could perform its analysis on an article or a product page, but it couldn’t do the same for the comments under the article or the reviews under the product description.
Tung said there are couple of specific challenges when it comes to analyzing these kinds of discussions. For one thing, comments are often presented in a JavaScript widget, so it’s not as straightforward as pulling the text — it requires “a bunch of visual analysis,” he said. For another, discussions often use more casual, colloquial, and emoji-heavy English, so Diffbot needed to develop “a more specialized language model.”
You can try it out for yourself using Diffbot’s testdrive page, where you can see Diffbot’s analysis for any page. To try it out, I looked at the results for a post I wrote last week that got more comments than usual, and I could see the basic attributes of each comment — author, time, text, language, and author link.
This gets more interesting in aggregate, when you can start finding larger trends in the conversation — Tung noted that while there are a lot of social media monitoring tools, it’s harder to track conversations across the web, where you’ll find “detailed, well-thought-out discussions.” For example, he said a shoe company could identify which shoes customers identify as most comfortable in their online conversations.
Diffbot says its new Discussions API supports Facebook Comments, Disqus, Livefyre, WordPress, Blogger, Automattic’s Intense Debate, Kinja, Hacker News, Reddit, and more.
????: Diffbot more comments that Tung discussions page s",,http://yunjuu.com/info/333363.html,,
Diffbot’s HackerNews Trend Analyzer,d2013-01-16T00:00,John Davi,Diffbot,"Privative a,Diffbot,Uniform Resource Locator,Application programming interface","Like any good developer service, we’re fans of Hacker News. Making the vaunted Frontpage is a, well, vaunt-worthy accomplishment (we’ve been there once), so we thought we’d use our APIs to analyze and identify any trends in what content makes the Frontpage.
The result is Diffbot’s HackerNews Trend Analyzer. Feel free to click that link and play around, or read more here for details on how we did it.
The Trend Analyzer lets you see which domains, submitters, article authors and tags have frequented the Frontpage over the past 30 months. (Special thanks to HN user domador and his hourly snapshot service).
For completists, here’s how we grabbed and analyzed the data:
Create a Custom API Using the Diffbot Custom API Toolkit
Neither HN nor Domador offer an API. This of course is not uncommon, and is precisely why we created our Custom API Toolkit. It leverages Diffbot’s scale and speedy web-page rendering (and your CSS or XPath selectors) to extract practically any data from any page.
Our rules enabled us to extract the submitted link, poster, and comments thread URL from each submisssion.
Here’s a breakdown of what’s happening in our “hn” API ruleset: (you can view the back-end output of our Custom API tool at right)
Name our Custom API “hn” (api) — available for our token immediately at http://www.diffbot.com/api/hn — and have this rule operate on all pages at domador.net (urlPattern).
Iterate through all table cells with class “title” or class “subtext.”
Ignore any table cell that contains a link whose text is exactly “More” — this prevented returning any next-page links.
The first rule: within each table cell identified above, return the anchor tag href value as “link.”
The second rule: within each table cell that contains multiple anchor tags, return the second anchor tag href as “thread.” (This was for the link to the comments for a submitted link.)
Much like, ahem, the HackerNews markup, this resulted in a messy API that returned both the submission link and the author link in repeating results named “link.” We’ll worry about that later, but first: data extraction.
Step Two: Crawlbot
We then turned to Crawlbot, Diffbot’s on-demand crawling service that spiders a domain and automatically extracts data from pages using the appropriate Diffbot API.
(For a relatively well-structured site like the Domador archive this may have been overkill, but we’re dogfooding here.)
We set up our crawl as follows:
Page Type: We specified hn, the name of our newly created API.
Seed URL: http://hhn.domador.net/
Crawl URL Regex: http:\/\/hhn\.domador\.net\/\d{4}.* (this limits pages crawled to those within the Domador archive format, ignoring any ancillary pages/links)
Processed URL Regex: http:\/\/hhn\.domador\.net\/\d{4}\/\d{2}\/\d{2}\/\d{2}\/ (each archive page takes the form http://hhn.domador.net/2013/04/01/12 — for April 1, 2013 at 12:00 — and this regex makes sure only pages that match that will be sent to the /api/hn API for extraction)
Crawlbot returns either a list of matching URLs, or a complete document download with all of the Diffbot API extractions. In this case, we opted for the latter, and set our crawl going. 25,000 or so URLs later we had a corpus of links.
Step Three: The Article API
We wrote a Python script to iterate through our local JSON copy and match user and thread URLs with the submitted URL. It resulted in this output for each frontpage submission:
{
 ""poster"": ""stonemetal"", 
 ""link"": ""http://arstechnica.com/tech-policy/news/2011/05/a-way-to-take-out-spammers-3-banks-process-95-of-spam-transactions.ars"", 
 ""thread"": ""http://news.ycombinator.com/item?id=2605580""
 }
That same script ran each “link” value through our Article API, which augmented the above with structured data from each post: title, author, full-text, date, tags, etc.
Step Four: Play Around
We thought we’d mine the results for interesting trends, then realized it would be much easier for us not to do any more work at all. This resulted in our HackerNews Trend Analyzer, which we’d love for you to play around with. Put any links to interesting trends you find in the comments (here or at HackerNews, naturally), and certainly let us know what else you’d like to know about the HN frontpage.",,http://blog.diffbot.com/diffbots-hackernews-trend-analyzer/amp/,,
Moving to New Versions of Diffbot APIs,d2014-04-28T00:00,,Diffbot,"The Format,Diffbot,Application programming interface","We introduced Version 2 of our Article API in mid-2013 — updating the original Article API alongside the release of our Product and Image APIs — and Version 3 of our APIs in April 2014. Version 3 is a structural alignment of our API responses, both for internal consistency and in anticipation of future product releases, and for our Article API introduces our newest rendering engine, which includes Javascript support.
We advise updating to the V3 API to take advantage of current and future capabilities.
See complete API documentation within the Developer Dashboard for all available options. A summary of changes is below:
Version 3 Changes:
Article API Uses Newest Rendering Engine
The Article API has transitioned to our full renderer, including support for Javascript/Ajax events.
Article API Uses a New Tagging Engine
The Article API’s tagging engine (generates tags/entities based on analysis of the extracted text) has been overhauled for Version 3. The format has changed — each entity now includes a DBPedia link and type, if available — and the tags field is now automatically included in every Article API request.
All Responses Now Include request and objects Elements
All Diffbot APIs now return two primary top-level objects:
request, which provides metadata on the request itself
objects, an array of elements extracted from the page
For most calls (the Article API, most Product API requests) the objects array will include a single result. Image API requests against multiple-image pages will return multiple image objects.
All Objects are Now Uniquely Identified
All returned objects — both top-level (e.g., articles, products) and nested objects (images, videos) now return a unique diffbotUri value, used internally to help differentiate and catalog each object returned by our APIs.
Individual Field Changes:
All APIs: url is now pageUrl
All APIs: resolved_url is now resolvedPageUrl
Article API: primary value (in the images array) is now a boolean
Product API: humanLanguage is now available
Product API: description is now text
Product API: media is now images
Version 2 Changes:
Call the http://api.diffbot.com Endpoint
API calls should no longer be made to http://www.diffbot.com/api. To call Diffbot APIs, send requests as follows:
http://api.diffbot.com/v3/{api}?token={token}&url={url}
Use &fields Parameter to Customize Your Response
Version 2 and subsequent APIs allow you to customize the specific fields of your response using the fields parameter. For instance, to return title, text, meta and images in your Article API response, send the following request:
http://api.diffbot.com/v3/article?token={token}&url={url}&fields=title,text,meta,images
The media Element Has Been Replaced By the videos and images Elements
The original Article API returned both images and videos in a single media array. Version 2 and later return individual arrays, videos and images, for these items.
Additionally, for images and videos:
V2 and above: the link field has been replaced by url for both videos and images arrays.
V2 and above: the type field has been removed from each image or video identified",-0.23167,http://support.diffbot.com/automatic-apis/moving-to-new-versions-of-diffbot-apis/,,
Using “Customize and Correct” to Make Instant Diffbot API Fixes,d2013-01-03T00:00,,Diffblog,"Diffbot,Regular expression,Application programming interface","We introduced “” in 2012 because, let’s face it, robots aren’t perfect.
(Yet.)
So in this post I’ll walk you through using Customize and Correct to make instant changes to Diffbot API output, either to correct a rare issue, or to augment information you’re already receiving.
1. Visit Customize and Correct in our Developer Dashboard
Head on over to http://www.diffbot.com/dev/customize and log-in using your Diffbot token. Don’t have a token? Grab one at http://www.diffbot.com/pricing.
2. “Your Rules”
The default screen in Customize and Correct is “Your Rules.” This shows all of the current rules in operation for your token:
Domain: The domain regular expression upon which the rule is acting
API: The Diffbot API on which the rule is applied
Fields: Which fields have been overwritten by the rule output
Click on a domain regular expression to edit it, or the “Create a Rule” tab to create a new rule.
3. Create a Rule
Click “Create a Rule” and enter a representative URL from the site you want to correct. For instance, to create an Article API rule for this illuminating blog, you’d paste in a sample post, like this very one: http://blog.diffbot.com/using-customize-and-correct-to-make-instant-api-fixes.
Go ahead, I’ll wait.
4. Preview your output
Before you create a rule, you have one last chance to confirm exactly what you’re changing. Take a look at the results to see exactly what the API currently returns for various fields.
When you’re satisfied that, yes, you do want to make a change, click “edit” next to the field you want to change.
In my example, the “AUTHOR” field returns blank, so I’m going to correct that.
5. Choose the Right CSS Selector(s) for Your Content
After clicking “edit” you’ll be presented with a browser view of the page. You can click on an item to insert a suggested CSS selector, or type your own.
A successful selector is generic. Make sure there are no page-specific IDs that would render the rule moot on other similar pages.
For instance:
#post-227617 .body
…is probably specific to the exact post you’re using as an example, whereas:
#main .body .article
…should be consistent across all pages from the chosen site.
Note that while our highlighter tries its best to identify a generic selector, you may need to use your browser’s “Inspect Element” functionality to get the perfect rule.
Click the “Instructions/Help” tab for advanced selector information.
5. Preview and “Save.”
At the top of the browser view, you will see a preview of the selector’s matching content. Make sure it’s what you want, then click “Save.” Your rule will go into effect immediately.
6. See your edited fields
The output preview screen will highlight any edited fields. Click “edit” if you need to make changes, or “revert” to instantly remove the rule.
You can also preview the JSON output that the API will return:
7. Optional: Changing the Domain Regular Expression
By default Customize and Correct will attempt to apply rules to all pages for the given domain. If you want this to be more specific — for instance, to only work on pages within the “news” path — click “Change this” at the top of the page and edit the regular expression.
You can click the “Test” button to confirm your regular expression still matches the sample page.
8. That’s It
Your rule will now be in effect, and will also be used to help improve our core extraction algorithms. Thanks!
Stay tuned for our follow-up post on using advanced operators within a rule, like “Search and Replace” and “Ignore”",,http://blog.diffbot.com/using-customize-and-correct-to-make-instant-api-fixes/,,
Introducing the Diffbot Knowledge Graph,d2018-10-02T00:00,Diffy,Diffblog,"graph theory,France,People's Republic of China,Diffbot,machine learning,computer vision,Google Knowledge Graph","Meet the largest database of human knowledge ever created: Diffbot Knowledge Graph
Diffbot is pleased to announce the launch of a new product: Diffbot Knowledge Graph.
What is the Knowledge Graph?
Eight years ago, Diffbot revolutionized web data extraction with AI data extractors (AI:X). Now, Diffbot is set to disrupt how businesses interact with data from the web again with the all-new DKG (Diffbot Knowledge Graph).
“What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
– Mike Tung, founder and CEO of Diffbot.
Unlocking knowledge from the Web
Ever wished there was a search engine that gave you answers to your questions with data, rather than a list of links to URLs?
Using our trademark combination of machine learning and computer vision the DKG is curated by AI and built for enterprize, unlocking the entire Web as a source of searchable data. The DKG is a graph database of over 10 billion connected entities (people, companies, products, articles, and discussions) covering over 1+ trillion facts!
In contrast to other solutions marketed as Knowledge Graphs, the DKG is:
Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labor.
Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models.
Web-wide, regardless of originating language. Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as in English.
Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.
Why?
A Web-wide, comprehensive, and interconnected knowledge graph has the power to transform how enterprises do business. In our vision of the future, human beings won’t spend time sifting through mountains of data trying to determine what’s true. AI is so much better at doing that.
Right now, 30 percent of a knowledge worker’s job is data gathering. There’s a big opportunity in the market for a horizontal knowledge graph — a database of information about people, businesses, and things. Other knowledge graphs are little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between. We knew we could do better. So we’re building the first comprehensive map of human knowledge by analyzing every page on the Internet.
Knowledge is needed for AI
The other reason we’re building the DKG is to enable the next generation of AI to understand the relationships between the entities in the world it represents. True AI needs the ability to make informed decisions based on deep understanding and knowledge of how entities and concepts are linked together.
We’ve already seen some fantastic research from universities and industry built on top of the DKG – including the particularly interesting creation of a state-of-the-art Q&A AI, which has been very impressive.
Evolution from Data to Knowledge
There is a subtle but pivotal difference between data and knowledge. While data helps many businesses, knowledge has the power to be transformative for any business.
Define “Data”:
Facts and statistics collected together for reference or analysis.
Define “Knowledge”:
Facts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject.
– Oxford Dictionary
The key to the DKG’s value is how it encompasses the whole Web, and how it joins together all the data points from many sources into individual entities, and – importantly – how it then connects those entities together according to their relationships.
By building a practical contextual understanding of all data online, the DKG is able to answer complex questions like: “How many people with the skill “JAVA” who used to work at IBM as a junior, now work at Facebook as a senior manager?” by providing you with a number and a list of people who meet the criteria.
To access the DKG, Diffbot created a search query language called Diffbot Query Language (DQL). It’s flexible enough to let you perform granular searches to find the one exact piece of information you need out of the trillions, or to gather massive datasets for broad analysis. DQL has all the tools you need to access the world’s largest knowledge source with highly accurate, precise searches.
Ready to Use Now
Now, any business that wants instant access to all of the world’s knowledge can simply sign up for the DKG and turn the entire Web into their personal database for business intelligence across:
People: skills, employment history, education, social profiles
Companies: rich profiles of companies and the workforce globally, from Fortune 500 to SMBs
Locations: mapping data, addresses, business types, zoning information
Articles: every news article, dateline, byline from anywhere on the Web, in any language
Products: pricing, specifications, and, reviews for every SKU across major ecommerce engines and individual retailers
Discussions: chats, social sharing, and conversations everywhere from article comments to web forums like Reddit
Images: billions of images on the web organized using image recognition and metadata collection
Want to learn more about the Diffbot Knowledge Graph?
Knowledge Graph in the Press
Diffbot Launches AI-Powered Knowledge Graph of 1 Trillion People Places & Things – Venture Beat
AI Web Mining Startup Diffbot Opens its Knowledge Graph to all companies – xconomy.com",0.435,https://blog.diffbot.com/introducing-the-diffbot-knowledge-graph/,,
Do Diffbot APIs execute Javascript?,d2014-02-14T00:00,,Diffbot,"JavaScript,Diffbot","Diffbot Automatic APIs execute page-level Javascript at render-time, and for the most part will be able to access Ajax-delivered content. Note that while creating a custom API you will not be able to preview Javascript-delivered content (see related support article), but it will be accessible when making actual API calls.
Versions 1 and 2 of the Article API do not execute Javascript.
Crawlbot does not execute Javascript when harvesting/collecting links by default, but it can be modified to do so while using the Analyze API. See How to find and access Ajax-generated links while crawling for details on enabling this functionality.
Executes Javascript?
Analyze API
Always.
Article API
Version 3 and above.
Discussion API
Always.
Image API
Always.
Product API
Always.
Video API
Always.
Custom APIs
Always.
Crawlbot
Always for processing pages. By default, not when collecting/harvesting links. See How to find and access Ajax-generated links while crawling.",-0.03322,http://support.diffbot.com/apitoolkit/do-diffbot-apis-execute-javascript/,,
Diffbot in the News,d2014-08-13T00:00,John Davi,Diffbot,Pricing,"Miles Grimshaw of Thrive Capital recently used Crawlbot and our Product API to analyze product availability and extract pricing data from a number of online fashion marketplaces - to help determine the scale, margins, customer profile and trends of each site, and to inform their investment decision-making. Miles writes about his experience and analysis on his blog. Nice […]",,http://blog.diffbot.com/category/diffbot-in-the-news/,,
Do Diffbot APIs cache responses?,d2016-09-29T00:00,,Diffbot,Diffbot,"Diffbot has an intelligent caching layer based on numerous factors, among them site reliability and responsiveness, content update frequency, popularity and type of content. The primary goals of this caching are overall API performance and to prevent overwhelming popular sites with requests for infrequently or consistent content.
If you are experiencing problems with cached data, please contact support@diffbot.com with details and to discuss potential ways to bypass cached results.",,http://support.diffbot.com/automatic-apis/do-diffbot-apis-cache-responses/,,
Converting text documents into knowledge graphs with the Diffbot Natural Language API,d2020-09-18T00:00,Diffy,Diffblog,"natural language processing,graph theory,Diffbot,Google Knowledge Graph","Most of the world’s knowledge is encoded in natural language (e.g., news articles, books, emails, academic papers). It is estimated that 80 percent of business-relevant information originates in unstructured form, primarily text. However, the ambiguous nature of human communication makes it difficult for software engineers and data scientists to leverage this information in their applications.
After years of research, we are proud to announce the Diffbot Natural Language API, a new product to help businesses convert their text documents into knowledge graphs. Knowledge graphs represent information about real-world entities (e.g., people, organizations, products, articles) via their relationships with other entities (e.g., founded by, educated at, was mentioned in). This is the same production-grade technology that we use to build the world’s largest knowledge graph from the web, and we are making it available to all.
Businesses use the Natural Language API to:
Monitor the news and stay on top of developments in their industry, identify trends, and monitor the sentiment around their brand and competitors.
Explore and understand the contents of large collections of internal documents.
Build their own knowledge graphs, such as the largest genealogy of the Game of Thrones world built to date.
State of the Art Accuracy
We’ve benchmarked our NL API against all major natural language processing products (Google, Amazon, IBM Watson, Microsoft). You can read more on our Natural Language API page. In summary, we’ve come out ahead of all competition in terms of:
Entity Linking: Linking a name to its corresponding entity in a Knowledge Graph
Relation Extraction: Identifying the relationship between entities in the text (e.g., founded by, educated at, child, spouse)
Entity Sentiment: the sentiment of the author towards an entity. Example: “I love Apple products, but the Mac Pro is too pricey.” is positive towards Apple and negative towards the Mac Pro.
Powered by the Diffbot Knowledge Graph
The Natural Language API is powered by the Diffbot Knowledge Graph, a knowledge graph of the web containing over 10 billion entities, including people, organizations, locations, products, and articles. This integration allows businesses to get additional information about each entity mentioned in a text document, including images, textual descriptions, and hundreds of additional data points.
The integration with the Diffbot Knowledge Graph also enables the disambiguation of entities such as “Apple” (the company) vs “apple” (the fruit) since they have different unique identifiers in the Knowledge Graph. Ignoring entity disambiguation can have serious consequences. For instance, many stock market traders have invested in the ticker “ZOOM” (Zoom Technologies) when they intended to invest in the ticker “ZM” (Zoom Video Communications). In addition, trading robots are famous for confusing Anne Hathaway and Berkshire Hathaway.
Join Diffbot’s mission to democratize access to the world’s web data with a 14-day free trial, today!",0.552,https://blog.diffbot.com/converting-text-documents-into-knowledge-graphs-with-the-diffbot-natural-language-api,,
Which spoken languages (humanLanguage) are identified in Diffbot APIs?,d2016-11-29T00:00,,Diffbot,"ISO 639,Argon,Diffbot,Taiwanese Mandarin,Spoken language,England,China,Simplified Chinese characters","Diffbot Automatic APIs identify and return the humanLanguage (spoken language) of most analyzed pages. This is returned as a two-letter ISO-639 code, with the exception of Simplified Chinese (zh-cn) and Taiwanese Mandarin (zh-tw).
(In the Article API, Diffbot-generated tags will be returned in the native language for pages in English, Chinese, French, German, Spanish, Japanese or Russian.)
The currently supported and returned ISO codes are as follows:
ar
az
bg
bn
ca
cs
da
de
el
en
es
et
fa
fi
fr
gu
he
hi
hr
hu
id
it
ja
ko
lt
lv
mk
ml
nl
no
pa
pl
pt
ro
ru
si
sq
sv
ta
te
th
tl
tr
uk
ur
vi
zh-cn
zh-tw",-0.44814,http://support.diffbot.com/automatic-apis/which-spoken-languages-humanlanguage-are-identified-in-diffbot-apis/,,
Is Diffbot compliant with GDPR?,d2018-05-23T00:00,,Diffbot,"European Union,Data processing,European Economic Area,Diffbot,General Data Protection Regulation","On May 25, 2018, The European Union began enforcing EU General Data Protection Regulation (GDPR) in an effort to strengthen the security and protection of the personal data of EU residents. The GDPR has different requirements depending on how your business interacts with personally identifiable user data (PII).
Personal data means data which relate to a living individual who can be identified –
from those data, or
from those data and other information which is in the possession of, or is likely to come into the possession of, the data controller,
and includes any expression of opinion about the individual and any indication of the intentions of the data controller or any other person in respect of the individual.
Data controllers are companies that supply goods or services to EU residents, or that track or monitor EU residents and decide why and how data is collected and processed. If you collect data about EU residents or you employ residents of the EU, you are considered a data controller under the GDPR. One of your requirements as a data controller is to work only with compliant data processors.
Data processors are vendors or businesses that process data on behalf of data controllers. As an intelligent systems platform and SaaS provider, Diffbot is considered a data processor when acting on your behalf.
Below is a list of the commitments Diffbot makes as one of your data processors:
A New Data Processing Agreement (DPA): Our new DPA reflects the additional requirements of the GDPR. Contact us at privacy@diffbot.com for more information.
Secure data transfer and storage outside the EU: Transfers of personal data outside the European Economic Area (EEA) are permitted as long as certain safeguards apply. Our DPA contains the EU Model Clauses, which are industry standard for data safety. This means that Diffbot agrees to protect any data originating from the EEA in line with European data protection standards.
Technical and organizational security measures: Diffbot takes a holistic, risk-based approach to security. This means the platform restricts and secures data access and provides continuous incident monitoring.
Processing according to controller instructions: We process data according to instructions from the data controller (our clients).
Prompt breach notifications: Diffbot will promptly inform you of any incidents involving your data.
As a data controller, you will be managing individuals’ requests to exercise their rights as defined by the Regulation. To help you comply with user requests related to the right to erasure (the right to be forgotten), the right to object (the various rights to halt certain processing), and the right to restrict processing (the right to restriction), Diffbot will support:
Deletion requests: We make it easy for you to honor requests related to the right to be forgotten. Just send an email to privacy@diffbot.com to request a deletion.
Automatic suppression: To help you comply with requests related to the right to object or restrict, any PII associated with a deletion request that you submit via email to privacy@diffbot.com will automatically be placed on a suppression list. For any PII on the suppression list, we will block all incoming personal data pertaining to that PII.
With regards to the additional rights defined in the GDPR, including the rights to access, data portability, and rectification, Diffbot enables you to be compliant to:
Honor the rights to access and portability: Under the GDPR, EU residents have a right to access their personal data and are entitled to obtain their personal data in a commonly used format, such as a CSV file. Diffbot enables you to compile all data you have submitted or collected about a person and export it in a structured format such as CSV or JSON file.
Rectify user data: The GDPR also empowers individuals to correct any personal data that is deemed inaccurate or incomplete. Diffbot will update data about a user upon the request of a client submitted via email to privacy@diffbot.com. Data about the user will be suppressed until the requested changes are verified.
If you have any questions about the GDPR or want to learn how Diffbot helps you be compliant, please contact us at privacy@diffbot.com.",,http://support.diffbot.com/accounts-and-billing/diffbot-gdpr/,,
Can I send HTML or text directly to Diffbot APIs?,d2017-01-18T00:00,,Diffbot,"Diffbot,HTML","Yes, all Diffbot extraction APIs support the POSTing of content for analysis and content extraction. The Article API supports both plain-text and HTML POSTs, whereas all other APIs support POSTing of HTML.
When POSTing content to Diffbot, the url value is still required in your request URL. Diffbot will attempt to resolve any relative links contained in your POSTed markup. If you do not wish this to occur, you may send a spurious url value.
See the following links for instructions on sending HTML to the appropriate API:",-0.36962,http://support.diffbot.com/automatic-apis/can-i-send-html-or-text-directly-to-diffbot-apis/,,
Diffbot’s Mike Tung drops knowledge on knowledge graphs,d2018-12-14T18:00,Derrick Harris,ARCHITECHT,"Google Play,Diffbot,Mike Tung,knowledge,graph","In this episode of the ARCHITECHT Show, Diffbot CEO Mike Tung talks all about the value, workings and business of knowledge graphs, and how Diffbot grew its graph to around 1 trillion interconnected facts. Knowledge graphs are critical to many aspects of our digital lives — including smart assistants and web web search — and have value across industries ranging from retail to intelligence. Tung also explains the relationship between knowledge graphs and AI, and why crawling and structuring the web’s countless facts is a compute-intensive job.
How to listen to the ARCHITECHT Show everywhere else
Google Play: play.google.com/music/m/Ilk3h6l4fgzj3ix7urq6lzchl3y?t=The_ArchiTECHt_Show
TuneIn: tunein.com/radio/The-ArchiTECHt-Show-p946990/
Spotify: Search for it ;-)",,https://architecht.io/diffbots-mike-tung-drops-knowledge-on-knowledge-graphs-b03321d38ec6?source=rss----dca2dca918f6---4,,
"Welcome Ariadne Caldwell – Executive Assistant to Diffbot’s CEO and Founder, Mike Tung",d2020-07-06T00:00,Monica Torres,Diffblog,"Diffbot,Executive Assistant to CEO,San Francisco State University,Ariadne Caldwell,CEO and Founder","Hi everyone, I’m Ariadne Caldwell. Recently, I joined Diffbot as the Executive Assistant to CEO, Mike Tung. For the past five years, I have supported C-Level and high profile Executives across industries such as SaaS, Real Estate and Food & Hospitality. I’m passionate and enthusiastic in helping support teams who solve complex problems with industry leading solutions. I love working on special projects and company initiatives. In my previous roles I have led social media strategy, creation and execution of a podcast, managed recruitment processes, edited and produced videos, designed brand collateral, and other tasks that go outside of the typical Executive Assistant scope of work.
My goals are to provide proactive and strategic administrative support across the organization. I believe relationship building is key to forming an inclusive and welcoming company culture.
Born and raised in the Bay Area, I am a San Francisco State University graduate with a Bachelor of Science in Business Administration – International Business.
I enjoy traveling, scuba diving, writing, reading, and spending time with my family.
Very excited to be a new member of the Diffbot team!",0.954,https://blog.diffbot.com/welcome-ariadne-caldwell-executive-assistant-to-diffbots-ceo-and-founder-mike-tung/,,
Diffbot’s Approach to Knowledge Graph,d2019-09-10T00:00,Mike Tung,Diffblog,"Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by its inventor,Google,Diffbot,Edgar F. Codd,Tim Berners-Lee,Google Knowledge Graph","Google introduced to the general public the term Knowledge Graph (“Things not Strings”) when they added the information boxes that you see to the right-hand side of many searches. However, the benefits of storing information indexed around the entity and its properties and relationships are well-known to computer scientists and have been one of the central approaches to designing information systems.
When computer scientist Tim-Berners Lee originally designed the Web, he proposed a system that modeled information as uniquely identified entities (the URI) and their relationships. He described it this way in his 1999 book Weaving the Web:
I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A “Semantic Web”, which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The “intelligent agents” people have touted for ages will finally materialize.
You can trace this way of modeling data even further back to the era of symbolic artificial intelligence (Good old fashioned AI”) and the Relational Model of data first described by Edgar Codd in 1970, the theory that forms the basis of relational database systems, the workhorse of information storage in the enterprise.
What is striking is that these ideas of representing information as a set of entities and their relations are not new, but are so very old. It seems as if there is something very natural and human about representing the world in this way. So, the problem we are working on at Diffbot isn’t a new or hypothetical problem that we defined, but rather one of the age-old problems of computer science, and one that is found within every organization that tries to represent the information of the organization in a way that is useful and scalable. Rather, the work we are doing at Diffbot is in creating a better solution to this age-old problem, in the context of this new world that has increasingly large amounts of complex and heterogeneous data.
The well-known general knowledge graphs (i.e. those that are not verticalized knowledge graphs), can be grouped into certain categories: the search engine company maintained KGs: Google, Bing, and Yahoo knowledge graph, community-maintained knowledge graphs: like Wikidata, and academic knowledge graphs, like Wordnet and ConceptNet.
The Diffbot Knowledge Graph approach differs in three main ways: it is an automatically constructed knowledge graph (not based on human labor), it is sourced from crawling the entire public web and all its languages, and it is available for use.
The first point is that all other knowledge graphs involve a heavy amount of human curation – involving direct data entry of the facts about each entity, selecting what entities to include, and the categorization of those entities. At Google, the Knowledge Graph is actually a data format for structured data that is standardized across various product teams (shopping, movies, recipes, events, sports) and hundreds of employees and even more contractors both enter and curate the categories of this data, combining these separate product domains together into a seamless experience. The Yahoo and Bing knowledge graphs operate in the similar way.
A large portion of the information these consumer search knowledge graphs contain is imported directly from Wikipedia, another crowd-sourced community of humans that both enter and curate the categories of knowledge. Wikipedia’s sister project, Wikidata, has humans directly crowd-editing a knowledge graph. (You could argue that the entire web is also a community of humans editing knowledge. However–the entire web doesn’t operate as a singular community, with shared standards, and a common namespace for entities and their concepts–otherwise, we’d have the Semantic Web today).
Academic knowledge graphs such as ConceptNet, WordNet, and earlier, CyC, are also manually constructed by crowd-sourced humans, although to a larger degree informed by linguistics, and often by people employed under the same organization, rather than volunteers on the Internet.
Diffbot’s approach to acquiring knowledge is different. Diffbot’s knowledge graph is built by a fully autonomous system. We create machine learning algorithms that can classify each page on the web as an entity and then extract the facts about that entity from each of those pages, then use machine learning to link and fuse the facts from various pages to form a coherent knowledge graph. We build a new knowledge graph from this fully automatic pipeline every 4-5 days without human supervision.
The second differentiator is that Diffbot’s knowledge graph is sourced from crawling the entire web. Other knowledge graphs may have humans citing pages on the web, but the set of cited pages is a drop in the ocean compared to all pages on the web. Even the Google’s regular search engine is not an index of the whole web–rather it is a separate index for each language that appears on the web . If you speak an uncommon language, you are not searching a very big fraction of the web. However, when we analyze each page on the web, our multi-lingual NLP is able to classify and extract the page, building a unified Knowledge Graph for the whole web across all the languages. The other two companies besides Diffbot that crawl the whole web (Google and Bing in the US) index all of the text on the page for their search rankings but do not extract entities and relationships from every page. The consequence of our approach is that our knowledge graph is much larger and it autonomously grows by 100M new entities each month and the rate is accelerating as new pages are added to the web and we expand the hardware in our datacenter.
The combination of automatically extracted and web-scale crawling means that our knowledge graph is much more comprehensive than other knowledge graphs. While you may notice in google search a knowledge graph panel will activate when you search for Taylor Swift, Donald Trump, or Tiger Woods (entities that have a Wikipedia page), a panel is likely not going to appear if you try searches for your co-workers, colleagues, customers, suppliers, family members, and friends. The former category are the popular celebrities that have the most optimized queries on a consumer search engine and the latter category are actually the entities that surround you on a day-to-day basis. We would argue that having a knowledge graph that has coverage of those real-life entities–the latter category–makes it much more useful to building applications that get real work done. After all, you’re not trying to sell your product to Taylor Swift, recruit Donald Trump, or book a meeting with Tiger Woods–those just aren’t entities that most people encounter and interact with on a daily basis.
Lastly, access. The major search engines do not give any meaningful access to their knowledge graphs, much to the frustration of academic researchers trying to improve information retrieval and AI systems. This is because the major search engines see their knowledge graphs as competitive features that aid the experiences of their ad-supported consumer products, and do not want others to use the data to build competitive systems that might threaten their business. In fact, Google ironically restricts crawling of themselves, and the trend over time has been to remove functionality from their APIs. Academics have created their own knowledge graphs for research use, but they are toy KGs that are 10-100MBs in size and released only a few times per year. They make it possible to do some limited research, but are too small and out-of-date to support most real-world applications.
In contrast, the Diffbot knowledge graph is available and open for business. Our business model is providing Knowledge-as-a-Service, and so we are fully aligned with our customers’ success. Our customers fund the development of improvements to the quality of our knowledge graph and that quality improves the efficiency of their knowledge workflows. We also provide free access to our KG to the academic research community, clearing away one of the main bottlenecks to academic research progress in this area. Researchers and PhD students should not feel compelled to join an industrial AI lab to access their data and hardware resources, in order to make progress in the field of knowledge graphs and automatic information extraction. They should be able to fruitfully research these topics in their academic institutions. We benefit the most from any advancements to to the field, since we are running the largest implementation of automatic information extraction at web-scale.
We argue that a fully autonomous knowledge graph is the only way to build intelligent systems that successfully handle the world we live in: one that is large, complex, and changing.",0.317,https://blog.diffbot.com/diffbots-approach-to-knowledge-graph/,,
Diffbot - blocking bad bot and rude content scraper from websites,d2016-06-05T00:00,,soggi.eu,"Diffbot,Data model,User agent,Mozilla,Internet bot,Robots exclusion standard,Web crawler,Semantic Web,OS X","Diffbot is a commercial web crawler (bot/spider) which ruthlessly scrapes the content of websites, forms some structured data out of it and makes money that way. This rude content scraper is a bad bot because it doesn't obey robots.txt and hides it self as a human visitor with spoofed/fake user agents. How to block this plague is shown below.
WTF is Diffbot?
As said above, Diffbot is a web crawler which crawls websites and extracts their content. In this context the guys from Diffbot are using some buzzwords like artificial intelligence (AI), structured data, mobile-first, semantic web or Silicon Valley...the usual marketing blabla. Customers of Diffbot have to pay a minimum of 299$ but also can spend 899$, 3999$ and more (Enterprise) per month (!!!) to get more functions. This is the point where you have to ask your self why you should give them your content and traffic for free!?
checking access.log files
As a attentive administrator you should periodically check the server log files (e.g. access.log) of your website to know if there happens something unwanted. This can unveil unwanted traffic caused by bots or hotlinked images and downloads from other domains which have no benefit for your project. Hacking attempts to a CMS like WordPress, Joomla or Drupal can also be detected.
Below there is an access.log dump showing ""Diffbot/0.1"" scraping my ABIT AN7 page. It was the only HTTP request with this user-agent at that time.
anon-54-218-44-232.amazonaws.com - - [12/Jan/2016:05:46:46 +0100] ""GET /mbs/abit/AN7.htm HTTP/1.1"" 200 3858 ""http://www.diffbot.com"" ""Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729; Diffbot/0.1; +http://www.diffbot.com)""
It's a bad bot - let's block it!
There is a quasi-standard for bots since 1994 - the robots exclusion standard which outlines the recommendations for ethical crawling and can be realized with a robots.txt file in the root of a domain. Good bots comply with this convention, check for robots.txt first, obey their directives and then start to crawl, if they are allowed to.
Diffbot not even fetches the robots.txt not to speak of obeying it's directives. OK, their slogan is ""Automatically extract web pages as structured data. No rules required."" - so our own slogan ""Diffbot fuck off! No rules required."" leads us to the next step which means simply blocking it by the User-Agent string with Apache's .htaccess (or equivalent techniques on other web servers). Two possible ways to feed Diffbot with a HTTP 403 (Forbidden) using .htaccess are shown below.
SetEnvIfNoCase User-Agent ""Diffbot"" bad
SetEnvIf Request_URI ""^/robots\.txt$"" ok

Order deny,allow
deny from env=bad
allow from env=ok
This method needs the Apache module mod_setenvif to be activated, which should be the case on most Apache instances. Note, Diffbot still has the chance to obey robots.txt in the future because it is not affected from the HTTP 403.
RewriteEngine on

RewriteCond %{HTTP_USER_AGENT} Diffbot [NC]
RewriteCond %{REQUEST_URI} !robots\.txt$ [NC]
RewriteRule ^.* - [F,L]
This method needs the mod_rewrite module to be activated, which is not the case on some Apaches because it can slow down the server if used incorrectly. Again Diffbot can access robots.txt.
It's getting worse - let's block it!
If you might have thought the problem has been solved, your're wrong. Diffbot comes back with other User-Agents (even camouflaged as a human visitor using Chrome 38 on Mac OS X 10.10), now downloads CSS, images, javascript, executes the latter and to top it all it comes from many IP ranges used by cloud computing services of Amazon AWS (EC2, S3, SNS, CloudFront), Google (googleusercontent), Hurricane Electric and Microsoft (Azure).
This time my ABIT UL8 and ABIT BE6 pages were affected.
anon-54-198-213-53.amazonaws.com - - [12/Jan/2016:10:06:39 +0100] ""GET /mbs/abit/UL8.htm HTTP/1.1"" 403 - ""-"" ""Readability/5bf615 - http://readability.com/about/""
anon-146-148-52-184.googleusercontent.com - - [12/Jan/2016:10:06:42 +0100] ""GET /mbs/abit/UL8.htm HTTP/1.1"" 403 20 ""-"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
anon-54-213-195-43.amazonaws.com - - [12/Jan/2016:10:06:43 +0100] ""GET /mbs/abit/UL8.htm HTTP/1.1"" 403 20 ""http://www.diffbot.com"" ""Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729; Diffbot/0.1; +http://www.diffbot.com)""
anon-54-163-90-165.ip.invalid - - [03/Jan/2016:05:37:44 +0100] ""GET /mbs/abit/BE6.htm HTTP/1.1"" 403 - ""-"" ""Readability/5bf615 - http://readability.com/about/""
anon-40-118-242-35.ip.invalid - - [03/Jan/2016:05:37:48 +0100] ""GET /mbs/abit/BE6.htm HTTP/1.1"" 200 3099 ""-"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
anon-40-118-242-35.ip.invalid - - [03/Jan/2016:05:37:48 +0100] ""GET /soggi.css HTTP/1.1"" 200 934 ""http://soggi.eu/mbs/abit/BE6.htm"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
anon-40-118-242-35.ip.invalid - - [03/Jan/2016:05:37:48 +0100] ""GET /files/mbs/image/abit/BE6.jpg HTTP/1.1"" 200 90408 ""http://soggi.eu/mbs/abit/BE6.htm"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
anon-40-118-242-35.ip.invalid - - [03/Jan/2016:05:37:48 +0100] ""GET /astat/js/ HTTP/1.1"" 200 18114 ""http://soggi.eu/mbs/abit/BE6.htm"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
anon-40-118-242-35.ip.invalid - - [03/Jan/2016:05:37:49 +0100] ""GET /astat/js/?action_name=... HTTP/1.1"" 204 - ""http://soggi.eu/mbs/abit/BE6.htm"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.101 Safari/537.36""
In conclusion, all these IP ranges can be blocked (IP address blocking) - it is very unlikely that a real human using a proxy server or suchlike on those servers wants to access your page. But be careful, only use the code below, if you don't use these cloud features by your own - blocking the IP ranges will break them, and thereby your own web services, for sure. The following code extends the mod_setenvif code from above, blocks other pests too (Hetzner, OVH!) and pokes holes for non problematic requests.
SetEnvIfNoCase User-Agent ""^(-?|Asperatusbot|Mozilla(\/.\..)?|QCrawl|robot)$"" bad
SetEnvIfNoCase User-Agent ""^('|=|\/|BUFF|Jersey|Mozilla.*Mozilla|Pcore|pshtt|User.?Agent)"" bad
SetEnvIfNoCase User-Agent ""(\\x|\""\)|\ \ Safari|\(compatible;?(\ )?\)|\(Mobile;\ rv:|0\ Gecko|Mozilla\/5.*MSIE\ [2-8]|MSIE\ (9|1[01]).*\.NET|Windows(\ (10|NT\)|XP\))|\))|X\ 10_10_0)"" bad
SetEnvIfNoCase User-Agent ""(aylienbot|betaBot|Bitvore|BoogleBot|cmscrawler|CommonCrawler|devworx|Diffbot|Digincore|evc-batch|exif-search|Feosey|FOCA|GarlikCrawler|Genieo|Getintent|HybridBot|Indy|ips(-agent|\ Community)|Kemvibot|KomodiaBot|mfibot|MixrankBot|Nutch|Pulsepoint|Scrapy|semantic|SiteTruth|Stratagems|Surdotly|Synapse|TWMBot|Web(Copy|collage|Fuck)|wonderbot)"" bad
SetEnvIf Request_URI ""^/robots\.txt$"" ok
SetEnvIf User-Agent ""(Cliqzbot|DeuSu|DuckDuckGo|facebook|Feedfetcher-Google|Flipboard|ia_archiver|Pinterest|safesearch\.avira\.com|SkypeUriPreview|W3C-checklink)"" ok

Order deny,allow
deny from env=bad
allow from env=ok

# Amazon.com, Inc. / AS14618, AS16509
deny from 23.20.0.0/14
deny from 34.192.0.0/10
deny from 35.160.0.0/13
deny from 46.51.128.0/17
deny from 50.16.0.0/14
deny from 50.112.0.0/16
deny from 52.0.0.0/10
deny from 52.64.0.0/12
deny from 52.84.0.0/14
deny from 52.88.0.0/13
deny from 52.192.0.0/11
deny from 54.64.0.0/11
deny from 54.144.0.0/12
deny from 54.160.0.0/11
deny from 54.192.0.0/12
deny from 54.208.0.0/13
deny from 54.216.0.0/14
deny from 54.220.0.0/15
deny from 54.224.0.0/11
deny from 107.20.0.0/14
deny from 184.72.0.0/15
deny from 2406:da00::/24
deny from 2600:1F00::/24

# Google Inc. / AS15169 / NetName: GOOGLE-CLOUD (and other cloud customers)
# Googlebot is not affected, comes from 66.249.0.0/16
deny from 8.35.192.0/21
deny from 23.251.128.0/19
deny from 35.184.0.0/13
deny from 104.154.0.0/15
deny from 104.196.0.0/14
deny from 107.178.192.0/18
deny from 130.211.0.0/16
deny from 146.148.0.0/17

# Hetzner / AS24940
deny from 5.9.0.0/16
deny from 46.4.0.0/16
deny from 78.46.0.0/15
deny from 85.10.192.0/18
deny from 88.198.0.0/16
deny from 136.243.0.0/16
deny from 138.201.0.0/16
deny from 144.76.0.0/16
deny from 148.251.0.0/16
deny from 176.9.0.0/16
deny from 178.63.0.0/16
deny from 188.40.0.0/16
deny from 213.133.96.0/19
deny from 213.239.192.0/18
deny from 2a01:4f8::/29

# Hurricane Electric, Inc. / AS6939
deny from 64.71.128.0/18
deny from 65.19.128.0/18

# Microsoft Corporation / AS8075
# bingbot/msnbot are not affected, come from 40.77.167.0/24, 65.55.0.0/16, 131.253.24.0/24, 157.55.39.0/24, 191.232.0.0/16, 199.30.0.0/16 and 207.46.13.0/24
deny from 40.80.0.0/12
deny from 40.96.0.0/11
deny from 104.40.0.0/13
deny from 104.208.0.0/13
deny from 138.91.0.0/16

# OVH SAS / AS16276, AS35540
deny from 5.39.0.0/17
deny from 5.135.0.0/16
deny from 5.196.0.0/16
deny from 37.59.0.0/16
deny from 37.187.0.0/16
deny from 46.105.0.0/16
deny from 79.137.0.0/17
deny from 51.254.0.0/15
deny from 91.121.0.0/16
deny from 91.134.0.0/16
deny from 92.222.0.0/16
deny from 94.23.0.0/16
deny from 109.190.0.0/16
deny from 142.4.192.0/19
deny from 144.217.0.0/16
deny from 149.202.0.0/16
deny from 151.80.0.0/16
deny from 158.69.0.0/16
deny from 164.132.0.0/16
deny from 167.114.0.0/16
deny from 176.31.0.0/16
deny from 178.32.0.0/15
deny from 188.165.0.0/16
deny from 192.95.0.0/18
deny from 192.99.0.0/16
deny from 198.27.64.0/18
deny from 198.50.128.0/17
deny from 198.245.48.0/20
deny from 213.251.128.0/18
deny from 2001:41d0::/32
deny from 2402:1f00::/32
deny from 2607:5300::/32
additional help and informations
Additional help and informations can be gathered from Webmaster World, especially from their subforum Webmaster World - Search Engine Spider and User Agent Identification.
some tips for the Diffbot operators
Here are some tips for the Diffbot operators and the operators of all the other bad bots.
make the bot compatible with the robots exclusion standard
give the bot a short User-agent for the robots.txt
let the bot obey the directives in the robots.txt
give the bot a precise HTTP User-Agent header
don't let the bot pretend to be a human (spoofing/faking)
add a page to your website which explains how to control the bot with the robots.txt
The User-agent for the robots.txt could simply be Diffbot and the HTTP User-Agent field could be filled with Mozilla/5.0 (compatible; Diffbot/0.1; +https://www.diffbot.com/bot/), where all the informations about why the bot crawls and how to control it would be found on the specified page.
Believe me, there will be still enough to crawl, because most webmasters don't care or even don't know about robots.txt, server logs and .htaccess.
other recommended articles
(published on 2017/02/17)",,http://soggi.eu/misc/articles/Diffbot-blocking-bad-bot-rude-content-scraper-from-websites.htm,,
Diffbot APIs Are Getting Very META,d2013-07-15T00:00,John Davi,Diffblog,"Meta,Diffbot,Application programming interface","We noticed recently that a common use for our Custom API Toolkit was augmenting Diffbot’s Automatic APIs with custom fields to return markup <META> tag data: meta descriptions, OpenGraph and Twitter Card tags, Schema.org microdata, etc.
We figured we’d save you the trouble of hand-curating rules, so we added the <META> parameter across all of our APIs. If you include &meta in your API request, you’ll receive an object containing the content from all of the page’s <META> elements, including nested objects for OpenGraph, Twitter Card, and Schema.org microdata.
August 2, 2013 update: We have added support for oEmbed data. If the submitted URL has an oEmbed endpoint, the full oEmbed object will be returned in a nested ‘oembed’ element.
Have a look at more specifics in our Article API documentation, or simply try out &meta today.
Don’t have a Diffbot token? Sign up for a free account.",-0.27203,http://blog.diffbot.com/diffbot-apis-now-with-more-meta/,,
Can Diffbot access content within an intranet or requiring a login?,d2014-02-06T00:00,,Diffbot,"Intranet,Diffbot","(Quick answer: Yes!)
Diffbot APIs are commonly used to extract content from public web pages. There are a few ways to handle pages not publicly accessible or that provide additional information to logged-in users:
Authenticate Using Custom Headers (Cookies)
The most consistently applicable way to “log-in” to a site is to provide a cookie value corresponding to a logged-in user. (You can set custom headers in individual API requests, while crawling or while performing Bulk Processing jobs. See how to set custom headers.)
To capture a cookie value in the first place:
First, log-in to the site using your regular browser.
Open your browser developer tools’ “Network” panel. If it’s empty, you may need to refresh your page to see the network requests made.
Select the primary page request and then find the “cookie” entry in your request headers. This will likely be a long string. Select and copy the entire string.
Optional: much of the data in a cookie string will not be necessary, so you can reduce some of the content here if you can easily determine which subset of the data is required for logging in.
You can now use this cookie value in your individual requests or within your Crawlbot crawls or Bulk Processing jobs.
POST the Content Directly
If you have access to your target content (e.g., you are processing pages within a corporate intranet, or you have an offline archive of markup or text), you can POST HTML directly to any of our API endpoints, automatic or custom. Diffbot will process your content as it would a directly-accessible web page.
See specific API documentation for more details on how to craft your POST:
Basic Authentication
To access pages that require basic access authentication, include the username and password in your API request’s url parameter, e.g. http%3A%2F%2FUSERNAME:PASSWORD@www.diffbot.com
A full request example:
http://api.diffbot.com/v3/article?token=yourtoken&url=http%3A%2F%2FUSERNAME:PASSWORD@www.diffbot.com",-0.12488,http://support.diffbot.com/apitoolkit/can-diffbot-access-content-within-an-intranet-or-requiring-a-login/,,
http://support.diffbot.com/global-index/how-can-i-use-the-global-index/,d2017-01-19T23:16:12,,Diffbot,"Diffbot,Application programming interface","Access to the Global Index requires a paid subscription to a Diffbot Plus, Professional or Enterprise plan. Each plan provides a monthly allotment of Global Index search queries in addition to regular extraction API calls.
The Global Index is built atop the Diffbot Search API and is populated by Diffbot’s Article and Discussion APIs; as such, API syntax and data output are consistent with the rest of the Diffbot platform.
Our updated Developer Dashboard — scheduled for a mid-2017 release — also offers a UI for performing Global Index searches. Contact us if you are interested in acquiring beta access to the new Dashboard.
]]>",,http://support.diffbot.com/topics/global-index/feed/,,
Using “Customize and Correct” to Make Instant Diffbot API Fixes,d2013-01-16T00:00,,Diffbot,"Diffbot,Regular expression,Application programming interface","We introduced “Customize and Correct” in 2012 because, let’s face it, robots aren’t perfect.
(Yet.)
So in this post I’ll walk you through using Customize and Correct to make instant changes to Diffbot API output, either to correct a rare issue, or to augment information you’re already receiving.
1. Visit Customize and Correct in our Developer Dashboard
Head on over to http://www.diffbot.com/dev/customize and log-in using your Diffbot token. Don’t have a token? Grab one at http://www.diffbot.com/pricing.
2. “Your Rules”
The default screen in Customize and Correct is “Your Rules.” This shows all of the current rules in operation for your token:
Domain: The domain regular expression upon which the rule is acting
API: The Diffbot API on which the rule is applied
Fields: Which fields have been overwritten by the rule output
Click on a domain regular expression to edit it, or the “Create a Rule” tab to create a new rule.
3. Create a Rule
Click “Create a Rule” and enter a representative URL from the site you want to correct. For instance, to create an Article API rule for this illuminating blog, you’d paste in a sample post, like this very one: http://blog.diffbot.com/using-customize-and-correct-to-make-instant-api-fixes.
Go ahead, I’ll wait.
4. Preview your output
Before you create a rule, you have one last chance to confirm exactly what you’re changing. Take a look at the results to see exactly what the API currently returns for various fields.
When you’re satisfied that, yes, you do want to make a change, click “edit” next to the field you want to change.
In my example, the “AUTHOR” field returns blank, so I’m going to correct that.
5. Choose the Right CSS Selector(s) for Your Content
After clicking “edit” you’ll be presented with a browser view of the page. You can click on an item to insert a suggested CSS selector, or type your own.
A successful selector is generic. Make sure there are no page-specific IDs that would render the rule moot on other similar pages.
For instance:
#post-227617 .body
…is probably specific to the exact post you’re using as an example, whereas:
#main .body .article
…should be consistent across all pages from the chosen site.
Note that while our highlighter tries its best to identify a generic selector, you may need to use your browser’s “Inspect Element” functionality to get the perfect rule.
Click the “Instructions/Help” tab for advanced selector information.
5. Preview and “Save.”
At the top of the browser view, you will see a preview of the selector’s matching content. Make sure it’s what you want, then click “Save.” Your rule will go into effect immediately.
6. See your edited fields
The output preview screen will highlight any edited fields. Click “edit” if you need to make changes, or “revert” to instantly remove the rule.
You can also preview the JSON output that the API will return:
7. Optional: Changing the Domain Regular Expression
By default Customize and Correct will attempt to apply rules to all pages for the given domain. If you want this to be more specific — for instance, to only work on pages within the “news” path — click “Change this” at the top of the page and edit the regular expression.
You can click the “Test” button to confirm your regular expression still matches the sample page.
8. That’s It
Your rule will now be in effect, and will also be used to help improve our core extraction algorithms. Thanks!
Stay tuned for our follow-up post on using advanced operators within a rule, like “Search and Replace” and “Ignore”",,http://blog.diffbot.com/using-customize-and-correct-to-make-instant-api-fixes/amp/,,
Diffbot:?????????????????,d2015-03-31T11:30,,ifeng.com,"??,????,????,??,??,????,??,??,??,??????","Diffbot????????,?“??”??????,???????
?????? 3?31?
????????,?????????,????,??????????????????????,????????????????????????????Diffbot???,????????????,?“??”???????,??????????????????????
??????????Diffbot?30???,????????????????????????API?
Diffbot?????????,????????,?“??”??????,??????,??????????????????
??????????Mike Tung 30?????????????,??Diffbot??????????????????????,????????????????????
“????????,????????Yelp??????,???????Yelp??API??,???Diffbot,????????????”Tung??
?Diffbot????,?????????????????,???????????????
??,?????????????“??????:??????????”?????,??????,????????4???????,????“??”?“??”?“??”?“????”?“??”(??????)?“??”???????,????????????????
Tung?,Diffbot????,??????????????????,??????????????
?????“??”??????,Diffbot?2009?????,?????????????,??2012??????200??????,???????????,????12??
Tung???,Diffbot???????????????,???????????????Diffbot????????????Stanford StartX?????,????????Sun????????(??)???Andreas Bechtolsheim??????
??,Diffbot??????????eBay????????????????,Diffbot???4???????:??14?????,??1??API???????299??,??25??API??,???????????0.001??,????????????“??”?“??API”?,????4999??,??500??API??,???????????0.0009??,????????????
Tung?,Diffbot???????API??????“???”??,??????????,????????????????????
“?????????????,”Tung?,“?????????????????,?????????‘???’??????????”",,http://share.iclient.ifeng.com/sharenews.f?aid=493443&vt=5&fromType=vampire,,
Analyzing Consumer Marketplaces Using Diffbot’s Product API,d2014-08-13T00:00,,The Infovore's Dilemma,marketplace,"Diffbot is a powerful scraping tool that can be used to automatically parse a wide variety of web pages using computer vision. Developers are using the product to power reading services (Instapaper, Digg, Reverb, Onswipe, Longform), price comparison engines, media monitoring tools, and many other apps and systems.
When analyzing consumer products and platforms, I am eager to explore novel ways to find data that can help inform my decisions. This fall, I was looking at a handful of fashion resale marketplaces that had seen rapid growth. The specific marketplace we were analyzing provided us with lots of data, but we wanted a more thorough understanding of their competitors. I spun up Diffbot’s Crawlbot tool to quickly run crawls on the three main marketplaces and automatically extract pricing data using Diffbot’s Product API.
The first set of analysis helped us understand the relative size of each of the marketplaces and price of the products. The prices are important as it may inform the potential profile of customers and also the basket size from which the platform will earn a percentage. Four months later we ran the same analysis on two of the marketplaces to compare how they had evolved over time. Out of respect, I chose to anonymize the marketplaces to MPA, MPB, MPC.
MARKETPLACE DATA
Number of Items – Diffbot’s Crawlbot found 61,240 items on MPB with offer prices vs. 18,992 items on MPA[1].
List Price - Items on MPA have a noticeably higher list price than those on MPB. The mean list price on MPA is 277% that of MPB ($521 vs. $188), and 75% of the items on MPC are less than $120 vs. $388 on MPA. This may indicate that MPA is attracting higher quality items for resale.
Offer Price - MPA’s mean offer price is 219% that of MPB ($234 vs. $107). MPC sells significantly higher priced items, but discounts steeply, so the mean offer price ($275) is only 117% that of MPA but 257% that of MPB. 75% of items on MPB are less than $50, vs. $143 on MPA and $235 on MPC. MPB clearly skews significantly cheaper.
Average Discount – The distribution of percent discount is roughly equal for MPA and MPB. MPC, however, offers a much steeper discount from list price.
The figure below clearly indicates two important details: 1) MPB has significantly more products in their marketplace and 2) MPA has a much healthier long tail of higher priced products. The long tail MPA has captured will help increase AOV and may attract and retain higher value consumers.
ANALYSIS OVER TIME
In December MPA had ~19k items for sale vs. ~63k on MPB
In March MPA now has ~264k items for sale (14.6x growth) vs. ~278k on MPB (4.4x growth)
In December MPA had a much healthier long tail of higher priced items (the 75th percentile was $149 vs. $49 on MPB)
In March MPA has an almost equal number of products but has maintained a healthier long tail of higher priced items (the 75th percentile was $87 vs. $45) which will drive higher AOV and help attract higher value customers",,http://milesgrimshaw.com/analyzing-marketplaces-diffbot/,,
Comments for Diffbot Support,,,Diffbot,,,,http://support.diffbot.com/comments/feed/,,
Can Diffbot crawl sites that use “infinite” or “endless” scrolling?,d2017-01-18T00:00,,Diffbot,"Scrolling,Diffbot,Scroll,Infinity,Website","Currently Crawlbot does not interact with sites to retrieve or pursue links that appear when a page is scrolled — so-called “infinite” or “endless” scrolling. Crawlbot will only pursue links that are available upon an initial page load.
(Related: How to find and access Ajax-generated links while crawling.)
In most cases sites will offer alternative means to find the same links:
related links (to other posts or products) on individual post or product pages
search filters or category links that narrow the number of results
a sitemap file (e.g. sitemap.xml) or similar map to individual item pages
If you find a site that is unable to be crawled without page-scrolling, you may be able to improve results via the following approach:
Write custom Javascript via Diffbot’s custom X-Evaluate header, implementing a click or scroll event — or multiple click/scroll events.
Store your X-Evaluate header as a custom rule against the Analyze API for the site in question.
Use the aforementioned method to execute Ajax/Javascript while crawling
.
For assistance with the above, feel free to contact us at support@diffbot.com.",-0.14543,http://support.diffbot.com/crawlbot/can-diffbot-crawl-sites-that-use-infinite-or-endless-scrolling/,,
Diffbot State of Machine Learning Report – 2018,d2018-12-04T00:00,Dru Wynings,Diffblog,"Diffbot,machine learning,Google Knowledge Graph","In what will likely be the first of many reports from the team here at Diffbot, we wanted to start with a topic near and dear to our (silicon) hearts: machine learning.
Using the Diffbot Knowledge Graph, and in only a matter of hours, we conducted the single largest survey of machine learning skills ever compiled in order to generate a clear, global picture of the machine learning workforce. All of the data contained here was pulled from our structured database of more than 1 trillion facts about 10 trillion entities (and growing autonomously every day).
Of course, this is only scraping the surface of the data contained in our Knowledge Graph and, it’s worth noting, what you see below are not just numbers in a spreadsheet. What each of these data points represents are actual entities in our Knowledge Graph, each with their own set of data attached and linked to thousands of other entities in the KG.
So, when we say there are 720,000+ people skilled in machine learning – each of those people has their own entry in the Knowledge Graph, rich with publicly available information about their education, location, public profiles, work history, and more.",0.517,https://blog.diffbot.com/diffbot-state-of-machine-learning-report-2018/,,
Knowledge Graph Comparison: GDELT VS. Diffbot,d2020-05-20T00:00,Merrill Cook,Diffblog,"Yahoo,graph theory,Kalev Leetaru,political science,Web mining,Diffbot,Philip Schrodt,Georgetown University,Google Knowledge Graph,GDELT Project","There are only a handful of publicly available knowledge graphs. And among those, only a few provide data with enough breadth to in some way represent the entire internet, and with enough granularity to be useful.
The Global Database of Events, Language, and Tone (the GDELT Project) is one such database, and within certain use cases a strong contender with Diffbot’s Knowledge Graph. Unlike many knowledge graphs that are primarily of academic interest, Diffbot and the GDELT Project both extract data at the scale of the web, process it and synthesize this data into enormous (and usable) graphs.
In this guide we’ll compare and contrast these two heavyweights of web extraction, processing, and knowledge graph creation.
In particular, we’ll look at:
GDELT Data Products
Diffbot’s Data Products
Diffbot’s Use Cases
If you’re a researcher looking for access to the world’s largest database, be sure to check out our 14-day free trial or to contact sales for opportunities for academic partnerships.
GDELT’s History
While web data providers do pivot and adapt to the times, the history of a web extraction and graphing organization can provide details on what their mission and primary focus are.
The Global Database of Events, Language, and Tone (GDELT) was co-founded by Kalev H. Leetaru of Yahoo! and Georgetown University. Leetaru had pioneered some of the earliest web mining technologies at the National Center for Supercomputing Applications (the home of the modern web). GDELT was a continuation of these inquiries that coalesced around a 2011 conference paper by co-creator Philip Schrodt.
Schrodt, as a political scientist, was interested in political and governance applications of web mining, as evidenced in his outlining paper for GDELT titled “Automated Production of High-Volume, Near-Real-Time Political Event Data.” Over the last decade, GDELT has grown into one of the largest databases of just this: a catalog of political events, who the actors of these events are, and how these events are perceived by the general public.
Today GDELT is supported by Google Jigsaw, a unit within Google responsible for threat emergence and detection.
GDELT Data Products
GDELT’s primary data offering is that of a several petabyte-sized database with entries on events, actors in events, news mentions of events, and sentiment analysis. A range of interrelated datasets can be accessed which total trillions of data points.
The three primary streams supporting these data sets include:
A stream codifying physical activities of over 300 types worldwide (for example, a conference, or a sporting event)
A stream codifying people, places, and organizations as well as themes and emotions for parties affected by events
And a stream codifying visual imagery from news coverage
Together these streams cover worldwide news from print and web-based sources in nearly real time, with streams updating every 15 minutes around the clock. Supplementary sources that are also encoded include over 215 years of digitized books, 21 billion words of academic literature, and many publicly available closed circuit video feeds.
The data products that are provided from this dataset include:
The GDELT Event Database categorizes physical activities occurring around the world
GDELT Global Knowledge Graph provides the ability to see how events affect or are affected by organizations and people
GDELT Visual Global Knowledge Graph is a random sampling of millions of images from the news of a nation presented in one day increments
GDELT GKG Special Collections are sources of particular interest processed through GDELTs other tools. These include digitized books and academic literature
GDELT Use Cases
GDELT has primarily gained notoriety within research and governance settings, though commercial implementation of their databases is certainly possible. Some of the most well-known use cases for GDELTs products include:
Disaster Monitoring and Reporting
Event Monitoring
Risk Assessment
Influencer Analysis
Social Science Research
News Monitoring
Sentiment Analysis
Within research settings, GDELT is often used to gain geographic information related to news or academic literature mentions. In governance settings, GDELT is often used to gauge sentiment across broad geographic areas. For example, one of the most publicly known uses of GDELT was Foreign Policy Magazine’s mapping of media outrage at the passing of the Affordable Care Act plotted geographically.
Among event and sentiment-centered databases, it is noted that there is only one comparably sized database known as the U.S. Department of Defense’s Worldwide Integrated Crisis Early Warning System. This database is not publicly available, however. Diffbot’s Knowledge Graph can be used for similar sentiment-centered queries on billions of entities. With key differences being that Diffbot does not source data from non web-based sources, GDELTs sentiment data goes back to 1979, and Diffbot provides a much wider range of fact types.
Diffbot’s Data Products
Diffbot’s data products center around the process of turning unstructured data from across the web into structured, contextual data.
Data parsed through Diffbot’s cutting-edge machine vision and natural language processing systems is primarily available through four routes:
Diffbot’s Knowledge Graph extracts data from across the web every 3-5 days. Unstructured data is parsed and fused into connected entities such as organizations, people, articles, products, and more.
Diffbot’s Automatic Extraction APIs can be pointed to a precise location you want data from. Data is then parsed into entities similarly to the Knowledge Graph including organizations, people, articles, products, discussions, and more. Automatic extraction APIs are best if you want data that is updated more regularly than the Knowledge Graph, or if you know precisely what data you want to extract
Diffbot’s Enhance looks over Knowledge Graph data with a slightly different matching algorithm. Enhance is primarily used for data enrichment. When you have some information on an entity and you would like to enrich this data, Enhance is your best bet. Enhance is available with multiple integrations include Excel and Google Sheets.
Diffbot’s Crawlbot extracts structured data from entire sites at once (or on a schedule). This data can then be parsed by an Automatic API (or wrangled with tools of your choosing). Crawlbot is best used if you want to extract data from a domain with many sub pages or if you want to extract on a schedule.
While more technical users can use custom data extraction through APIs, Knowledge Graph and Enhance offer access to the world’s largest Knowledge Graph composed of over 2 trillion facts and over 10 billion Knowledge Graph entities.
Diffbot Use Cases
Diffbot’s web data is semantic, meaning that extracted data is encoded alongside what the data means. This enables Diffbot’s AI systems to infer facts about different entity types. It also allows Diffbot’s AI systems to encode relationships between different entities. For example, a product entity may contain facts about it’s pricing, availability, and reviews. This product may be linked to a larger brand, or company entity, which would be composed of different types of facts.
The breadth, structure, and interlinked nature of Diffbot’s data supports a huge range of use cases. And allows for \unstructured web data — once processed — to answer complex searches.
Some example searches that Diffbot’s Knowledge Graph can answer include:
What companies in San Francisco in Finance employ more than 20 data scientists?
What is the sentiment of articles written about an organization by week?
What is the breakdown of skills by organizations in a given industry?
And countless other examples…
Results are explorable down to individual and interlinked profiles of people, organizations, products, and more.
The major categories of Diffbot’s use cases include:
Within each of these use cases studies have suggested that data teams can spend up to 80% of their time and resources just sourcing, cleaning, and verifying the quality of data. Diffbot helps to minimize this burden through a range of Knowledge-As-A-Service solutions. Additionally, Diffbot’s data is sourced from a wider range of sources than potentially any other commercially-available data provider.
GDELT Versus Diffbot Data Structures
From a high-level perspective, GDELT and Diffbot organize their data similarly. Both GDELT and Diffbot employ knowledge graphs for the organization of entities.
Graphs are an informatics concept and organizational schemas that focuses on preserving the relationships between entities. The focus on relationships is greatly distinguishable from how “traditional” databases work, which are built to preserve the integrity of data for individual database entries. Even relational databases, built to include information on the relationships between database entries are highly inefficient when dealing with large relational datasets when compared to Knowledge Graphs.
The “knowledge” portion of “knowledge graphs” relate to the semantic nature of the data. This “smart data” is encoded to preserve the meaning of the data next to the data itself. This means new facts can be inferred programmatically. This also means that searches can traverse different entity types, exposing valuable insights into complex relationships.
From a high-level perspective, GDELT and Diffbot also source their data similarly. Both employ what could be termed “web-wide crawlers.” Some differences between Diffbot and GDELT data sources include:
GDELT includes some print-based news data sources
GDELT offers some historical data going back to 1979
GDELT looks for events, sentiment, and actors in events
Diffbot crawls the entirety of the public web (not just news sources)
Diffbot can be pointed at any domain for rules-free crawling
Diffbot is optimized for many different data types (organizations, people, articles, products, discussions, and more)
While there are definite similarities between GDELT and Diffbot data offerings, one of the largest differences is the subject matter of data extracted from across the web.
Diffbot’s AI-enabled Knowledge Graph and Automatic Extraction APIs parse facts from across the web into an ever evolving set of entity types. Through machine learning, Diffbot’s entities are optimized to contain information that humans find valuable. For example, a fact about a fundraising round may be pertinent to an organization, but likely not to an article.
What this means is that entities in Diffbot’s Knowledge Graph are structured to mirror entities in the real world.
A present list of Diffbot’s Knowledge Graph entity types includes:
AdministrativeArea
Article
Corporation
Degree Entity
Discussion
EducationMajorEntity
EducationalInstitution
EmploymentCategory
Event
Image
Intangible
Landmark
LocalBusiness
Miscellaneous
Organization
Person
Place
Post
Product
Role
Skill
Or Video
On the other hand, GDELT is based on only a subset of extractable facts found in data sources. In particular, GDELT is interested in events — and particularly in events of political importance. Entity types that GDELT tracks and deems important for supporting this mission include:
Events
People
Sentiment
This doesn’t mean that there isn’t any overlap in coverage between the two knowledge graphs. You could search the Diffbot Knowledge Graph to gain sentiment analysis scores within news articles about a given person or event. But that is not the single focus point. You could similarly search for people entities given applicable parameters in both knowledge graphs. But the data returned about these people would be substantially different.
Analysis of GDELT Offerings
GDELT Overview
GDELT, short for The Global Database of Events, Language, and Tone, is the world’s largest event-centered database. Well-used in research and governance applications, GDELT provides a near real-time database of actors (people or organizations), events, and sentiment towards these actions. GDELT data is sourced from both print and selected web-based news sources, and
What Type of Data is Available?
Event data
Sentiment analysis data about events
Knowledge graph of entities involved in events (people, organizations, news mentions)
News data
Academic literature data
GDELT Features
GDELT Analysis Service Platform For Analysis and Visualization of Data
Google BigQuery Access to Data
Downloads for All Data or by Time Frame
News Image Aggregation Visualization
Precise geographic data for events and those involved
GDELT Use Cases
Disaster Monitoring and Reporting
Event Monitoring
Risk Assessment
Influencer Analysis
Social Science Research
News Monitoring
Sentiment Analysis
GDELT Pricing
GDELT as well as the GDELT analysis service are free. Individuals or organizations may even repurpose data for use in their own products as long as they cite GDELT with a link.
Analysis of Diffbot Offerings
Diffbot Overview
Diffbot sources Knowledge Graph™ data through web-wide crawls that are then parsed by a cutting-edge machine vision and natural language processing AI system. Data is organized into entity types that are interlinked and populated by facts. Data within Diffbot entities is semantic, meaning that data is encoded next to the “meaning” or context of that data. At a general level, Diffbot takes unstructured data from billions of sites across the web and provides this data structure within the world’s largest knowledge graph.
Diffbot also offers customizable data extraction solutions including Crawlbot as well as custom Extraction APIs.
What type of data is available?
Organizational (firmographic) data
Person data
Product data
News mention data
Other entity data
Diffbot Features
Access to structured entity data extracted from pages across the web (Knowledge Graph)
Data enrichment (Enhance)
Bulk crawler
Custom Extraction APIs
Excel, Google Sheets, Tableau integrations
Diffbot Use Cases
Market Intelligence
News Monitoring
Ecommerce
Machine Learning
Web Scraping
Diffbot Pricing
$299-$899/Month
$299/Month Standard “Startup” Plan
Interested in how Diffbot can help your news monitoring or research use cases (among others)? Check out our recent news monitoring case study, or sign up for a free 14-day trial today!",0.525,https://blog.diffbot.com/knowledge-graph-comparison-gdelt-vs-diffbot/,,
How We Increased Our Lead Contact Rate by 46% with Diffbot Enhance,d2020-10-15T00:00,Jerome Choo,Diffblog,"Diffbot,Salesforce.com","Hi! This is Jerome from Diffbot. You might’ve seen us around before. We’re known for our automatic extraction APIs, and our knowledge graph of the public web. Today, I’d like to introduce you to Diffbot Enhance, lead enrichment anywhere you need it.
Lead enrichment doesn’t get enough credit
When I first saw it in action, it looked like a gimmick - just fields populated in a CRM sold with shockingly pricey annual contracts up-sold alongside Salesforce.
Like keeping your personal address book up to date. Helpful? Sure. Necessary? Not really.
Sales always insists it’s helpful though. I didn’t get it.
Fast forward a few years, we noticed one day that 62% of our inbound leads never make it to a demo call. 62%! These are people who choose to ignore the self-start trial option, fill out a 6 field form, pass a captcha, and click a button that literally says request a demo.
What we discovered was that once a lead lands in our inbox, a huge amount of time was being wasted in manual research and qualification. “Googling a lead” doesn’t sound like much, but tack on assigning, replying, and scheduling, repeated hundreds of times a week - that’s a whole lot of menial time. Time that is spent ignoring leads who need a solution now, not tomorrow. And as we all know, the early bird gets the worm.
What we need is a way to automate
Filtering out low quality leads or spam
Assigning the correct account rep for the territory
Providing scheduling availability
Booking the meeting
The latter two aren’t terribly difficult, but to automate the first two, we need to enrich our leads. I’m starting to get it now.
We built Diffbot Enhance from our knowledge graph to enrich each inbound lead with attributes like company size, location, and industry. Then a custom backend service powered by Chili Piper (they’re great!) identifies ideal customer matches based on these attributes and automatically presents the lead with an availability calendar for a matched AE to schedule with.
All this happens automatically, immediately after a lead form is submitted on our website. No more inbox refreshing on weekends or wading through spam. A quality of life improvement for our account execs, while increasing our inbound contact rate by a cool +46%. Not too shabby.
I learned that enrichment is only as useful as the pain it is automating.
The pain isn’t in googling a lead, it’s acting on that information, over and over again, where it really counts - not sitting in a CRM.
For this reason, Diffbot Enhance is not an app. Not in the “pretty interface” sense of the word that is. It’s a suite of integrations and an API, designed to be accessible anywhere you need it to automate research and decision making.
I’m particularly excited about our Google Sheets and Microsoft Excel add-ons which unlock the ability to look up organizations and people with a simple formula.
In this example, the only input required is a URL in the “lead” column. Diffbot Enhance automatically fills in the rest, and the priority score is calculated from matches in the enhanced columns.
Postmates and DoorDash are lighting up like Christmas trees because I’ve setup conditional formatting to look for leads that are:
In San Francisco
Running some type of services business
Around 10k employees or more
While not as showy, a practical application in my week to week is uploading a list of our latest paid customers and looking for firmographic trends to find our next channel.
This level of automated firmographic segmentation truly shines in lead funnels and email campaigns, where just a touch of personalization could go a long way.
For the no-coders - Find us on Zapier and let it work its magic in your lead gen automations or other nurture pipelines. Zapier’s partner network of CRMs is extensive, allowing you to enhance leads and automate pipelines no matter what you use.
Finally, our REST API is wonderful for all kinds of custom use cases (like our lead routing example), and even includes a bulk option for enhancing hundreds of records at once.
On the technology that powers Diffbot Enhance - it’s pretty freakin’ incredible.
We built the largest and most accurate knowledge graph in the world, created using machine learning models that read and understand the web like humans, except way faster. And then we built Diffbot Enhance to find organizations and people in it, with match rates rivaling enterprise enrichment providers like Clearbit.
Except unlike Clearbit, our data is built by a machine, not people, and 100% sourced from the public web by us, not through third party data brokers. We’re one of just a few companies actively crawling the public web. Fun fact - we’ve been crawling and extracting data behind the scenes for companies like Clearbit for over a decade.
Enrichment today costs well over $20,000 a year.
That’s a minimum figure, not a maximum. Because of how human intensive data processing is, the data is often only available to b2b enterprise companies who are able to bank on big margins to make up for the cost. Our data is processed nearly entirely by a machine, which brings me to one more thing -
Diffbot Enhance costs 5 cents a lead.
That’s 1/5 the per-lead cost of our nearest price competitor.
1/20 the minimum annual contract cost.
For over 50+ traits and attributes per lead.
Did I mention how incredible this technology is?
I can wax poetic on our technology all day. But here’s the fundamental vision for Diffbot Enhance:
Unlock productivity growth by automating data gathering for everyone.
Diffbot Enhance is available today at enhance.diffbot.com. We’re excited to see what you build with us.
Yours,
Jerome Choo",,https://blog.diffbot.com/how-we-increased-our-lead-contact-rate-by-46-with-diffbot-enhance/,,
Diffbot Web Mining Hack Day,d2011-01-16T00:00,Mike Tung,Diffbot,"Diffbot,Hackathon,World Wide Web,Semantic Web","On June 25th, over 80 of Silicon Valley’s top hackers, designers, and students gathered at the Diffbot offices in Palo Alto in the hopes of building the next great Web 3.0 app. Over the next 13 hours, participants learned about data and analysis APIs, formed teams, and wrote code.
At the end of the night, only one team could win the prize for best App.
Best App Award: HeatSync
HeatSync solves a simple problem: knowing where people are nearby. HeatSync aggregates check-in data from FourSquare, Gowalla, Google, and Twitter into an interactive heatmap centered near you (for the demo, only FourSquare data was used). The team displayed a great combination of backend talent, user experience, and product design.
Video: http://vimeo.com/25652129
People’s Choice Award: My Moods
The winner of the popular vote was My Moods, an app that helps you change your emotional state by reading news. My Moods use the Diffbot API to extract articles from the web and the EffectCheck service to perform sentiment analysis.
See the full list of hacks
Thanks to all of our sponsors for making this event possible.",,http://blog.diffbot.com/diffbot-web-mining-hack-day/amp/,,
Diffbot,d2017-02-23T11:30:29,,TOPBOTS,"Algorithm,Machine learning,Web page,Web scraping,Website,Diffbot,Computer vision,Application programming interface","About The Company
Diffbot develops machine learning and computer vision algorithms and public APIs for extracting data from web pages / web scraping with better-than-human-level accuracy across any website or language. Automatic APIs retrieve every possible piece of data from a web page. Crawlbot automatically finds every important page on any site.
Clients & Case Studies
Cisco
Adobe
Yandex
eBay
Amazon
Interested in working with this company? TOPBOTS produces a detailed vendor report based on feedback from industry experts, investors, and client companies. Submit your contact information to request access.",,http://www.topbots.com/project/diffbot-web-scraping/,,
Diffbot: Knowledge Graph API with Mike Tung,d2018-10-31T00:00,SE Daily,Software Engineering Daily,"Mike Tung,Google,Diffbot,We Edit Podcasts,Knowledge Graph,Wikipedia,American Petroleum Institute","Google Search allows humans to find and access information across the web. A human enters an unstructured query into the search box, the search engine provides several links as a result, and the human clicks on one of those links. That link brings up a web page, which is a set of unstructured data. Humans can read and understand news articles, videos, and Wikipedia pages.
Google Search solves the problem of organizing and distributing all of the unstructured data across the web, for humans to consume. Diffbot is a company with a goal of solving a related, but distinctly different problem: how to derive structure from the unstructured web, understand relationships within that structure, and allow machines to utilize those relationships through APIs.
Mike Tung is the founder of Diffbot. He joins the show to talk about the last decade that he has spent building artificial intelligence applications, from his research at Stanford to a mature, widely used product in Diffbot. I have built a few applications with Diffbot, and I encourage anyone who is a tinkerer or prototype builder to play around with it. It’s an API for accessing web pages as structured data.
Diffbot crawls the entire web, parsing websites, using NLP and NLU to comprehend those pages, and using probabilistic estimations to draw relationships between entities. It’s an ambitious product, and Mike has been working on it for a long time. I enjoyed our conversation.
Show Notes
We recently launched a new podcast: Fintech Daily! Fintech Daily is about payments, cryptocurrencies, trading, and the intersection between finance and technology. You can find it on fintechdaily.co or Apple and Google podcasts. We are looking for other hosts who want to participate. If you are interested in becoming a host, send us an email: host@fintechdaily.co
Transcript
Transcript provided by We Edit Podcasts. Software Engineering Daily listeners can go to weeditpodcasts.com/sed to get 20% off the first two months of audio editing and transcription services. Thanks to We Edit Podcasts for partnering with SE Daily. Please click here to view this show’s transcript.
Sponsors",,https://softwareengineeringdaily.com/2018/10/31/diffbot-knowledge-graph-api-with-mike-tung/,,
How does Diffbot handle duplicate pages/content while crawling?,d2015-07-10T00:00,,Diffbot,"The Duplicate,AFC Ajax,Diffbot","Crawlbot will often encounter duplicate pages (with different URLs) while canvassing a site. There are a handful of ways Diffbot helps you handle these duplicates:
Pages with duplicate HTML sources will be ignored while crawling
While crawling (spidering for links), and before sending a URL to be processed, Crawlbot examines the raw HTML source of each page and compares it to the source HTML of all previously-spidered pages. Any exact matches to previously-seen pages will be flagged as duplicates and ignored.
The duplicate comparison is made on the raw HTML source only. Only when processing a page will Javascript be executed.
The Crawlbot URL Report — available from each crawl’s status page, or via the Crawlbot API — will note each duplicate URL, and the document ID (docId) of the page it duplicates.
Note: If your crawl takes advantage of our Analyze API’s ability to execute Javascript to find Ajax-delivered links, Crawlbot’s duplication detection will be disabled. This is because Ajax-powered sites can have identical HTML source code for multiple pages, even though the actual on-page content (when Javascript is fully executed) is quite different.
Pages with a different canonical link definition will be ignored
Note: This behavior can be disabled on an individual crawl basis via the useCanonical argument in the Crawlbot API.
Two things will happen when a page contains a canonical link element different from its own URL:
The current page will be skipped/ignored as a duplicate.
The canonical URL will be automatically added to the Crawlbot queue (if not already in the queue)
Similar to above, duplicate pages will be so identified in the Crawlbot URL Report.
Duplicated extractions will have the same diffbotUri
Each Diffbot JSON object contains the diffbotUri field. The value is uniquely calculated from a subset of extracted fields and can be used to uniquely identify the extracted content. The diffbotUri will be the same across duplicate extractions.
For example, the diffbotUri value for this page is article|3|-897978830.
For URLs that are not exact-source duplicates (and are thus not ignored while crawling), but that result in the same extracted output, the diffbotUri values will be the same. When you process your crawl data, filtering and removing objects with the same diffbotUri will allow you to retain only one example of each entity.",-0.24905,http://support.diffbot.com/crawlbot/how-does-diffbot-handle-duplicate-pagescontent/,,
Diffbot’s HackerNews Trend Analyzer,d2013-04-25T00:00,John Davi,Diffblog,"Hacker News,Microsoft FrontPage,Diffbot","Like any good developer service, we’re fans of Hacker News. Making the vaunted Frontpage is a, well, vaunt-worthy accomplishment (we’ve been there once), so we thought we’d use our APIs to analyze and identify any trends in what content makes the Frontpage.
The result is Diffbot’s HackerNews Trend Analyzer. Feel free to click that link and play around, or read more here for details on how we did it.
The Trend Analyzer lets you see which domains, submitters, article authors and tags have frequented the Frontpage over the past 30 months. (Special thanks to HN user domador and his hourly snapshot service).
For completists, here’s how we grabbed and analyzed the data:
Create a Custom API Using the Diffbot Custom API Toolkit
Neither HN nor Domador offer an API. This of course is not uncommon, and is precisely why we created our Custom API Toolkit. It leverages Diffbot’s scale and speedy web-page rendering (and your CSS or XPath selectors) to extract practically any data from any page.
Our rules enabled us to extract the submitted link, poster, and comments thread URL from each submisssion.
Here’s a breakdown of what’s happening in our “hn” API ruleset: (you can view the back-end output of our Custom API tool at right)
Name our Custom API “hn” (api) — available for our token immediately at https://www.diffbot.com/api/hn — and have this rule operate on all pages at domador.net (urlPattern).
Iterate through all table cells with class “title” or class “subtext.”
Ignore any table cell that contains a link whose text is exactly “More” — this prevented returning any next-page links.
The first rule: within each table cell identified above, return the anchor tag href value as “link.”
The second rule: within each table cell that contains multiple anchor tags, return the second anchor tag href as “thread.” (This was for the link to the comments for a submitted link.)
Much like, ahem, the HackerNews markup, this resulted in a messy API that returned both the submission link and the author link in repeating results named “link.” We’ll worry about that later, but first: data extraction.
Step Two: Crawlbot
We then turned to Crawlbot, Diffbot’s on-demand crawling service that spiders a domain and automatically extracts data from pages using the appropriate Diffbot API.
(For a relatively well-structured site like the Domador archive this may have been overkill, but we’re dogfooding here.)
We set up our crawl as follows:
Page Type: We specified hn, the name of our newly created API.
Seed URL: http://hhn.domador.net/
Crawl URL Regex: http://hhn.domador.net/d{4}.* (this limits pages crawled to those within the Domador archive format, ignoring any ancillary pages/links)
Processed URL Regex: http://hhn.domador.net/d{4}/d{2}/d{2}/d{2}/ (each archive page takes the form http://hhn.domador.net/2013/04/01/12 — for April 1, 2013 at 12:00 — and this regex makes sure only pages that match that will be sent to the /api/hn API for extraction)
Crawlbot returns either a list of matching URLs, or a complete document download with all of the Diffbot API extractions. In this case, we opted for the latter, and set our crawl going. 25,000 or so URLs later we had a corpus of links.
Step Three: The Article API
We wrote a Python script to iterate through our local JSON copy and match user and thread URLs with the submitted URL. It resulted in this output for each frontpage submission:
{
 ""poster"": ""stonemetal"", 
 ""link"": ""http://arstechnica.com/tech-policy/news/2011/05/a-way-to-take-out-spammers-3-banks-process-95-of-spam-transactions.ars"", 
 ""thread"": ""http://news.ycombinator.com/item?id=2605580""
 }
That same script ran each “link” value through our Article API, which augmented the above with structured data from each post: title, author, full-text, date, tags, etc.
Step Four: Play Around
We thought we’d mine the results for interesting trends, then realized it would be much easier for us not to do any more work at all. This resulted in our HackerNews Trend Analyzer, which we’d love for you to play around with. Put any links to interesting trends you find in the comments (here or at HackerNews, naturally), and certainly let us know what else you’d like to know about the HN frontpage.",,https://blog.diffbot.com/diffbots-hackernews-trend-analyzer/,,
Diffbot’s New Product API Teaches Robots to Shop Online,d2013-07-31T00:00,John Davi,Diffblog,"Web service,Diffbot","Diffbot’s human wranglers are proud today to announce the release of our newest product: an API for… products!
The Product API can be used for extracting clean, structured data from any e-commerce product page. It automatically makes available all the product data you’d expect: price, discount/savings amount, shipping cost, product description, any relevant product images, SKU and/or other product IDs.
Even cooler: pair the Product API with Crawlbot, our intelligent site-spidering tool, and let Diffbot determine which pages are products, then automatically structure the entire catalog. Here’s a quick demonstration of Crawlbot at work:
We’ve developed the Product API over the course of two years, building upon our core vision technology that’s extracted structured data from billions of web pages, and training our machine learning systems using data from tens of thousands of unique shopping sites. We can’t wait for you to try it out.
What are you waiting for? Check out the Product API documentation and dive on in! If you need a token, check out our pricing and plans (including our Free plan).
Questions? Hit us up at support@diffbot.com.",,http://blog.diffbot.com/diffbots-new-product-api-teaches-robots-to-shop-online,,
Diffbot’s HackerNews Trend Analyzer,d2013-04-25T00:00,John Davi,Diffblog,"Diffbot,Fad,HackerNews,Hacker News","Like any good developer service, we’re fans of Hacker News. Making the vaunted Frontpage is a, well, vaunt-worthy accomplishment (we’ve been there once), so we thought we’d use our APIs to analyze and identify any trends in what content makes the Frontpage.
The result is Diffbot’s HackerNews Trend Analyzer. Feel free to click that link and play around, or read more here for details on how we did it.
The Trend Analyzer lets you see which domains, submitters, article authors and tags have frequented the Frontpage over the past 30 months. (Special thanks to HN user domador and his hourly snapshot service).
For completists, here’s how we grabbed and analyzed the data:
Create a Custom API Using the Diffbot Custom API Toolkit
Neither HN nor Domador offer an API. This of course is not uncommon, and is precisely why we created our Custom API Toolkit. It leverages Diffbot’s scale and speedy web-page rendering (and your CSS or XPath selectors) to extract practically any data from any page.
Our rules enabled us to extract the submitted link, poster, and comments thread URL from each submisssion.
Here’s a breakdown of what’s happening in our “hn” API ruleset: (you can view the back-end output of our Custom API tool at right)
Name our Custom API “hn” (api) — available for our token immediately at http://www.diffbot.com/api/hn — and have this rule operate on all pages at domador.net (urlPattern).
Iterate through all table cells with class “title” or class “subtext.”
Ignore any table cell that contains a link whose text is exactly “More” — this prevented returning any next-page links.
The first rule: within each table cell identified above, return the anchor tag href value as “link.”
The second rule: within each table cell that contains multiple anchor tags, return the second anchor tag href as “thread.” (This was for the link to the comments for a submitted link.)
Much like, ahem, the HackerNews markup, this resulted in a messy API that returned both the submission link and the author link in repeating results named “link.” We’ll worry about that later, but first: data extraction.
Step Two: Crawlbot
We then turned to Crawlbot, Diffbot’s on-demand crawling service that spiders a domain and automatically extracts data from pages using the appropriate Diffbot API.
(For a relatively well-structured site like the Domador archive this may have been overkill, but we’re dogfooding here.)
We set up our crawl as follows:
Page Type: We specified hn, the name of our newly created API.
Seed URL: http://hhn.domador.net/
Crawl URL Regex: http://hhn.domador.net/d{4}.* (this limits pages crawled to those within the Domador archive format, ignoring any ancillary pages/links)
Processed URL Regex: http://hhn.domador.net/d{4}/d{2}/d{2}/d{2}/ (each archive page takes the form http://hhn.domador.net/2013/04/01/12 — for April 1, 2013 at 12:00 — and this regex makes sure only pages that match that will be sent to the /api/hn API for extraction)
Crawlbot returns either a list of matching URLs, or a complete document download with all of the Diffbot API extractions. In this case, we opted for the latter, and set our crawl going. 25,000 or so URLs later we had a corpus of links.
Step Three: The Article API
We wrote a Python script to iterate through our local JSON copy and match user and thread URLs with the submitted URL. It resulted in this output for each frontpage submission:
{
 ""poster"": ""stonemetal"", 
 ""link"": ""http://arstechnica.com/tech-policy/news/2011/05/a-way-to-take-out-spammers-3-banks-process-95-of-spam-transactions.ars"", 
 ""thread"": ""http://news.ycombinator.com/item?id=2605580""
 }
That same script ran each “link” value through our Article API, which augmented the above with structured data from each post: title, author, full-text, date, tags, etc.
Step Four: Play Around
We thought we’d mine the results for interesting trends, then realized it would be much easier for us not to do any more work at all. This resulted in our HackerNews Trend Analyzer, which we’d love for you to play around with. Put any links to interesting trends you find in the comments (here or at HackerNews, naturally), and certainly let us know what else you’d like to know about the HN frontpage.",-0.06466,http://blog.diffbot.com/diffbots-hackernews-trend-analyzer/,,
http://support.diffbot.com/?p=530,d2017-01-19T19:48:12,,Diffbot,Application programming interface,"The images field of the Article API is a collection (an array of individual image objects), and, if overridden, requires that two fields be updated within the API Toolkit:
The repeating container or element that represents each image.
The actual image element (typically IMG) itself.
See this related article for help with the basics of a collection within the Toolkit. But as an example, let’s assume we need to create a rule to return images from this post. Let’s stick in a sample image of Diffy to play with:
First, edit the “images” rule, and find the CSS selector or element that appears for each image you want to retrieve:
In the case of this post, there is no containing div or similar element that wraps each image. Instead, each image is simply present within its own p element within the entry-content parent. So our repeating selector will be:
.entry-content p img
(It’s OK to use the img itself as your repeating element.)
This means that when returning images from this web site, the Article API will return every img within any p element within the .entry-content container.
Now, confirm the “url” field is representative of what you want to extract from each repeating object. By default the Toolkit will look for the src of the img element, and in this case, that’s exactly what we want:
Once you’ve confirmed or edited the “url” field, your subsequent API requests will return the overridden image values instead of the default Diffbot response.
Note that some additional fields — caption, image dimension information, etc. — will not be returned when an image is manually selected.
]]>",,http://support.diffbot.com/topics/apitoolkit/feed/,,
Does Diffbot handle non-English pages?,d2014-02-06T00:00,,Diffbot,"Diffbot,Computer vision","In a word, yes! Or, if you like:
Sí
Oui
Ja
???
??
HIja’
Because Diffbot APIs rely on computer vision, they tend to do very well identifying similar elements from pages in most languages. We also supply our algorithms with training data in multiple languages, so as to better identify linguistic and other differences.",0.1644,http://support.diffbot.com/automatic-apis/does-diffbot-handle-non-english-pages/,,
Do Diffbot APIs follow redirects?,d2014-02-12T00:00,,Diffbot,"URL,Diffbot","Yes, Diffbot extraction APIs will follow various forms of redirecting URLs, including 301 redirects and meta refresh redirects.
In these cases the API will return the resolvedPageUrl field indicating the final destination URL.
For Custom APIs and other domains with custom rules applied: rules can be applied based on either the original URL or the resolved URL, with resolved URL rules taking precedence. (If rules exist for both, the rules applied to the original rule will not take effect.)
Within Crawlbot: if you enter patterns or regular expressions to limit pages crawled or processed, these patterns will apply only to the original URL as detected within page links. Crawlbot will not resolve URLs when comparing against the urlCrawlPattern, urlProcessPattern, urlCrawlRegEx, or urlProcessRegEx fields.",-0.26413,http://support.diffbot.com/apitoolkit/do-diffbot-apis-follow-redirects/,,
Comparison of Web Data Providers: Alexa vs. Ahrefs vs. Diffbot,d2020-05-08T00:00,Merrill Cook,Diffblog,"Alexa Internet,Diffbot,Ahrefs,Data Providers,backlinks","Many cornerstone providers of martech bill themselves out as “databases of the web.” In a sense, any marketing analytics or news monitoring platform that can provide data on long tail queries has a solid basis for such a claim. There are countless applications for many of these web databases. But what many new users or those early in their buying process aren’t exposed to is the fact that web-wide crawlers can crawl the exact same pages and pull out extensively different data.
In this guide we’ll look at three of the largest purveyors of web data sourced from the entire public web: Alexa, Ahrefs, and Diffbot. We’ll aim to point out how these three providers have both legitimate claims to web-wide breadth within their databases, and provide categorically different data and knowledge products.
Skip to:
Link Data or Linked Data?
One of the first major differences between data provided from Ahrefs, Alexa, and Diffbot’s web crawlers can be hard to suss out from marketing materials.
This is largely due to the fact that it hinges on the difference of two letters.
Alexa and Ahrefs are enduringly-popular providers of backlink, keyword and SEO-related data. Their products provide access to some of the largest databases of literal links between pages on the web. Coupled with data on which pages rank for search queries as well as traffic estimates, both Alexa and Ahrefs are able to extrapolate the relative SEO strengths and popularities of sites across the web.
Ahrefs and Alexa crawl the web for link data that is then processed to provide SEO intelligence for marketers. The central component of these offerings is data on links and data extrapolated from links.
Data you may expect to gain from Ahrefs and Alexa web-wide crawls include:
The number and quality of backlinks on a domain
Which keywords a domain ranks for in search queries
Estimates on traffic and popularity of a site
The Domain Rank or Alexa Rank (depending on provider)
A suite of tools that leverage link data for lead generation and competitive analysis
Diffbot also crawls roughly the entire public web. And also stores entries for virtually every piece of publicly published content. But there are some key differences between what Diffbot data looks like at the end of the day. This is due to facts like:
Diffbot parses crawled content from across the web differently than both Ahrefs and Alexa
The characteristics and structure of Diffbot data entities are greatly different than Ahrefs and Alexa’s database entries
A fundamental difference here involves data “entities” versus “entries.”
You see, the results of Diffbot’s web-wide crawling (known as Diffbot’s Knowledge Graph™, or KG) are composed of a variety of types of entities. Unlike Alexa and Ahrefs, who provide entries on various link, traffic, and keyword metrics for websites (one entry type), Diffbot’s KG takes data on websites and processes it to provide facts on various entities found in the world.
The current list of entity types within Diffbot’s KG includes:
AdministrativeArea
Article
Corporation
Degree Entity
Discussion
EducationMajorEntity
EducationalInstitution
EmploymentCategory
Event
Image
Intangible
Landmark
LocalBusiness
Miscellaneous
Organization
Person
Place
Post
Product
Role
Skill
Or Video
One of the key features of these KG entities is that different types of facts are important for different types of entities. For example, a person entity may have a fact related to their current employer. Meanwhile, a corporation entity may have facts related to their subsidiaries and brands.
This flexibility, inherent to knowledge graphs, allows data returned by Diffbot’s web crawlers to meaningfully represent many types of “things.” With entity types ranging from products, to people, to discussions, this leads to many, many use cases.
A foundational difference between a knowledge graph and another “database of the web” is that knowledge graphs are comprised of entities that mirror the structure of what they’re representing. Additionally, a hierarchy — called an ontology — is implied between many entity types. For example, an award could be given to a university, but a university can’t be “given” to an award.
The rules and possibilities for this type of data structure can be summed up by the term “semantic data.”
Semantic data is data that’s encoded alongside the “meaning” of this data.
In the case of Diffbot’s KG, this means that facts attached to entities are included because they are judged to be a pertinent part of that entity type. They have “meaning” because:
They somehow mirror the structure of an entity out in the world.
They preserve useful relationships between entity types.
By way of another example, we know that seed funding round data has some meaning within the context of an organization’s history. It’s unlikely that a seed funding round has meaning for an article entity. (Now that would be a valuable piece of content!)
Knowledge graphs are such a natural fit for representing semantic data because they’re centered around the relationships between entities. Where traditional databases (even relational databases) are built to preserve the integrity of entries in the database, KG’s are built to preserve the relationships between entities.
This conceptual shift is exceedingly valuable when trying to represent objects from the world (as most all databases are trying to do). As most of the “meaning” we derive from observation comes in the form of the relationships between different entity types.
These types of relationships are explorable at a web-wide scale within Diffbot’s Knowledge Graph.
In summary, KG entity types are populated with semantic data that shows the relationships between entities in the world. Modelling facts in this way at a web-wide scale is extremely valuable for many use cases.
To return to the key differences between Alexa, Ahrefs, and Diffbot data, let’s summarize the most important points:
How web crawler data is parsed by Alexa, Ahrefs, and Diffbot:
Alexa and Ahrefs => arrange web crawler data on the features and composition of web sites
Diffbot => arranges web crawler data on what content from these websites actually means
The structure and characteristics of data from Alexa, Ahrefs, and Diffbot:
Alexa and Ahrefs => data on the number, quality and type of link data for one type of entry (web pages)
Diffbot => data parsed into facts placed within many contextually-linked entity types (organizations, people, products, web pages, and many more)
Just because a group of crawlers all crawl the entire web doesn’t mean the resulting data is remotely similar.
Web-Wide Versus Niche Web Data Extraction
At most recent count, the following web crawling stats relate to the three “databases of the web” in question:
Ahrefs: 5 billion web pages crawled a day. 16 trillion known links.
Alexa: Provides detailed data on top 30 million sites by popularity.
Diffbot: Extracts data on over 98% of the public web. Over 10 billion entities and 2 trillion facts.
Even the most minor websites are likely to get crawled by these three web crawlers.
By comparison, many niche web crawlers only crawl a specific subset of sites.
Examples of niche crawlers include:
Crawlers for ecommerce data from Amazon
Crawlers for contact data pointed at specific target sites
Crawlers for mentions on social media
And many more
Additionally, some crawlers may crawl a wider set of sites, but only look for specific data on those sites.
Examples of this form of niche crawlers include:
Old-school outreach “bots”
Crawlers that monitor discussions and reviews
Search engine index crawlers
Marketing data-related crawlers
In this latter sense, these crawlers may provide some of the “widest” crawls available. But they don’t meaningfully extract or interact with all aspects of the page that’s being crawled.
Both types of niche crawlers above are typically ordered around well-defined use cases for a small set of verticals or user roles. This isn’t a bad thing. As in many fields, both niche and more general providers do play valuable parts in a given ecosystem.
It’s also worth noting that when compared to other martech crawlers, Ahrefs and Alexa are anything but niche. They provide some of the widest and most up-to-date data on a wide range of marketing-related metrics.
With all that said, the range of data and entity types extracted from sites crawled by Diffbot is much wider than those extracted by Ahrefs and Alexa. They may be some of the largest web crawlers online, but their focus areas are niche compared to Diffbot.
This difference can be seen by comparing the primary use cases for Alexa, Ahrefs, and Diffbot.
Nearly all of Ahrefs and Alexas use cases can be found within Diffbot’s market intelligence and news monitoring use cases. This isn’t to say the resulting data of any one provider is better. Rather that the structure and nature of what Diffbot’s web crawler extracts has many more applications.
Primary Ahrefs Use Cases
Competitive Analysis
Keyword Research
Backlink Research
Content Research
Rank Tracking
Web Monitoring
Primary Alexa Use Cases
Keyword Research
Competitive Website Analysis
SEO Analysis
Checking Backlinks
Target Audience Analysis
Primary Diffbot Use Cases
Market Intelligence
Ecommerce
News Monitoring
Machine Learning
Web Scraping
Analysis of Ahrefs’ Database of the Web
Overview
Ahrefs provides the world’s largest index of live backlinks. Their crawlers continuously work their way through over 54 billion web pages and report changes to backlinks between pages. Their index of backlinks is updated roughly every 15 minutes with backlinks from newly (re-)crawled pages.
Ahrefs’ index of backlinks, coupled with keyword rankings for pages on the web allows Ahrefs to extrapolate to many forms of marketing information detailed below.
What type of data is available?
Backlink data
“Keyword difficulty” (how challenging it is to rank a page for a keyword)
What keywords a given page or site ranks for
Relative rankings on the strength of a domain or page
Competing page analysis
Content gaps
Data-driven keyword suggestions
Site scan data related to SEO
Estimated traffic
Features
Site Explorer
Keyword Explorer
Site Audit
Rank Tracker
Content Explorer
Link Intersect
Batch Analysis
Browser Plugins
Use Cases
Competitive Intelligence
Web/News Monitoring
Keyword-Related Marketing Research
SEO Analysis
Lead Generation
Who It’s For
Marketers
Publishers
Growth Teams
SEO Professionals
PPC Professionals
Public Relations Professionals
Content Marketers
Webmasters
Pricing
$99-$999/Month
$179/Month Standard Plan
Analysis of Alexa’s Database of the Web
Overview
Alexa provides information on the relative popularity of domains through the sampling of traffic patterns provided by individuals who utilize the Alexa plugin. Popularity is extrapolated from the number of unique visitors who visit a domain as well as the overall traffic to a domain. Additionally, Alexa crawls a large portion of the entire web to extract backlink data.
This link data, in conjunction with data on which content is ranking for which keywords on search engines is used to provide keyword and PPC recommendations and tracking. Perhaps the most unique offering of Alexa is one of the largest publicly available indexes of traffic patterns online. These are offered in the form of the Alexa traffic rank for pages as well as an insights panel related to audience analysis.
What type of data is available?
Web traffic data
Audience demographics
Backlink data
“Keyword difficulty” (how challenging it is to rank a page for a keyword)
What keywords a given page or site ranks for
Relative rankings on the strength of a domain or page
Competing page analysis
Content gaps
Share of “voice” in a given niche
Features
Site audits
Organic keyword research
PPC keyword research
Web competitor analysis and research
Keyword analysis
Traffic analysis
Audience analysis
Share of voice analysis
Site comparison
Browser plugin
Use Cases
Marketers
Publishers
Growth Teams
SEO Professionals
PPC Professionals
Public Relations Professionals
Content Marketers
Webmasters
Pricing
$79-$299/Month
$149/Month Standard “Advanced” Plan
Analysis of Diffbot’s Database of the Web
Overview
Diffbot sources Knowledge Graph™ data through web-wide crawls that are then parsed by a cutting-edge machine vision and natural language processing AI system. Data is organized into entity types that are interlinked and populated by facts. Data within Diffbot entities is semantic, meaning that data is encoded next to the “meaning” or context of that data. At a general level, Diffbot takes unstructured data from billions of sites across the web and provides this data structure within the world’s largest knowledge graph.
Diffbot also offers customizable data extraction solutions including Crawlbot as well as custom Extraction APIs.
What type of data is available?
Organizational (firmographic) data
Person data
Product data
News mention data
Other entity data
Features
Access to structured entity data extracted from pages across the web (Knowledge Graph)
Data enrichment (Enhance)
Bulk crawler
Custom Extraction APIs
Excel, Google Sheets, Tableau integrations
Use Cases
Market Intelligence
News Monitoring
Ecommerce
Machine Learning
Web Scraping
Pricing
$299-$899/Month
$299/Month Standard “Startup” Plan",0.424,https://blog.diffbot.com/comparison-of-web-data-providers-alexa-vs-ahrefs-vs-diffbot/,,
Using Diffbot Proxy Servers / Proxy IPs,d2016-04-30T00:00,,Diffbot,"Proxy server,Diffbot,IP address","In some cases—when crawling or processing data from certain sites—you may need to diversify the IP addresses of your requests. In this event you can utilize Diffbot’s fleet of proxy IPs to more consistently retrieve results.
Diffbot offers two levels of proxy IPs:
Our default proxy servers are usable for most sites and at most volumes. Usage of these proxies incurs an additional API call for each page processed: each page processed using a proxy will count as two API calls.
Our dynamic proxy servers effectively offer a new IP address for each request, and are usable for even the most difficult-to-crawl sites. Usage of dynamic proxy servers is limited to Plus, Professional or Enterprise customers, and pricing is dependent on data volume.
Additional notes:
Some popular sites always require proxies. These domains have proxies enabled globally and will incur a two-API-call rate for all tokens.
Details on your account’s proxy usage will be available via our Account API, in your Developer Dashboard, and in your monthly invoices.
The use of proxies will likely increase the response time of individual API calls. (See suggestions for improving API response times.)
If you would like to utilize proxies for specific sites or individual crawls, please contact Diffbot Support.",-0.2015,http://support.diffbot.com/crawlbot/using-diffbot-proxy-servers-proxy-ips/,,
Diffbot ??????????????????,d2015-04-01T00:00,Anthony Ha,TechCrunch,"Facebook,????,????,WordPress,Automattic,?????,??????","????????? Diffbot ???????·?(MikeTung)??,Diffbot ????“????????????????”??,Diffbot ??????????????????????????????
???·???????????????,???????????????????,??????,?????????????????????????????????????????????(Diffbot ????????????? eBay?)
????,Diffbot ???????? ???? ????,??????????????????????
?? ·???,?????????,??????????????,???????? JavaScript ????,????????????——???“???????,”?????,??????????????????????????????,?? Diffbot ?????“???????????”?
?????? Diffbot ? ???? ????:?????,????? Diffbot ???????????????????,???????????????? ???? ,??????????????????——??????????????——??????????????????
????????????????,??????????——??·???,???????????????,???????????????,??????“????????????”????,??????????????????????????
Diffbot ??,????? Discussions API ?? Facebook Comments?Disqus?Livefyre?WordPress?Blogger?Automattic ? Intense Debate?Kinja?Hacker News ? Reddit ??
??:??",,http://techcrunch.cn/2015/04/01/diffbot%E5%88%A9%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90%E7%BD%91%E9%A1%B5%E5%B9%B6%E6%8A%93%E5%8F%96%E5%85%B3%E9%94%AE%E5%86%85%E5%AE%B9/,,
Can multiple Diffbot extraction APIs be used in a single crawl?,d2017-01-19T00:00,,Diffbot,Diffbot,"Crawlbot crawls are meant to work with a single Diffbot extraction API. If you wish to process multiple types of pages through separate APIs, your options are:
Use the Analyze API
The Analyze API will automatically determine the page-type of each page crawled, and structure the data from supported pages. This content can be filtered using the Search API, or the JSON downloaded in full and filtered using the type field.
Set-up Multiple Crawls
To explicitly use multiple APIs for crawling a single site, you’ll need to set-up multiple crawls, each using an independent API. You can use Crawlbot’s multiple mechanisms for controlling/narrowing your crawls to ensure that each separate crawl job only processes the right type of pages.",-0.29899,http://support.diffbot.com/crawlbot/can-multiple-diffbot-extraction-apis-be-used-in-a-single-crawl/,,
How Diffbot’s Automatic APIs Helped Topic’s Content Marketing App Get To Market Faster,d2020-05-14T00:00,Merrill Cook,Diffblog,content marketing,"The entrepreneurs at Topic saw many of their customers struggle with creating trustworthy SEO content that ranks high in search engine results.
They realized that while many writers may be experts at crafting a compelling narrative, most are not experts at optimizing content for search. Drawing on their years of SEO expertise, this two-person team came up with an idea that would fill that gap.
They came up with Topic, an app that helps users create better SEO content and drive more organic search traffic.They had a great idea. They had a fitting name. The next step was figuring out the best way to get their product to market.
The Problem
Topic needed a scalable and precise way that a 2-person team could extract article content and metadata from top search result articles from across the web.
Key Takeaways
Rule-based web extraction is hard to scale when dealing with pages from across the web
Knowledge-as-a-service circumvents the cost of messy data and maintaining freshness in extracted data
Extracted data is only as good as its structure and metadata
The Solution",,https://blog.diffbot.com/how-diffbots-automatic-apis-helped-topics-content-marketing-app-get-to-market-faster/,,
Diffbot Infrastructure with Mike Tung,d2019-10-15T00:00,SE Daily,Software Engineering Daily,"San Francisco,Mike Tung,structural geology,PagerDuty,American Eagle Outfitters,Diffbot,LiveCode,World Wide Web,We Edit Podcasts,Software Engineering Daily","Diffbot is a knowledge graph that allows developers to interface with the unstructured web as if it was a structured database. In today’s show, Diffbot CEO Mike Tung returns for a second discussion about how he has built Diffbot and how Diffbot is used.
The web has many different entities. Web pages, topics, people, stories, articles, companies, and much more. Humans use a search engine to find answers to their questions within web pages. Machines need to find answers to these kinds of questions as well, but a machine is not sophisticated enough to figure out answers from an unstructured web page.
Diffbot brings structure to those webpages, and gives them an API interface for developers to build on top of. In order to create this system in a cost-efficient manner, Diffbot runs its own data centers, where web scraping, machine learning, and API infrastructure are all used to build the Diffbot application.
Mike joins me for an interview about creating Diffbot, as well as his strategy for running the business.
Sponsorship inquiries: sponsor@softwareengineeringdaily.com
Check out our active projects:
We are hiring a head of growth. If you like Software Engineering Daily and consider yourself competent in sales, marketing, and strategy, send me an email: jeff@softwareengineeringdaily.com
The SEDaily app for iOS and Android includes all 1000 of our old episodes, as well as related links, greatest hits, and topics. Subscribe for ad-free episodes.
Transcript
Transcript provided by We Edit Podcasts. Software Engineering Daily listeners can go to weeditpodcasts.com/sed to get 20% off the first two months of audio editing and transcription services. Thanks to We Edit Podcasts for partnering with SE Daily. Please click here to view this show’s transcript.
Sponsors
PagerDuty helps your company’s digital operations run more smoothly. PagerDuty helps you intelligently pinpoint issues like outages, as well as capitalize on opportunities, empowering teams to take the right, real time action. To see how companies like GE, Vodafone, Box and American Eagle Outfitters rely on PagerDuty to continuously improve their digital operations visit PagerDuty.com.
Cruise is a San Francisco-based company building a fully electric self-driving car service. Cruise is a place where you can build on your existing skills while developing new skills and experiences that are pioneering the future of industry. There are opportunities for backend engineers, frontend developers, machine learning programmers, and many more positions. At Cruise you will be surrounded by talented, driven engineers-–all while helping make cities safer and cleaner. Apply to work at Cruise, by going to getcruise.com/careers.
Vettery is an online hiring marketplace that connects highly qualified workers with top companies. Vettery keeps the quality of workers and companies on the platform high, because they vet both workers and companies. Check out vettery.com/sedaily, and get a $300 sign-up bonus if you accept a job through Vettery.
MongoDB is the most popular document-based database built for modern application developers and the cloud era. Try MongoDB today with Atlas, the global cloud database service that runs on AWS, Azure, and Google Cloud. Configure, deploy, and connect to your database in just a few minutes. Check it out at mongodb.com/atlas.",,https://softwareengineeringdaily.com/2019/10/15/diffbot-infrastructure-with-mike-tung/,,
Comparison of Web Extraction Providers: Import.io vs. Diffbot,d2020-05-26T00:00,Merrill Cook,Diffblog,"Diffbot,machine learning","Harnessing the public web as data is one of the smartest things product, marketing, PR, and machine learning teams can do. It also opens up a host of questions.
What type of data is valuable to us?
How accurate do we need our data to be?
How timely do we need our data to be?
What are our data sources?
The answers to these questions will inform many aspects of your web data strategy. And should inform whether you provide for your data extraction and maintenance needs in house or from a firm that specializes in web extraction.
In this guide we’ll look at two of the most prominent web data extraction providers: Diffbot and Import.io.
We’ll look at the process of extracting data from the web through both service providers and expose some foundational differences about what data is extracted, and how data is processed (or not).
Additionally, we’ll look at categorical differences between what these providers supply.
Notably, the difference between a web data extraction provider and Knowledge-As-A-Service provider.
Table of Contents:
How to Extract Data with Import.io
Import.io offers one of the most beginner-friendly web extraction solutions available. For data that is already somewhat structured, simply inputting a URL will pull up a rendering of the page and what data Import.io believes you want extracted.
For well-known sites and single pages, this typically works as expected. We tried it out on Amazon search results for “Vitamin C.” The resulting fields of data included:
Product title
Product Page Link
Pricing Data
Sale Pricing Data
Availability
Average review scores
Essentially, Import.io quickly extracted all visual and non-visual front end data from items on the search result page.
Additionally you can train the extractor on multiple pages. This is helpful in the event you’re going to want to extract across paginated pages or pages that are slightly different to one another.
Once you save your scraper you can set your scrape to a given schedule as well as import a list of urls to extract from. With the help of a crawling tool such as Screaming Frog one can quickly input entire sections of sites to be crawled.
Granted, Import.io works right out of the box with selection of elements to be extracted via the UI for many major sites. For sites where data is less well structured, or where you may need to perform a complex series of interactions to get to the data, however, less technical users may be up for a challenge. Xpath and Regular Expressions can be used to hone what Import.io is trying to extract. But those are also processes many less technical users may not be up for.
Web Data Integration Solutions
Now extraction of single or small batches of pages are really just the start of what Import.io does. Import.io bill themselves out as providers of web data integration solutions. What they mean by this is that they provide support for some post-extraction processing as well as the consuming of these data feeds.
In particular, Import.io offers the following features for Premium account holders:
The ability to set up interactions with a page to surface data you’re after
Data transforms that take a data feed, and process the data in some pre-specified manner before being sent your way
The ability to only extract data that has changed since the last data extraction process
Reporting on when extractions were run, the results of extractions, and changes to your data
Visualization of data
Depending on your tier of service, a team of data extraction specialists to help get your data extraction set up
Together these features can take a one-off extraction and turn it into an ongoing data stream to be consumed in spreadsheets, dashboards, or apps of your choosing.
Import.io is trusted by many industry leaders and holds a strong place within the web data extraction industry. But as we’ll see in the following section, there are categorical differences between aid in data extraction (as Import.io provides) and the creation of web-based knowledge.
How to Extract Data with Diffbot
Diffbot’s mission is to help individuals turn unstructured web data into insightful structured information. We’re a Knowledge-as-a-Service provider, and we accomplish this through several routes. While Diffbot’s web data extraction products are largely built on the same underlying technologies, there’s likely one or two products that will help you to achieve your unique web data sourcing needs more directly.
The three primary ways to gain access to extracted data via Diffbot include:
Diffbot’s Knowledge Graph
Diffbot’s Automatic Extraction APIs are some of the underlying tech that powers Diffbot’s other products. While not the preferred extraction method for non-technical users, it’s worth being aware of how the Automatic Extraction APIs work if you’re going to be using any of Diffbot’s products.
These powerful extraction APIs are built to provide rule-less data extraction for page types that comprise more than 98% of the public web. Machine vision and natural language processing systems in our extraction APIs don’t just extract data from the pages you provide, they understand what the data means.
This means you aren’t just pulling unstructured data from the web and manipulating its formatting. Rather, you’re extracting unstructured data and letting our cutting-edge AI turn it into information (for more on this, read about the DIKW Pyramid).
Our Automatic Extraction APIs are broken into individual APIs depending on the type of data you want to extract. These include:
Additionally, if you want all data types returned from a given domain, you can point the Analyze API to a location.
The results of your extraction via one of the above APIs mirrors the structure of data contained in Diffbot’s Knowledge Graph. You can think of our Knowledge Graph as the result of pointing the above APIs at billions of web pages every few days, followed by additional processing to ensure the proper assimilation of facts from across the web.
Remember how we mentioned that our AI-enabled data extraction actually understands the meaning of the data being extracted? Well explaining the Knowledge Graph is a great way to show exactly what we mean by this.
The Knowledge Graph essentially crawls billions of pages every several days. Facts about different entity types are extracted. And based on a sophisticated process that gauges the reliability of these facts, these facts are included within the records of individual entities.
What are entities, you ask?
For Diffbot’s Knowledge Graph, entities are collections of records that represent organizations, people, or “things.” Knowledge Graph entities are meant to represent something out in the world (and that’s talked about on the public web) that are useful for human purposes. And so our list of entity types expands over time.
Presently, the types of entities you can gain information on include:
AdministrativeArea
Article
Corporation
Degree Entity
Discussion
EducationMajorEntity
EducationalInstitution
EmploymentCategory
Event
Image
Intangible
Landmark
LocalBusiness
Miscellaneous
Organization
Person
Place
Post
Product
Role
Skill
Or Video
Each of these entity types contains a collection of facts pertinent to that entity type, and judged to be the most accurate (as there are often conflicting “facts” online). Additionally, entities are connected to one another. For example, a collection of person entities will comprise a list of employees within a corporation entity. And a list of skills will be present inside of a person entity.
Because Knowledge Graph entities are structured to preserve the relationships between different entities, much more valuable questions can be asked of the Knowledge Graph than other web data sources.
Example questions you could ask the Knowledge Graph:
What was the sentiment score of all articles written about a competitor product in Korean?
How many individuals with data science skills work for Intel?
What’s the sales price of an internationally sold product when it’s sold in India?
And many others…
Now you may be wondering what this has to do with data extraction…
Many web data providers provide the ability to pull data from an unstructured page and structure it. In the case of the Knowledge Graph, we’ve already extracted data from virtually the entirety of the public web. The data extraction is taken care of so you can worry about what you’re going to do with your web data.
While other tools may aid you in extracting data by yourself, Diffbot’s Knowledge Graph lets you leave data extraction to the experts so that you can move on to other more central issues.
While the Knowledge Graph is at the center of Diffbot’s web data universe, we mentioned our third route to extracted data earlier in this section and we’ll cover that now. Diffbot Enhance is a data enrichment tool meant to update or flesh out data that you may already have.
Enhance runs over the underlying Knowledge Graph with a separate matching algorithm geared towards finding specific entities. Where Knowledge Graph is meant for searching across extracted data, Enhance is meant for finding specific entries and ensuring their accuracy.
Enhance comes with a range of integrations including Excel, Google Sheets, and Tableau. This allows non-technical users to quickly pull in the power of the Knowledge Graph to enrich existing data in many of the most popular productivity softwares. As with the Knowledge Graph itself, Enhance allows you to skip the data extraction phase of knowledge work and simply draw from our comprehensive already-extracted KG.
Web Data Extraction Versus Knowledge-As-A-Service
While we touched on this difference to some extent in the last section, in this section we’ll look over some of the key conceptual differences between what Diffbot and Import.io provide.
Both Import.io and Diffbot provide web data extraction tools. These are tools that can pull unstructured data from a page and transform this data in some way so that it is more usable.
Web data extraction tools commonly require you (or someone on your team) to:
Locate the web data you need
Determine how regularly you need your data updated
Determine what format you want to consume the extracted data in
Configure extraction tools (depending on how unstructured the data is)
Wrangle, cleanse, and transform the data extracted
This process takes up more than 80% of many data teams’ time and resources.
Knowledge-As-A-Service providers like Diffbot are built on the new understanding that data aggregation, verification, and upkeep costs can quickly overrun the value of the data itself.
While Diffbot’s Automatic Extraction APIs ARE web data extraction tools, they’re structured to return results in formats similar to our Knowledge Graph or Enhance products. This allows users to skip time consuming data wrangling, cleansing, and transforming steps within the data life cycle. Even Diffbot’s most extraction-focused product is a far cry from many competing web extraction services.
Furthermore, our Automatic Extraction APIs are just part of Diffbot’s data sourcing continuum. Within many use cases, Diffbot users employ multiple web data tools, enabling the full benefits of Knowledge-As-A-Service. Namely, that Diffbot’s Knowledge Graph and Enhance products cut up to 80% of time from a typical data lifecycle.
One final note is that there is a key qualitative difference between extracted data, and semantic web data parsed into information. You can organize tabular data into useful structures. You can ask semantic data complex questions in human language.
This allows data teams and decision makers to spend more time asking impactful questions of their data, and less time simply questioning their data.
All three products (as well as Crawlbot, a related web crawling tool) are available in all tiers of Diffbot’s service plans. Interested in what our Knowledge Graph, Automatic Extraction APIs, or Enhance can do for you? Try us out with a 14-day free trial today!
Analysis of Import.io Offerings
Import.io Overview
Import.io is one of the most beginner-friendly web data extraction platforms available. For sites with semi-structured data, Import.io almost always works directly out-of-the-box. For more convoluted extraction sites, Regular Expressions and XPath can make your extractions more precise.
Among paid plans users can set their extraction on a timer, choose to only update fields that have changed, and employ a range of preset data transforms on extraction data. Additionally, Import.io offers several methods to visualize extracted data within custom dashboards.
What type of data is available?
Text Data
Pricing Data
Link Data
Image Data
Import.io Features
A visual UI for selecting elements you want extracted from a page
The ability to extract data on a timer
Reports on how scheduled data extractions change over time
The ability to visualize extracted data
The ability to customize extractions with Regular Expressions and XPath
Import.io Use Cases
Market Intelligence
Pricing Data Extraction
Import.io Pricing
$0/Month Community Tier For 1,000 pages extracted a month
$299/Month for lowest paid tier per user
Analysis of Diffbot Offerings
Diffbot Overview
Diffbot sources Knowledge Graph™ data through web-wide crawls that are then parsed by a cutting-edge machine vision and natural language processing AI system. Data is organized into entity types that are interlinked and populated by facts. Data within Diffbot entities is semantic, meaning that data is encoded next to the “meaning” or context of that data. At a general level, Diffbot takes unstructured data from billions of sites across the web and provides this data structure within the world’s largest knowledge graph.
Diffbot also offers customizable data extraction solutions including Crawlbot as well as custom Extraction APIs.
What type of data is available?
Organizational (firmographic) data
Person data
Product data
News mention data
Other entity data
Diffbot Features
Access to structured entity data extracted from pages across the web (Knowledge Graph)
Data enrichment (Enhance)
Bulk crawler
Custom Extraction APIs
Excel, Google Sheets, Tableau integrations
Diffbot Use Cases
Market Intelligence
News Monitoring
Ecommerce
Machine Learning
Web Scraping
Diffbot Pricing
$299-$899/Month
$299/Month Standard “Startup” Plan
Interested in how Diffbot can help your news monitoring or research use cases (among others)? Check out our recent news monitoring case study, or sign up for a free 14-day trial today!",0.491,https://blog.diffbot.com/comparison-of-web-extraction-providers-import-io-vs-diffbot/,,
Diffbot:?????????????????,d2015-03-31T00:00,,cyzone.cn,"??,??,????,????,??,??,????,EBay,??,???????","????????,?????????,????,??????????????????????,????????????????????????????Diffbot???,????????????,?“??”???????,??????????????????????
??????????Diffbot?30???,????????????????????????API?
Diffbot?????????,????????,?“??”??????,??????,??????????????????
??????????Mike Tung 30?????????????,??Diffbot??????????????????????,????????????????????
“????????,????????Yelp??????,???????Yelp??API??,???Diffbot,????????????”Tung??
?Diffbot????,?????????????????,???????????????
??,?????????????“??????:??????????”?????,??????,????????4???????,????“??”?“??”?“??”?“????”?“??”(??????)?“??”???????,????????????????
Tung?,Diffbot????,??????????????????,??????????????
?????“??”??????,Diffbot?2009?????,?????????????,??2012??????200??????,???????????,????12??
Tung???,Diffbot???????????????,???????????????Diffbot????????????Stanford StartX?????,????????Sun???????????Andreas Bechtolsheim??????
??,Diffbot??????????eBay????????????????,Diffbot???4???????:??14?????,??1??API???????299??,??25??API??,???????????0.001??,????????????“??”?“??API”?,????4999??,??500??API??,???????????0.0009??,????????????
Tung?,Diffbot???????API??????“???”??,??????????,????????????????????
“?????????????,”Tung?,“?????????????????,?????????‘???’??????????”",,http://www.cyzone.cn/a/20150331/271569.html,,
Introducing the Diffbot Knowledge Graph,d2018-10-02T00:00,Diffy,Diffblog,"Artificial intelligence,Diffbot,Knowledge Graph,IBM,Knowledge","Diffbot is pleased to announce the launch of a new product: Diffbot Knowledge Graph.
Eight years ago, Diffbot revolutionized web data extraction with AI data extractors (AI:X). Now, Diffbot is set to disrupt how businesses interact with data from the web again with the all-new DKG (Diffbot Knowledge Graph).
“What we’ve built is the first Knowledge Graph that organizations can use to access the full breadth of information contained on the Web. Unlocking that data and giving organizations instant access to those deep connections completely changes knowledge-based work as we know it.”
– Mike Tung, founder and CEO of Diffbot.
Unlocking knowledge from the Web
Ever wished there was a search engine that gave you answers to your questions with data, rather than a list of links to URLs?
Using our trademark combination of machine learning and computer vision the DKG is curated by AI and built for enterprize, unlocking the entire Web as a source of searchable data. The DKG is a graph database of over 10 billion connected entities (people, companies, products, articles, and discussions) covering over 1+ trillion facts!
In contrast to other solutions marketed as Knowledge Graphs, the DKG is:
Fully autonomous and curated using Artificial Intelligence, unlike other knowledge graphs which are only partially autonomous and largely curated through manual labor.
Built specifically to provide knowledge as the end product, paid for and owned by the customer. No other company makes this available to their customers, as other knowledge graphs have been built to support ad-based search engine business models.
Web-wide, regardless of originating language. Diffbot technology can extract, understand, and make searchable any information in French, Chinese, and Cyrillic just as easily as in English.
Constantly rebuilt, from scratch, which is critical to the business value of the DKG. This rebuilding process ensures that DKG data is fresh, accurate, and comprehensive.
Why?
A Web-wide, comprehensive, and interconnected knowledge graph has the power to transform how enterprises do business. In our vision of the future, human beings won’t spend time sifting through mountains of data trying to determine what’s true. AI is so much better at doing that.
Right now, 30 percent of a knowledge worker’s job is data gathering. There’s a big opportunity in the market for a horizontal knowledge graph — a database of information about people, businesses, and things. Other knowledge graphs are little more than restructured Wikipedia facts with the simplest, most narrow connections drawn between. We knew we could do better. So we’re building the first comprehensive map of human knowledge by analyzing every page on the Internet.
Knowledge is needed for AI
The other reason we’re building the DKG is to enable the next generation of AI to understand the relationships between the entities in the world it represents. True AI needs the ability to make informed decisions based on deep understanding and knowledge of how entities and concepts are linked together.
We’ve already seen some fantastic research from universities and industry built on top of the DKG – including the particularly interesting creation of a state-of-the-art Q&A AI, which has been very impressive.
Evolution from Data to Knowledge
There is a subtle but pivotal difference between data and knowledge. While data helps many businesses, knowledge has the power to be transformative for any business.
Define “Data”:
Facts and statistics collected together for reference or analysis.
Define “Knowledge”:
Facts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject.
The key to the DKG’s value is how it encompasses the whole Web, and how it joins together all the data points from many sources into individual entities, and – importantly – how it then connects those entities together according to their relationships.
By building a practical contextual understanding of all data online, the DKG is able to answer complex questions like: “How many people with the skill “JAVA” who used to work at IBM as a junior, now work at Facebook as a senior manager?” by providing you with a number and a list of people who meet the criteria.
To access the DKG, Diffbot created a search query language called Diffbot Query Language (DQL). It’s flexible enough to let you perform granular searches to find the one exact piece of information you need out of the trillions, or to gather massive datasets for broad analysis. DQL has all the tools you need to access the world’s largest knowledge source with highly accurate, precise searches.
Ready to Use Now
Now, any business that wants instant access to all of the world’s knowledge can simply sign up for the DKG and turn the entire Web into their personal database for business intelligence across:
People: skills, employment history, education, social profiles
Companies: rich profiles of companies and the workforce globally, from Fortune 500 to SMBs
Locations: mapping data, addresses, business types, zoning information
Articles: every news article, dateline, byline from anywhere on the Web, in any language
Products: pricing, specifications, and, reviews for every SKU across major ecommerce engines and individual retailers
Discussions: chats, social sharing, and conversations everywhere from article comments to web forums like Reddit
Images: billions of images on the web organized using image recognition and metadata collection
Want to learn more about the Diffbot Knowledge Graph?",-0.25879,http://blog.diffbot.com/introducing-the-diffbot-knowledge-graph/,,
The Diffbot Master Plan (Part One),d2019-09-03T00:00,Mike Tung,Diffblog,"startup company,Bing,comprehensive planning,Hacker News,Diffbot,Matt Wells","Our mission at Diffbot is to build the world’s first comprehensive map of human knowledge, which we call the Diffbot Knowledge Graph. We believe that the only approach that can scale and make use of all of human knowledge is an autonomous system that can read and understand all of the documents on the public web.
However, as a small startup, we couldn’t crawl the web on day one. Crawling the web is capital intensive stuff, and many a well-funded startup and large company have gone bust trying to do so. Many of those startups in the late-2000s all raised large amounts of money with no more than an idea and a team to try to build a better Google. However they were never able to build technology that is 10X better before resources ran out. Even Yahoo eventually got out of the web crawling business, effectively outsourcing their crawl to Bing. Bing was spending upwards of $1B per quarter to maintain a fast-follower position.
As a bootstrapped startup starting out at this time, we didn’t have the resources to crawl the whole web nor were we willing to burn a large amount of investors’ money before proving the technology to ourselves.
So, we just decided to start developing the technology anyways, but without crawling the web.
We started perfecting the technology to automatically render and extract structured data from a single page, starting with article pages, and moving on to all the major kinds of pages on the web. We launched this as a paid API on Hacker News for developers, which meant that the only way we would survive was if the technology provided something of value that was better than what could be produced in-house or by off-the-shelf solutions. For many kinds of web applications automatically extracting structure from arbitrary URLs works 10X better compared to the approach of manually creating scraping rules for each site and maintaining these rulesets. Diffbot quickly powered apps like AOL, Instapaper, Snapchat, DuckDuckGo, and Bing, who used Diffbot to turn their URLs into structured information about articles, products, images, and discussion entities.
This niche market (the set of software developers that have a bunch of URLs to analyze) provided us a proving grounds for our technology and allowed us to build a profitable company around advancing the state-of-the-art in automated information extraction.
Our next big break came when we met Matt Wells, the founder of the Gigablast search engine, who we hired as our VP of Search. Matt had competed against Google in the first search wars in the mid-2000s (remember when there were multiple search engines?), had achieved a comparably-sized web index, with real-time search, with a much smaller team and hardware infrastructure. His team had written over half a million lines of C++ code to work out many of the edge-cases required to crawl the 99.999% of the web. Fortunately for us, this meant that we did not have to expend significant resources in learning how to operate a production crawl of the web, and could focus on the task of making meaning out of the web pages.
We integrated the Gigablast technology into Diffbot, essentially adding a highly optimized web rendering engine and our automatic classification and extraction technology to Gigablast’s spidering, storage, search, and indexing technology. We productized this as a product called Crawlbot, which allowed our customers to create their own custom crawls of sites, by providing a set of domains to crawl. Crawlbot worked as a cloud-managed search engine, crawling entire domains, feeding the urls into our automatic analysis technology, and returning entired structured databases.
Crawlbot allowed us to grow the market a bit beyond an individual developer tool to businesses that were interested in market intelligence, whether about products, news aggregation, online discussion, or their own properties. Rather than passing in individual URLs, Crawlbot enabled our customers to ask questions like “let me know about all price changes across Target, Macys, Jcrew, GAP, and 100 other retailers” or “let me build a news aggregator for my industry vertical”. We quickly attracted customers like Amazon, Walmart, Yandex, and major market news aggregators.
In the course of offering both the Extraction APIs and Crawlbot, our machine learning algorithms analyzed over 1 Billion URLs each month, and we used the fraction of a penny we earned on each of these calls to build a top-tier research team to improve the accuracy of these machine learning models. Another side-effect of this business model is that after 50 months, we had processed 50 billion URLs (and since our customers pay for our analysis of each URL, they are incentivized to send to us the most useful URLs on the web to process).
50 billion URLs is pretty close to the size of a decent crawl of the web (at least the valuable part of the web), and so we had confidence at this point that our technology could scale, both in terms of computational efficiency, as well as accuracy of the machine learning, as required by our demanding business customers. Because we had paid revenue from leading tech companies, we were confident that our technology surpassed what could be built in-house by their engineers. We had achieved many firsts: doing a full rendering at web scale and running sophisticated multi-lingual natural language processing and computer vision algorithms at web scale. So at this point, we started our own crawl of the web to fill in the gaps.
Our goal was to allow you to query for structured entities found anywhere across the web, but had to account for the fact that an entity (e.g. a person or a product) could appear on many pages on the web, and each appearance could contain differing sets of information with varying degrees of freshness. We had to solve the problem of entity resolution, which we call record linking, and resolving conflicts in the facts about the entity from different sources, which we call knowledge fusion. With these machine learning components in place, we were able to build a consistent universal knowledge graph, generated from an autonomous system from the whole web, another first. We launched the Knowledge Graph last year.
So in short, here is our roadmap so far:
Build a service that analyzes URLs using machine learning and returns structured JSON
Use the revenue and learnings from that to build a service on top of that to crawl entire domains and return structured databases
Use the revenue and learnings from that to build a service that allows you to query the whole web like a database and return entities
While the Extraction APIs and Crawlbot served a very Silicon-valley centric developer audience, due to its requirement of passing in individual urls or domains, the Knowledge Graph serves the much larger market of information professionals with business questions. This includes the market of business analysts, market researchers, salespersons, recruiters, and data scientists, a much larger segment of the population as compared to software developers.
As long as a question can be formed as a precise statement (using the Diffbot query language) it be answered by the Diffbot Knowledge Graph, and all of the entities and facts that match the query can be returned no matter where they originally appeared on the web.
Knowledge workers rely on the quality of information for their day-to-day work. They demand ever-improving accuracy, freshness, comprehensiveness, and detail, metrics that are aligned with our mission to build a complete map of human knowledge. We have an opportunity to build the first power tool for searching the web for knowledge professionals, one that is not ad-supported freeware, but where our technical progress is aligned with the values of our customers.",,https://blog.diffbot.com/the-diffbot-master-plan-part-one,,
http://support.diffbot.com/accounts-and-billing/does-diffbot-offer-manual-invoicing-custom-terms-or-other-payment-options/,d2016-06-15T23:17:38,,Diffbot,"Diffbot,Invoice,Payment,Web page,Uniform Resource Locator,Application programming interface","Diffbot pricing is based on the number of individual web pages extracted by our Automatic or Custom APIs. Each time you send a URL to a Diffbot API it counts as an API call. Repeated pages are counted as additional API calls.
Analyze API Requests
Pages sent to the Analyze API count as a single API call, even if the submitted link results in an article, product, discussion or other page-type extraction. (There is no double-charge for extracted pages.)
Multiple-Page Articles, Discussions or Custom APIs
If an article or discussion includes multiple concatenated pages — read more about page-concatenation — each individual page will count as a separate API call.
Crawlbot API, Bulk API and the Search API
Individual URLs sent via the Bulk API count as individual API calls. E.g., a bulk job containing 500 URLs will incur 500 API calls.
There is no charge for use of Crawlbot or our Bulk API; calls to those APIs are not charged. Within a crawl, pages that are merely crawled for links are not counted as API calls. Any pages processed within a crawl count as API calls. (Read about the difference between “crawled” and “processed” pages.)
Search API requests for download of individual crawl or bulk job data are not charged.
Proxy Usage
Calls that require use of Diffbot proxy servers will bill at a rate of 2x; that is, each page processed using a proxy will count as two API calls.
Error Responses
Requests that result in a Diffbot error response are not billed or counted as API calls.
]]>",,http://support.diffbot.com/topics/accounts-and-billing/feed/,,
"Does Diffbot offer manual invoicing, custom terms or other payment options?",d2016-06-15T00:00,,Diffbot,"Custom (law),Custom,Diffbot,Invoice,Payment","If you are unable to procure Diffbot services using a credit card, or need other custom billing or vendor approval arrangements made, we are happy to work with you (and/or your purchasing team) to help you start getting web data.
Custom account services or alternative billing arrangements require either a prepaid annual plan (at our Startup or Plus levels), or monthly billing of a Professional or Enterprise account. We are unable to provide custom billing or custom terms for Startup or Plus monthly billing.
For help getting started, contact your Account Manager or simply write support@diffbot.com.",-0.46146,http://support.diffbot.com/accounts-and-billing/does-diffbot-offer-manual-invoicing-custom-terms-or-other-payment-options/,,
http://support.diffbot.com/?p=540,d2017-01-19T23:16:12,,Diffbot,"Regular expression,Application programming interface","Sometimes a single site needs multiple custom rules, perhaps due to template differences or because you wish to extract different data from different types of pages.
If you’re creating a completely custom API, you can always create multiple APIs for the same site. For instance:
/api/categories for category extraction
/api/item for item extraction
These APIs could then be used where needed on the sites that have been customized.
If, however, you need to apply the same API to different parts of the same site, you can customize where your rule is in effect by tailoring your rule’s Domain Regex (URL pattern) in the API Toolkit:
By default when you create a new rule, the Domain Regex will apply it to the entire domain. By writing a customized regular expression, you can determine which subset of the web site will be affected by your rule. For example:
Adjusting the default Domain Regex to (http(s)?://)?(.*\.)?diffbot.com/products.* will restrict rules from being applied unless a URL contains diffbot.com/products.
Adjusting a Domain Regex to (http(s)?://)?(.*\.)?diffbot.com/company.* will restrict rules to those URLs that contain diffbot.com/company.
Using these different Domain Regex values will allow you to apply multiple rulesets within the same API to the same site.
]]>",,http://support.diffbot.com/feed/,,
What’s the Difference Between Web Scraping and Diffbot?,d2017-04-30T00:00,Dru Wynings,Diffbot,"Web crawler,Web scraping,World Wide Web,Diffbot,application programming interface","Web scraping is one of the best techniques for extracting important data from websites to use in your business or applications, but not all data is created equal and not all web scraping tools can get you the data you need.
Collecting data from the web isn’t necessarily the hard part. Web scraping techniques utilize web crawlers, which are essentially just programs or automated scripts that collect various bits of data from different sources.
Any developer can build a relatively simple web scraper for their own use, and there are certainly companies out there that have their own web crawlers to gather data for them (Amazon is a big one).
But the web scraping process isn’t always straightforward, and there are many considerations that cause scapers to break or become less efficient. So while there are plenty of web crawlers out there that can get you some of the data you need, not all can produce results.
Here’s what you need to know.
Getting Enough (of the Right) Data
There are actually plenty of ways you can get data from the web without using a web crawler. For instance, many sites have official APIs that will pull data for you. For example, Twitter has one here. If you wanted to know how many people were mentioning you on Twitter, you could use the API to gather that data without too much effort.
The problem, however, is that your options when using site-specific API are somewhat limited; you can only get information from one site at a time, and some APIs (like Twitter) are rate limited, meaning that you have to pay fees to access more information.
In order to make data useful, you need a lot of it. That’s where more generic web crawlers come in handy; they can be programmed to pull data from numerous sites (hundreds, thousands, even millions) if you know what data you’re looking for.
The key is that you have to know what data you’re looking for. Your average web crawler can pull data, but it can’t always give you structured data.
If you were looking to pull news articles or blog posts from multiple websites, for example, any web scraper could pull that content for you. But it would also pull ads, navigation, and a variety of other data you don’t want. It would then be your job to sort through that data for the content you do want.
If you want to pull the most accurate data, what you really need is a tool that can extract clean text from news articles and blog posts without extraneous data in the mix.
This is precisely why Diffbot has tools like our Article API (which does the above) as well as a variety of other specific APIs (like Product, Video, and Image and Page extraction) that can get you the right data from hundreds of thousands of websites automatically with zero configuration.
How Structure Affects Your Outcome
You also have to worry about the quality of the data you’re getting, especially if you’re trying to extract a lot of it from hundreds or thousands of sources.
Apps, programs and even analysis tools – or anything you would be feeding data to – for the most part rely on highly structured data, which means that the way your data is delivered is important.
Web crawlers can pull data from the web, but not all of them can give you structured data, or at least high-quality structured data.
Think of it like this: You could go to a website, find a table of information that’s relevant to your needs, and then copy it and paste it into an Excel file. It’s a time-consuming process, which a web scraper could handle for you en masse, and much faster than you could do it by hand.
But what it can’t do is handle websites that don’t already have that information formatted perfectly, like sites with badly formatted HTML code with little to no underlying structure, for example.
Sites with CAPTCHA codes, pay walls, or other authentication systems may be difficult to pull data from with a simple scraper. Session-based sites that track users with cookies, those that have server admins that block access to non-servers, or those that have a lack of complete item listings or poor search features can all wreak havoc when it comes to getting well-organized data.
While a simple web crawler can give you structured data, it can’t handle complexities or abnormalities that pop up when browsing thousands of sites at once. This means that no matter how powerful it is you’re still not getting all the data you could possibly get.
That’s why Diffbot works so well; we’re built for complexities.
Our APIs can be tweaked for complicated scenarios, and we have several other features, like entity tagging that can find the right data sources from poorly structured sites.
We offer proxying for difficult-to-reach sites that block traditional crawlers, as well as automatic ban detection and automatic retries, making it easier to get data from difficult sites. Our infrastructure is based on gigablast, which we’ve open sourced.
Why Simple Crawlers Aren’t Enough
There are many other issues with your average web crawler as well, including things like maintenance and stale data.
You can design a web crawler for specific purposes, like pulling clean text from a single blog or pulling product listings from an ecommerce site. But in order to get the sheer amount of data you need, you have to run your crawler multiple times, across thousands or more sites, and you have to adjust for every complex site as needed.
This can work fine for smaller operations, like if you wanted to crawl your own ecommerce site to generate a product database, for instance.
If you wanted to do this on multiple sites, or even on a single site as large as Amazon (which boasts nearly 500 million products and rising), you would have to run your crawler every minute of every day across multiple clusters of servers in order to get any fresh, usable data.
Should your crawler break, encounter a site that it can’t handle, or simply need an update to gather new data (or maybe you’re using multiple crawlers to gather different types of data), you’re facing countless hours of upkeep and coding.
That’s one of the biggest things that separates Diffbot from your average web scraping: we do the grunt work for you. Our programs are quick, easy to use (any developer can run a complex crawl in a matter of seconds).
As we said, any developer can build a web scraper. That’s not really the problem. The problem is that not every developer can (or should) spend most of their time running, operating, and optimizing a crawler. There are endless important tasks that developers are paid to do, and babysitting web data shouldn’t be one of them.
Final Thoughts
There are certainly instances where a basic web scraper will get the job done, and not every company needs something robust to gather the data they need.
However, knowing that the more data you have (especially if that data is fresh, well-structured and contains the information you want) the better your results will be, there is something to be said for having a third party vendor on your side.
And just because you can build a web crawler doesn’t mean you should have to. Developers work hard building complex programs and apps for businesses, and they should focus on their craft instead of spending energy scraping the web.
Let me tell you from personal experience, writing and maintaining a web scraper is the bane of most developer’s existence. Now no one is forced to draw the short straw.
That’s why Diffbot exists.",,http://blog.diffbot.com/whats-the-difference-between-web-scraping-and-diffbot/amp/,,
Diffbot Infrastructure with Mike Tung,d2019-10-15T00:00,SE Daily,Software Engineering Daily,"Android,Diffbot,Software Engineering Daily","Diffbot is a knowledge graph that allows developers to interface with the unstructured web as if it was a structured database. In today’s show, Diffbot CEO Mike Tung returns for a second discussion about how he has built Diffbot and how Diffbot is used.
The web has many different entities. Web pages, topics, people, stories, articles, companies, and much more. Humans use a search engine to find answers to their questions within web pages. Machines need to find answers to these kinds of questions as well, but a machine is not sophisticated enough to figure out answers from an unstructured web page.
Diffbot brings structure to those webpages, and gives them an API interface for developers to build on top of. In order to create this system in a cost-efficient manner, Diffbot runs its own data centers, where web scraping, machine learning, and API infrastructure are all used to build the Diffbot application.
Mike joins me for an interview about creating Diffbot, as well as his strategy for running the business.
Sponsorship inquiries: sponsor@softwareengineeringdaily.com
Check out our active projects:
We are hiring a head of growth. If you like Software Engineering Daily and consider yourself competent in sales, marketing, and strategy, send me an email: jeff@softwareengineeringdaily.com
FindCollabs is a place to build open source software.
The SEDaily app for iOS and Android includes all 1000 of our old episodes, as well as related links, greatest hits, and topics. Subscribe for ad-free episodes.
Transcript
Transcript provided by We Edit Podcasts. Software Engineering Daily listeners can go to weeditpodcasts.com/sed to get 20% off the first two months of audio editing and transcription services. Thanks to We Edit Podcasts for partnering with SE Daily. Please click here to view this show’s transcript.
Sponsors
PagerDuty helps your company’s digital operations run more smoothly. PagerDuty helps you intelligently pinpoint issues like outages, as well as capitalize on opportunities, empowering teams to take the right, real time action. To see how companies like GE, Vodafone, Box and American Eagle Outfitters rely on PagerDuty to continuously improve their digital operations visit PagerDuty.com.
Cruise is a San Francisco-based company building a fully electric self-driving car service. Cruise is a place where you can build on your existing skills while developing new skills and experiences that are pioneering the future of industry. There are opportunities for backend engineers, frontend developers, machine learning programmers, and many more positions. At Cruise you will be surrounded by talented, driven engineers-–all while helping make cities safer and cleaner. Apply to work at Cruise, by going to getcruise.com/careers.
Vettery is an online hiring marketplace that connects highly qualified workers with top companies. Vettery keeps the quality of workers and companies on the platform high, because they vet both workers and companies. Check out vettery.com/sedaily, and get a $300 sign-up bonus if you accept a job through Vettery.
MongoDB is the most popular document-based database built for modern application developers and the cloud era. Try MongoDB today with Atlas, the global cloud database service that runs on AWS, Azure, and Google Cloud. Configure, deploy, and connect to your database in just a few minutes. Check it out at mongodb.com/atlas.",0.746,https://softwareengineeringdaily.com/2019/10/15/diffbot-infrastructure-with-mike-tung,,
Diffbot Could Not Apply Rules Error,d2019-01-07T00:00,,Diffbot,"Error message,Ruler,Diffbot,Error","Error message: “Diffbot could not apply rules”.
This error means your custom rule is trying to find an element which doesn’t exist on the page, and that no other fields are being extracted that can be considered valid. In other words “I have nothing to extract according to the rules you gave me”.
If you want to avoid the error, you can add a wildcard field into the Custom rule you created. A wildcard field is one that always successfully extracts a value, so that Diffbot has something to extract even if the main content is not fetchable. A field like this should work fine:
selector: title
name: title
This will make sure that each URL always has at least a title field, and then anything else you define on top of this.",-0.3261,http://support.diffbot.com/errors/diffbot-could-not-apply-rules-error/,,
Diffbot System Status,,,Diffbot,,,,http://status.diffbot.com/,,
Can Diffbot APIs Extract Content from PDFs or Other Documents?,d2014-11-18T00:00,,Diffbot,"Diffbot,PDF","As of September 2016 Diffbot’s Automatic APIs are able to structure content from PDF files.
This is a beta functionality and only available in direct API calls—it is not currently possible to process PDFs while using Crawlbot. (PDF URLs will be successfully processed in Bulk Service jobs.)
Quality of PDF extraction varies and depends significantly on the underlying structure of the document itself.",-0.19658,http://support.diffbot.com/automatic-apis/can-diffbot-apis-extract-content-from-pdfs-or-other-documents/,,
How long can a single request take / what is the Diffbot API timeout?,d2016-02-15T00:00,,Diffbot,"Time-out (sport),Diffbot","A Diffbot API request — call to a Custom or Automatic API — can take, in theory, a maximum of 180 seconds (three minutes). If a request reaches this length it will be automatically timed-out and an error returned.
In practice: few if any requests will take anywhere close to this length of time, because Diffbot APIs have a default internal timeout of only 30 seconds for the retrieval of third-party content. That is, the download and rendering of the requested web page can take a maximum of 30 seconds before being truncated and an error returned.
The subsequent Diffbot processing of a rendered page is measured in milliseconds, so only in rare exceptions can a response take more than 30 seconds.
For multipage articles that are automatically or manually concatenated (see how Diffbot handles multiple-page articles and discussions), the 30-second fetch timeout applies to each individual page. So, for example, a site whose pages take an average of 15 seconds to load will incur, for a ten-page article, a total Diffbot API request time of ~150 seconds for a ten-page article.
The 30-second fetch timeout can be adjusted using the timeout argument in your Diffbot API request. E.g., a request argument of timeout=10000 will require a page to be fully retrieved in less than ten seconds; a request argument of timeout=60000 will extend this to one minute.",-0.4926,http://support.diffbot.com/automatic-apis/what-is-the-diffbot-timeout/,,
Diffbot – Sigue páginas sin rss desde Firefox,d2009-01-12T00:00,,WWWhat's new,"Diffbot,Firefox,Hoy","Hace ya algún tiempo os hablábamos de FeedBeater, una página que nos facilitaba la creación de un canal rss de aquellas páginas que carecieran de ella.
Hoy os traemos el complemento perfecto para esta herramienta, y se trata de Diffbot, el cual es un plugin que se instala en firefox y que, haciendo uso de FeedBeater nos ayuda, de una manera más rápida y directa, a la creación de estos canales para que el seguimiento sea más óptimo y no tengamos que estar entrando en las diferentes páginas cada cierto tiempo para comprobar si hay alguna nueva actualización.
Pero la cosa no se queda ahí, si no que además el mismo plugin ya dispone de un propio lector de feeds o rss, integrado en el navegador, por lo que podremos tener ordenados y separados aquellos feeds creados con esta herramienta, y no mezclarlos con el resto que tengamos en nuestro lector habitual. Aunque claro, nada nos impide añadir a Diffbot todos nuestros feeds y prescindir del resto de lectores que hayamos estado usando hasta el momento.
En definitiva, una gran herramienta, muy útil hoy, aunque esperemos que todas las web se vayan modernizando y aplicaciones de este estilo no nos hagan falta.",-0.989,https://wwwhatsnew.com/?p=11605,,
Diffbot?AI???????????????——???????????1????????,d2018-09-11T15:00,Kyle Wiggers,THE BRIDGE(?·????),"???,Google,????,??,??????,??,AI,???????,???????,??","Google ???????????????????????????????????????????????????????????????????????????????????????Google ? Knowledge Graph ?????????????????Knowledge Graph ????????? Google Home ?????????????????????????????????????????·???????????Knowledge Graph ???16?????????????????????????????????????????????????????????????????????????????????????????????????
???????Mike Tung ???????????????????????????
Tung ????????????????????????? Diffbot ????????Diffbot ???????????????????????????????????????????????????? Tung ????????????????????????????????????????Diffbot ?????????????????????????????(8??5?)??????????????
?????????????????????????????????????????????????????????????
Tung ?? VentureBeat ??????????????????????
????????????????????????????? Tung ?? AI ??????????? Diffbot ??????????????????????5???????Diffbot ?????????????????????????????????????????????????????????????????????Tung ??????Diffbot ????????????????????????????????????90%??20????????????????????(????????????Amazon.com ????????????????????????????????????????)
???????????·??·?·????????????????????????30%???????????????????????????????????????????????????????????(Tung ?)
Diffbot ????????????????Diffbot Knowledge Graph(DKG)???????????????????????????1?????????100????????????????Tung ??????????????1?3,000???????????????????????????(?????????????????????)??????(??????????????????????)???(???????????????????????????????????????)?????????(???????????????????????????)???(?????????????????????????)?
????????API ??????????????????????·???????????? Diffbot DQL ??????????????????Diffbot ???????? UI ?? DKG ??????????????????????????(?)??????????????????????????????????????????????????????????????????????????????
Microsoft ? eBay?Yandex?DuckDuckGo ? Diffbot ????????????????????????????????? DKG ???????????????????????Cisco?Salesforce?Crunchbase?Hubspot?Adobe?Instapaper?Onswipe ??????
Diffbot ????? Felicis Ventures ?????????????????? Aydin Senkut ???????????
???????Diffbot ????????????? AI ????????????????? Diffbot ???? AI ????????????Diffbot ???????????????????????????????????????????
Tung ?????????????????????????????????????????????????????????????Diffbot ??????????????Google ???????????????????????????? Enter ??????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????????????????????(?????????)???????????????????????????? CV(???)????????????????????????
Tung ?????????Diffbot ?????????1?????????(????1??????)???????????????????????????????????????????? DQL ??????????????(??????type:Person employments.employer.name:’Diffbot’???)???????????????????????????????????????????????????1???????????
?????????????????????????????????????1???????????(Tung ?)
Google ? Knowledge Graph ???????????????????????????????????????????2???????????????????????????????Diffbot ??????????????????2?????????????? Tung ?????Diffbot ??Google ? Knowledge Graph ????????????????????????????????????????????????Diffbot ???????????? DKG ???????????????????????????????? ?????????????????????????????????????????????????????????
??????Diffbot ??????????????????????????????1?????????????????????????????????????????????????????(Tung ?)
Diffbot ?2008??????????????????????????????????????28??????????VC Tencent ? Felicis Ventures?Amplify Partners ?????????????????????1,000????????????
??????????????
????????????????????????????????!",,http://thebridge.jp/2018/09/diffbot-launches-ai-powered-knowledge-graph-of-1-trillion-people-places-and-things,,
Diffbot: El próximo paso de la inteligencia artificial,d2020-10-20T05:00,,EXITOSA NOTICIAS - NOTICIAS DEL PERU Y EL MUNDO,"artificial intelligence,Diffbot,machine learning","Diffbot es la empresa dedicada a la creación de algoritmos que funcionan mediante el aprendizaje automático y a la recopilación de datos provenientes de la web.
La inteligencia artificial permite numerosas aplicaciones que ya están siendo utilizadas en casi todos los ámbitos de la vida. Dentro de la inteligencia artificial se encuentra el aprendizaje automático, la rama que se encarga de desarrollar técnicas y procedimientos que les permitan a las máquinas aprender cosas nuevas. En este marco se inserta Diffbot, una empresa dedicada al aprendizaje automático y la recopilación de datos de Internet que, a través de las API (siglas en inglés de Interfaz de Programación de Aplicaciones), pretende convertirse en la mayor base de conocimientos a nivel global. Sin duda, Diffbot es el próximo paso al que aspira la inteligencia artificial.
? ¿Qué es el aprendizaje automático?
Para entender cómo funciona Diffbot, es necesario definir dos conceptos claves de la tecnología que utiliza: la inteligencia artificial y el aprendizaje automático. La inteligencia artificial es la inteligencia que contienen las máquinas y que puede tener múltiples aplicaciones; desde poner en funcionamiento un casino en vivo como el que se encuentra aquí hasta facilitar un diagnóstico médico o recomendar música en YouTube, casi no existe sector que no aplique de una forma u otra inteligencia artificial. Por su parte, el aprendizaje automático es el proceso que lleva a cabo una máquina para adquirir conocimiento por sus propios medios.
? ¿Cómo es Diffbot?
Diffbot es la empresa dedicada a la creación de algoritmos que funcionan mediante el aprendizaje automático y a la recopilación de datos provenientes de la web. Con el objetivo de utilizar la inteligencia artificial para convertirse en la base de conocimientos más amplia a nivel mundial, Diffbot intentará adquirir conocimiento desde varios puntos de vista distintos y en diferentes idiomas. Para lograrlo, navegará como si fuera un ser humano y recopilará información hasta convertirse en un erudito de distintas disciplinas.
Esta tecnología basada en inteligencia artificial propone dar respuestas a la búsqueda del usuario de la misma manera que Google, pero agregando información adicional que el usuario no haya consultado. Por ejemplo, si el internauta quiere saber qué es el aprendizaje automático, Diffbot no solo responderá ese interrogante, sino que también brindará información sobre qué aplicaciones tiene y de qué manera funciona. Por otro lado, Diffbot generará información que utilizará en la confección de gráficos. Este tipo de relación imita a la de las personas creativas que interrelacionan distintos conceptos para llegar a una conclusión. De esta manera, al realizar una búsqueda en Internet, Diffbot será capaz de relacionar otros conceptos para brindar una respuesta más amplia. Siguiendo el ejemplo del aprendizaje automático, este algoritmo será capaz de relacionar al propio Diffbot entre los resultados obtenidos.
Diffbot imitará las capacidades humanas para encontrar la información que busca el usuario a través de la recopilación de datos con API. Sin embargo, a diferencia del ser humano, este algoritmo no tardará más que unos pocos segundos en encontrar la información que más se ajuste a la búsqueda del usuario y lo hará navegando por Internet. Para obtener una información precisa, Diffbot será capaz de categorizar los contenidos y dividirlos para que los datos sean aún más puntuales.
La inteligencia artificial y el aprendizaje automático se encuentran presentes en casi todos los ámbitos de la vida y posibilitan la creación de un algoritmo como el que utilizará Diffbot para convertirse en la base de conocimiento más amplia y completa del mundo. Diffbot utilizará las técnicas de aprendizaje automático y, junto con la extracción de datos provenientes de sitios web, será capaz de proporcionar respuestas concretas y relacionarlas con otros conceptos para que el usuario adquiera únicamente información relevante a su búsqueda. Este algoritmo planea convertirse en un verdadero erudito y con él la inteligencia artificial pasará al próximo nivel.",,https://exitosanoticias.pe/v1/diffbot-el-proximo-paso-de-la-inteligencia-artificial/,,
